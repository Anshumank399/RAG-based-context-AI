{"sample4": [{"text_lines": [{"polygon": [[218.0, 136.0], [574.0, 136.0], [574.0, 159.0], [218.0, 159.0]], "confidence": 0.9767172932624817, "text": "AutoAD III: The Prequel – Back to the Pixels", "bbox": [218.0, 136.0, 574.0, 159.0]}, {"polygon": [[87.0, 190.0], [170.0, 190.0], [170.0, 207.0], [87.0, 207.0]], "confidence": 0.9159510731697083, "text": "Tengda Han 1", "bbox": [87.0, 190.0, 170.0, 207.0]}, {"polygon": [[184.0, 190.0], [259.0, 190.0], [259.0, 208.0], [184.0, 208.0]], "confidence": 0.8907722234725952, "text": "Max Bain 1", "bbox": [184.0, 190.0, 259.0, 208.0]}, {"polygon": [[272.0, 190.0], [379.0, 190.0], [379.0, 208.0], [272.0, 208.0]], "confidence": 0.9249167442321777, "text": "Arsha Nagrani 1†", "bbox": [272.0, 190.0, 379.0, 208.0]}, {"polygon": [[482.0, 190.0], [568.0, 188.0], [568.0, 208.0], [482.0, 209.0]], "confidence": 0.8983858227729797, "text": "Weidi Xie 1,3", "bbox": [482.0, 190.0, 568.0, 208.0]}, {"polygon": [[388.0, 191.0], [474.0, 188.0], [474.0, 207.0], [389.0, 210.0]], "confidence": 0.8834744691848755, "text": "Gül Varol 1,2", "bbox": [388.0, 191.0, 474.0, 207.0]}, {"polygon": [[580.0, 191.0], [703.0, 191.0], [703.0, 207.0], [580.0, 207.0]], "confidence": 0.9382164478302002, "text": "Andrew Zisserman 1", "bbox": [580.0, 191.0, 703.0, 207.0]}, {"polygon": [[332.0, 210.0], [498.0, 211.0], [498.0, 226.0], [332.0, 225.0]], "confidence": 0.9643075466156006, "text": "2 LIGM, École des Ponts ParisTech", "bbox": [332.0, 210.0, 498.0, 226.0]}, {"polygon": [[99.0, 211.0], [320.0, 211.0], [320.0, 226.0], [99.0, 226.0]], "confidence": 0.9748839735984802, "text": "1 Visual Geometry Group, University of Oxford", "bbox": [99.0, 211.0, 320.0, 226.0]}, {"polygon": [[509.0, 211.0], [693.0, 211.0], [693.0, 226.0], [509.0, 226.0]], "confidence": 0.9668077230453491, "text": "3 CMIC, Shanghai Jiao Tong University", "bbox": [509.0, 211.0, 693.0, 226.0]}, {"polygon": [[227.0, 230.0], [558.0, 230.0], [558.0, 244.0], [227.0, 244.0]], "confidence": 0.9789266586303711, "text": "https://www.robots.ox.ac.uk/vgg/research/autoad/", "bbox": [227.0, 230.0, 558.0, 244.0]}, {"polygon": [[194.0, 264.0], [252.0, 264.0], [252.0, 279.0], [194.0, 279.0]], "confidence": 0.8857166767120361, "text": "Abstract", "bbox": [194.0, 264.0, 252.0, 279.0]}, {"polygon": [[645.0, 283.0], [702.0, 282.0], [702.0, 293.0], [646.0, 295.0]], "confidence": 0.8820031881332397, "text": "HowTo-AD", "bbox": [645.0, 283.0, 702.0, 293.0]}, {"polygon": [[12.0, 293.0], [52.0, 293.0], [52.0, 738.0], [12.0, 738.0]], "confidence": 0.9575395584106445, "text": "arXiv:2404.14412v1  [cs.CV]  22 Apr 202", "bbox": [12.0, 293.0, 52.0, 738.0]}, {"polygon": [[671.0, 295.0], [701.0, 295.0], [701.0, 304.0], [671.0, 304.0]], "confidence": 0.8502902388572693, "text": "(Ours)", "bbox": [671.0, 295.0, 701.0, 304.0]}, {"polygon": [[79.0, 297.0], [383.0, 297.0], [383.0, 312.0], [79.0, 312.0]], "confidence": 0.9819300770759583, "text": "Generating Audio Description (AD) for movies is a challeng-", "bbox": [79.0, 297.0, 383.0, 312.0]}, {"polygon": [[63.0, 313.0], [384.0, 313.0], [384.0, 327.0], [63.0, 327.0]], "confidence": 0.9807435274124146, "text": "ing task that requires fine-grained visual understanding and", "bbox": [63.0, 313.0, 384.0, 327.0]}, {"polygon": [[63.0, 329.0], [384.0, 329.0], [384.0, 343.0], [63.0, 343.0]], "confidence": 0.9823160767555237, "text": "an awareness of the characters and their names.  Currently,", "bbox": [63.0, 329.0, 384.0, 343.0]}, {"polygon": [[414.0, 330.0], [427.0, 330.0], [427.0, 393.0], [414.0, 393.0]], "confidence": 0.880386471748352, "text": "umber of description", "bbox": [414.0, 330.0, 427.0, 393.0]}, {"polygon": [[63.0, 344.0], [384.0, 344.0], [384.0, 358.0], [63.0, 358.0]], "confidence": 0.982389509677887, "text": "visual language models for AD generation are limited by a", "bbox": [63.0, 344.0, 384.0, 358.0]}, {"polygon": [[499.0, 350.0], [529.0, 350.0], [529.0, 361.0], [499.0, 361.0]], "confidence": 0.7454714179039001, "text": "MAD", "bbox": [499.0, 350.0, 529.0, 361.0]}, {"polygon": [[63.0, 359.0], [384.0, 359.0], [384.0, 374.0], [63.0, 374.0]], "confidence": 0.9829233288764954, "text": "lack of suitable training data, and also their evaluation is ham-", "bbox": [63.0, 359.0, 384.0, 374.0]}, {"polygon": [[63.0, 375.0], [382.0, 375.0], [382.0, 389.0], [63.0, 389.0]], "confidence": 0.9825157523155212, "text": "pered by using performance measures not specialized to the", "bbox": [63.0, 375.0, 382.0, 389.0]}, {"polygon": [[454.0, 387.0], [484.0, 387.0], [484.0, 396.0], [454.0, 396.0]], "confidence": 0.5876458883285522, "text": "ESMD(", "bbox": [454.0, 387.0, 484.0, 396.0]}, {"polygon": [[63.0, 391.0], [384.0, 391.0], [384.0, 404.0], [63.0, 404.0]], "confidence": 0.9739258885383606, "text": "AD domain. In this paper, we make three contributions: (i)", "bbox": [63.0, 391.0, 384.0, 404.0]}, {"polygon": [[508.0, 391.0], [552.0, 391.0], [552.0, 403.0], [508.0, 403.0]], "confidence": 0.8012073636054993, "text": "CMD-AD", "bbox": [508.0, 391.0, 552.0, 403.0]}, {"polygon": [[520.0, 405.0], [549.0, 405.0], [549.0, 414.0], [520.0, 414.0]], "confidence": 0.7844473123550415, "text": "(Ours)", "bbox": [520.0, 405.0, 549.0, 414.0]}, {"polygon": [[63.0, 406.0], [382.0, 406.0], [382.0, 420.0], [63.0, 420.0]], "confidence": 0.9830101132392883, "text": "We propose two approaches for constructing AD datasets with", "bbox": [63.0, 406.0, 382.0, 420.0]}, {"polygon": [[572.0, 419.0], [612.0, 419.0], [612.0, 431.0], [572.0, 431.0]], "confidence": 0.781715989112854, "text": "DiDeMo", "bbox": [572.0, 419.0, 612.0, 431.0]}, {"polygon": [[63.0, 422.0], [382.0, 422.0], [382.0, 435.0], [63.0, 435.0]], "confidence": 0.9834396839141846, "text": "aligned video data, and build training and evaluation datasets", "bbox": [63.0, 422.0, 382.0, 435.0]}, {"polygon": [[63.0, 437.0], [382.0, 437.0], [382.0, 452.0], [63.0, 452.0]], "confidence": 0.9800384044647217, "text": "using these. These datasets will be publicly released; (ii) We", "bbox": [63.0, 437.0, 382.0, 452.0]}, {"polygon": [[465.0, 447.0], [476.0, 447.0], [476.0, 455.0], [465.0, 455.0]], "confidence": 0.6658738255500793, "text": "10 2", "bbox": [465.0, 447.0, 476.0, 455.0]}, {"polygon": [[536.0, 447.0], [548.0, 447.0], [548.0, 454.0], [536.0, 454.0]], "confidence": 0.6589148044586182, "text": "10 2", "bbox": [536.0, 447.0, 548.0, 454.0]}, {"polygon": [[680.0, 447.0], [690.0, 447.0], [690.0, 454.0], [680.0, 454.0]], "confidence": 0.5582263469696045, "text": "105", "bbox": [680.0, 447.0, 690.0, 454.0]}, {"polygon": [[63.0, 453.0], [383.0, 453.0], [383.0, 467.0], [63.0, 467.0]], "confidence": 0.9800106883049011, "text": "develop a Q-former-based architecture which ingests raw video", "bbox": [63.0, 453.0, 383.0, 467.0]}, {"polygon": [[529.0, 458.0], [641.0, 458.0], [641.0, 469.0], [529.0, 469.0]], "confidence": 0.9588549733161926, "text": "Number of movies / videos", "bbox": [529.0, 458.0, 641.0, 469.0]}, {"polygon": [[63.0, 469.0], [383.0, 469.0], [383.0, 482.0], [63.0, 482.0]], "confidence": 0.9810525178909302, "text": "and generates AD, using frozen pre-trained visual encoders and", "bbox": [63.0, 469.0, 383.0, 482.0]}, {"polygon": [[409.0, 472.0], [728.0, 472.0], [728.0, 484.0], [409.0, 484.0]], "confidence": 0.9819632172584534, "text": "Figure 1. We propose two new movie Audio Description (AD) datasets", "bbox": [409.0, 472.0, 728.0, 484.0]}, {"polygon": [[63.0, 483.0], [384.0, 483.0], [384.0, 498.0], [63.0, 498.0]], "confidence": 0.981792688369751, "text": "large language models; and (iii) We provide new evaluation met-", "bbox": [63.0, 483.0, 384.0, 498.0]}, {"polygon": [[410.0, 487.0], [728.0, 487.0], [728.0, 499.0], [410.0, 499.0]], "confidence": 0.9784896373748779, "text": "with pixels – CMD-AD and HowTo-AD by temporally aligning or", "bbox": [410.0, 487.0, 728.0, 499.0]}, {"polygon": [[63.0, 499.0], [382.0, 499.0], [382.0, 513.0], [63.0, 513.0]], "confidence": 0.982624888420105, "text": "rics to benchmark AD quality that are well matched to human", "bbox": [63.0, 499.0, 382.0, 513.0]}, {"polygon": [[408.0, 500.0], [728.0, 500.0], [728.0, 514.0], [408.0, 514.0]], "confidence": 0.9854599237442017, "text": "textually transforming existing pixel video datasets. The marker size", "bbox": [408.0, 500.0, 728.0, 514.0]}, {"polygon": [[63.0, 514.0], [382.0, 514.0], [382.0, 528.0], [63.0, 528.0]], "confidence": 0.9837852120399475, "text": "performance. Taken together, we improve the state of the art on", "bbox": [63.0, 514.0, 382.0, 528.0]}, {"polygon": [[408.0, 514.0], [728.0, 516.0], [728.0, 530.0], [408.0, 528.0]], "confidence": 0.9853418469429016, "text": "is proportional to the total video durations and grey color indicates", "bbox": [408.0, 514.0, 728.0, 530.0]}, {"polygon": [[409.0, 529.0], [607.0, 529.0], [607.0, 543.0], [409.0, 543.0]], "confidence": 0.9779533743858337, "text": "datasets with features instead of raw pixels.", "bbox": [409.0, 529.0, 607.0, 543.0]}, {"polygon": [[63.0, 530.0], [143.0, 530.0], [143.0, 543.0], [63.0, 543.0]], "confidence": 0.9205553531646729, "text": "AD generation.", "bbox": [63.0, 530.0, 143.0, 543.0]}, {"polygon": [[408.0, 554.0], [728.0, 554.0], [728.0, 569.0], [408.0, 569.0]], "confidence": 0.9843233227729797, "text": "crucial objective of naming the characters in the generated text", "bbox": [408.0, 554.0, 728.0, 569.0]}, {"polygon": [[408.0, 570.0], [728.0, 570.0], [728.0, 584.0], [408.0, 584.0]], "confidence": 0.981013298034668, "text": "descriptions [ 21 ]. However, MAD only provides frame-level", "bbox": [408.0, 570.0, 728.0, 584.0]}, {"polygon": [[64.0, 579.0], [164.0, 579.0], [164.0, 594.0], [64.0, 594.0]], "confidence": 0.936130940914154, "text": "1. Introduction", "bbox": [64.0, 579.0, 164.0, 594.0]}, {"polygon": [[408.0, 586.0], [728.0, 586.0], [728.0, 600.0], [408.0, 600.0]], "confidence": 0.9804818034172058, "text": "CLIP features (and only at 5 Hz) and this has limited the ability", "bbox": [408.0, 586.0, 728.0, 600.0]}, {"polygon": [[408.0, 601.0], [728.0, 601.0], [728.0, 615.0], [408.0, 615.0]], "confidence": 0.9837526082992554, "text": "of generative models to provide fine-grained spatial details. Re-", "bbox": [408.0, 601.0, 728.0, 615.0]}, {"polygon": [[63.0, 608.0], [327.0, 608.0], [327.0, 622.0], [63.0, 622.0]], "confidence": 0.9770663976669312, "text": "Cinema is a matter of what's in the frame and what's out.", "bbox": [63.0, 608.0, 327.0, 622.0]}, {"polygon": [[408.0, 616.0], [728.0, 616.0], [728.0, 630.0], [408.0, 630.0]], "confidence": 0.9824090600013733, "text": "cent Visual Language Models (VLMs) [ 1 , 31 – 33 , 71 , 74 ] have", "bbox": [408.0, 616.0, 728.0, 630.0]}, {"polygon": [[265.0, 623.0], [340.0, 623.0], [340.0, 635.0], [265.0, 635.0]], "confidence": 0.9357471466064453, "text": "Martin Scorsese", "bbox": [265.0, 623.0, 340.0, 635.0]}, {"polygon": [[409.0, 632.0], [728.0, 632.0], [728.0, 645.0], [409.0, 645.0]], "confidence": 0.9846348762512207, "text": "accessed the spatial feature map of the image (or video) in order", "bbox": [409.0, 632.0, 728.0, 645.0]}, {"polygon": [[409.0, 647.0], [728.0, 647.0], [728.0, 660.0], [409.0, 660.0]], "confidence": 0.9844716787338257, "text": "to obtain fuller descriptions or answer more detailed questions.", "bbox": [409.0, 647.0, 728.0, 660.0]}, {"polygon": [[80.0, 659.0], [382.0, 659.0], [382.0, 674.0], [80.0, 674.0]], "confidence": 0.9836017489433289, "text": "Audio description (AD) is an accessibility tool for the blind", "bbox": [80.0, 659.0, 382.0, 674.0]}, {"polygon": [[425.0, 663.0], [728.0, 663.0], [728.0, 677.0], [425.0, 677.0]], "confidence": 0.9808207750320435, "text": "We make the following contributions: First, we provide two", "bbox": [425.0, 663.0, 728.0, 677.0]}, {"polygon": [[64.0, 677.0], [383.0, 676.0], [383.0, 689.0], [64.0, 690.0]], "confidence": 0.982427716255188, "text": "and visually impaired that describes visual content which", "bbox": [64.0, 677.0, 383.0, 689.0]}, {"polygon": [[409.0, 679.0], [728.0, 679.0], [728.0, 692.0], [409.0, 692.0]], "confidence": 0.9833271503448486, "text": "new datasets that can be used to train an AD generation model", "bbox": [409.0, 679.0, 728.0, 692.0]}, {"polygon": [[63.0, 691.0], [382.0, 691.0], [382.0, 705.0], [63.0, 705.0]], "confidence": 0.9796254634857178, "text": "is essential for following video programs 1 .  Automatically", "bbox": [63.0, 691.0, 382.0, 705.0]}, {"polygon": [[408.0, 694.0], [728.0, 694.0], [728.0, 709.0], [408.0, 709.0]], "confidence": 0.973438024520874, "text": "end. The datasets go beyond MAD [ 54 ] in that they", "bbox": [408.0, 694.0, 728.0, 709.0]}, {"polygon": [[63.0, 707.0], [382.0, 707.0], [382.0, 720.0], [63.0, 720.0]], "confidence": 0.984359622001648, "text": "generating AD text is a challenging task as the information must", "bbox": [63.0, 707.0, 382.0, 720.0]}, {"polygon": [[409.0, 710.0], [729.0, 710.0], [729.0, 724.0], [409.0, 724.0]], "confidence": 0.9839029312133789, "text": "provide video as input, rather than only a CLIP frame feature,", "bbox": [409.0, 710.0, 729.0, 724.0]}, {"polygon": [[63.0, 722.0], [384.0, 722.0], [384.0, 737.0], [63.0, 737.0]], "confidence": 0.9821147322654724, "text": "be accurate, character-aware, story-aware, complementary to", "bbox": [63.0, 722.0, 384.0, 737.0]}, {"polygon": [[409.0, 726.0], [729.0, 726.0], [729.0, 740.0], [409.0, 740.0]], "confidence": 0.9775680899620056, "text": "i.e., they go back to the pixels. The first dataset, CMD-AD ,", "bbox": [409.0, 726.0, 729.0, 740.0]}, {"polygon": [[63.0, 738.0], [382.0, 738.0], [382.0, 752.0], [63.0, 752.0]], "confidence": 0.9834246635437012, "text": "the soundtrack, and distilled into the gaps between speech. For", "bbox": [63.0, 738.0, 382.0, 752.0]}, {"polygon": [[409.0, 740.0], [728.0, 740.0], [728.0, 755.0], [409.0, 755.0]], "confidence": 0.9760017395019531, "text": "is constructed from two publicly available sources – the AD", "bbox": [409.0, 740.0, 728.0, 755.0]}, {"polygon": [[63.0, 752.0], [383.0, 754.0], [383.0, 769.0], [63.0, 767.0]], "confidence": 0.9830924868583679, "text": "TV broadcasters in the US and UK, providing AD for a certain", "bbox": [63.0, 752.0, 383.0, 769.0]}, {"polygon": [[409.0, 756.0], [728.0, 756.0], [728.0, 770.0], [409.0, 770.0]], "confidence": 0.9822869896888733, "text": "descriptions for films available from AudioVault 2 and the movie", "bbox": [409.0, 756.0, 728.0, 770.0]}, {"polygon": [[63.0, 769.0], [372.0, 769.0], [372.0, 783.0], [63.0, 783.0]], "confidence": 0.9831002950668335, "text": "percentage of video content has become a legal requirement.", "bbox": [63.0, 769.0, 372.0, 783.0]}, {"polygon": [[408.0, 771.0], [728.0, 771.0], [728.0, 786.0], [408.0, 786.0]], "confidence": 0.9787216186523438, "text": "clips available from CMD [ 4 ].  The challenge in this case is", "bbox": [408.0, 771.0, 728.0, 786.0]}, {"polygon": [[80.0, 785.0], [384.0, 785.0], [384.0, 799.0], [80.0, 799.0]], "confidence": 0.9821979403495789, "text": "With the current power of visual-to-text generative models,", "bbox": [80.0, 785.0, 384.0, 799.0]}, {"polygon": [[408.0, 786.0], [728.0, 786.0], [728.0, 801.0], [408.0, 801.0]], "confidence": 0.9832417368888855, "text": "how to determine the temporal alignment of these two sources", "bbox": [408.0, 786.0, 728.0, 801.0]}, {"polygon": [[63.0, 800.0], [384.0, 800.0], [384.0, 814.0], [63.0, 814.0]], "confidence": 0.9811025857925415, "text": "generating AD automatically is now becoming possible [ 66 ],", "bbox": [63.0, 800.0, 384.0, 814.0]}, {"polygon": [[409.0, 802.0], [729.0, 802.0], [729.0, 816.0], [409.0, 816.0]], "confidence": 0.982279896736145, "text": "given that one has only audio with AD, and the other (CMD)", "bbox": [409.0, 802.0, 729.0, 816.0]}, {"polygon": [[63.0, 816.0], [384.0, 816.0], [384.0, 829.0], [63.0, 829.0]], "confidence": 0.983761727809906, "text": "and there has been a recent flurry of interest in this goal [ 20 , 21 ],", "bbox": [63.0, 816.0, 384.0, 829.0]}, {"polygon": [[409.0, 817.0], [728.0, 817.0], [728.0, 832.0], [409.0, 832.0]], "confidence": 0.9821619391441345, "text": "is non-contiguous with timings unknown with respect to the", "bbox": [409.0, 817.0, 728.0, 832.0]}, {"polygon": [[63.0, 831.0], [384.0, 831.0], [384.0, 846.0], [63.0, 846.0]], "confidence": 0.978254497051239, "text": "kick-started by the availability of films and AD provided in", "bbox": [63.0, 831.0, 384.0, 846.0]}, {"polygon": [[409.0, 834.0], [728.0, 834.0], [728.0, 847.0], [409.0, 847.0]], "confidence": 0.9810406565666199, "text": "original movies. The second dataset, HowTo-AD , is constructed", "bbox": [409.0, 834.0, 728.0, 847.0]}, {"polygon": [[63.0, 848.0], [383.0, 848.0], [383.0, 861.0], [63.0, 861.0]], "confidence": 0.981076180934906, "text": "the MAD dataset [ 54 ]. Key innovations have included partial", "bbox": [63.0, 848.0, 383.0, 861.0]}, {"polygon": [[409.0, 849.0], [728.0, 849.0], [728.0, 862.0], [409.0, 862.0]], "confidence": 0.9810923933982849, "text": "from the large-scale HowTo100M video dataset [ 40 ] that", "bbox": [409.0, 849.0, 728.0, 862.0]}, {"polygon": [[63.0, 863.0], [382.0, 863.0], [382.0, 877.0], [63.0, 877.0]], "confidence": 0.9824130535125732, "text": "training of AD generative models using available large-scale", "bbox": [63.0, 863.0, 382.0, 877.0]}, {"polygon": [[409.0, 865.0], [729.0, 865.0], [729.0, 879.0], [409.0, 879.0]], "confidence": 0.9846498966217041, "text": "originally consists of YouTube videos with narrated instructions.", "bbox": [409.0, 865.0, 729.0, 879.0]}, {"polygon": [[63.0, 879.0], [384.0, 879.0], [384.0, 893.0], [63.0, 893.0]], "confidence": 0.9825073480606079, "text": "datasets [ 20 ], and the introduction of a character bank to", "bbox": [63.0, 879.0, 384.0, 893.0]}, {"polygon": [[409.0, 880.0], [728.0, 880.0], [728.0, 894.0], [409.0, 894.0]], "confidence": 0.9833438992500305, "text": "Inspired by the use of Language Models (LMs) to rephrase the", "bbox": [409.0, 880.0, 728.0, 894.0]}, {"polygon": [[63.0, 894.0], [382.0, 894.0], [382.0, 908.0], [63.0, 908.0]], "confidence": 0.9823000431060791, "text": "provide hints (as prompts) for the language model for the", "bbox": [63.0, 894.0, 382.0, 908.0]}, {"polygon": [[409.0, 895.0], [728.0, 895.0], [728.0, 909.0], [409.0, 909.0]], "confidence": 0.9830832481384277, "text": "instructions as video captions in HowtoCaption [ 53 ], we use", "bbox": [409.0, 895.0, 728.0, 909.0]}, {"polygon": [[409.0, 910.0], [727.0, 910.0], [727.0, 924.0], [409.0, 924.0]], "confidence": 0.9814386367797852, "text": "LMs to repurpose HowTo100M as an AD dataset containing", "bbox": [409.0, 910.0, 727.0, 924.0]}, {"polygon": [[80.0, 924.0], [381.0, 924.0], [381.0, 938.0], [80.0, 938.0]], "confidence": 0.9591798782348633, "text": "1 https://www.3playmedia.com/learn/popular-", "bbox": [80.0, 924.0, 381.0, 938.0]}, {"polygon": [[65.0, 938.0], [238.0, 939.0], [237.0, 952.0], [65.0, 951.0]], "confidence": 0.960640013217926, "text": "topics/audio-description/", "bbox": [65.0, 938.0, 238.0, 952.0]}, {"polygon": [[427.0, 939.0], [563.0, 940.0], [563.0, 952.0], [427.0, 951.0]], "confidence": 0.9548906087875366, "text": "https://audiovault.net", "bbox": [427.0, 939.0, 563.0, 952.0]}], "languages": ["en"], "image_bbox": [0.0, 0.0, 816.0, 1056.0], "page": 1}, {"text_lines": [{"polygon": [[409.0, 96.0], [729.0, 96.0], [729.0, 111.0], [409.0, 111.0]], "confidence": 0.9815005660057068, "text": "the same film has AD provided by several human annotators.", "bbox": [409.0, 96.0, 729.0, 111.0]}, {"polygon": [[205.0, 99.0], [381.0, 99.0], [381.0, 111.0], [205.0, 111.0]], "confidence": 0.9487016201019287, "text": "with pixels # movies # AD total duration", "bbox": [205.0, 99.0, 381.0, 111.0]}, {"polygon": [[117.0, 100.0], [152.0, 100.0], [152.0, 111.0], [117.0, 111.0]], "confidence": 0.8715294003486633, "text": "Dataset", "bbox": [117.0, 100.0, 152.0, 111.0]}, {"polygon": [[408.0, 112.0], [729.0, 112.0], [729.0, 126.0], [408.0, 126.0]], "confidence": 0.9840086102485657, "text": "On these and traditional metrics, we show that our new architec-", "bbox": [408.0, 112.0, 729.0, 126.0]}, {"polygon": [[104.0, 116.0], [165.0, 118.0], [165.0, 130.0], [103.0, 129.0]], "confidence": 0.9095170497894287, "text": "MovieNet [ 24 ]", "bbox": [104.0, 116.0, 165.0, 130.0]}, {"polygon": [[272.0, 118.0], [294.0, 118.0], [294.0, 128.0], [272.0, 128.0]], "confidence": 0.677026093006134, "text": "100", "bbox": [272.0, 118.0, 294.0, 128.0]}, {"polygon": [[221.0, 120.0], [230.0, 120.0], [230.0, 129.0], [221.0, 129.0]], "confidence": 0.040490470826625824, "text": ".", "bbox": [221.0, 120.0, 230.0, 129.0]}, {"polygon": [[408.0, 128.0], [728.0, 128.0], [728.0, 142.0], [408.0, 142.0]], "confidence": 0.9850523471832275, "text": "ture trained on raw pixels directly achieves impressive results for", "bbox": [408.0, 128.0, 728.0, 142.0]}, {"polygon": [[106.0, 131.0], [163.0, 131.0], [163.0, 143.0], [106.0, 143.0]], "confidence": 0.9073359370231628, "text": "LSMDC [ 46 ]", "bbox": [106.0, 131.0, 163.0, 143.0]}, {"polygon": [[276.0, 131.0], [321.0, 131.0], [321.0, 142.0], [276.0, 142.0]], "confidence": 0.8479434251785278, "text": "202 118k", "bbox": [276.0, 131.0, 321.0, 142.0]}, {"polygon": [[221.0, 132.0], [232.0, 132.0], [232.0, 143.0], [221.0, 143.0]], "confidence": 0.04370442405343056, "text": ".", "bbox": [221.0, 132.0, 232.0, 143.0]}, {"polygon": [[358.0, 132.0], [381.0, 132.0], [381.0, 142.0], [358.0, 142.0]], "confidence": 0.7957918643951416, "text": "147h", "bbox": [358.0, 132.0, 381.0, 142.0]}, {"polygon": [[275.0, 143.0], [321.0, 145.0], [321.0, 154.0], [275.0, 153.0]], "confidence": 0.8393951654434204, "text": "650  384k", "bbox": [275.0, 143.0, 321.0, 154.0]}, {"polygon": [[111.0, 144.0], [159.0, 144.0], [159.0, 155.0], [111.0, 155.0]], "confidence": 0.8801252245903015, "text": "MAD [ 54 ]", "bbox": [111.0, 144.0, 159.0, 155.0]}, {"polygon": [[222.0, 144.0], [233.0, 144.0], [233.0, 155.0], [222.0, 155.0]], "confidence": 0.46218419075012207, "text": "x", "bbox": [222.0, 144.0, 233.0, 155.0]}, {"polygon": [[354.0, 144.0], [382.0, 144.0], [382.0, 154.0], [354.0, 154.0]], "confidence": 0.8292153477668762, "text": "1027h", "bbox": [354.0, 144.0, 382.0, 154.0]}, {"polygon": [[408.0, 144.0], [728.0, 144.0], [728.0, 158.0], [408.0, 158.0]], "confidence": 0.9820123314857483, "text": "the task of Movie AD, outperforming previous works on both", "bbox": [408.0, 144.0, 728.0, 158.0]}, {"polygon": [[113.0, 155.0], [155.0, 155.0], [155.0, 167.0], [113.0, 167.0]], "confidence": 0.8738795518875122, "text": "CMD [ 4 ]", "bbox": [113.0, 155.0, 155.0, 167.0]}, {"polygon": [[221.0, 156.0], [232.0, 156.0], [232.0, 167.0], [221.0, 167.0]], "confidence": 0.04370442405343056, "text": ".", "bbox": [221.0, 156.0, 232.0, 167.0]}, {"polygon": [[272.0, 156.0], [295.0, 156.0], [295.0, 167.0], [272.0, 167.0]], "confidence": 0.7678779363632202, "text": "3605", "bbox": [272.0, 156.0, 295.0, 167.0]}, {"polygon": [[354.0, 156.0], [382.0, 156.0], [382.0, 167.0], [354.0, 167.0]], "confidence": 0.8267212510108948, "text": "1270h", "bbox": [354.0, 156.0, 382.0, 167.0]}, {"polygon": [[408.0, 159.0], [720.0, 159.0], [720.0, 174.0], [408.0, 174.0]], "confidence": 0.9822218418121338, "text": "the standard MAD [ 54 ] eval set, and our new proposed test set.", "bbox": [408.0, 159.0, 720.0, 174.0]}, {"polygon": [[66.0, 167.0], [202.0, 167.0], [202.0, 180.0], [66.0, 180.0]], "confidence": 0.9584356546401978, "text": "CMD [ 4 ] ∩ AudioVault-8K [ 20 ]", "bbox": [66.0, 167.0, 202.0, 180.0]}, {"polygon": [[221.0, 168.0], [232.0, 168.0], [232.0, 178.0], [221.0, 178.0]], "confidence": 0.09294334053993225, "text": "1.", "bbox": [221.0, 168.0, 232.0, 178.0]}, {"polygon": [[272.0, 167.0], [295.0, 167.0], [295.0, 178.0], [272.0, 178.0]], "confidence": 0.7937419414520264, "text": "1803", "bbox": [272.0, 167.0, 295.0, 178.0]}, {"polygon": [[356.0, 168.0], [382.0, 168.0], [382.0, 179.0], [356.0, 179.0]], "confidence": 0.7909115552902222, "text": "647h", "bbox": [356.0, 168.0, 382.0, 179.0]}, {"polygon": [[96.0, 185.0], [172.0, 185.0], [172.0, 198.0], [96.0, 198.0]], "confidence": 0.9228997230529785, "text": "CMD-AD (ours)", "bbox": [96.0, 185.0, 172.0, 198.0]}, {"polygon": [[272.0, 185.0], [322.0, 187.0], [322.0, 199.0], [271.0, 197.0]], "confidence": 0.8478749394416809, "text": "1432 101k", "bbox": [272.0, 185.0, 322.0, 199.0]}, {"polygon": [[357.0, 186.0], [382.0, 186.0], [382.0, 198.0], [357.0, 198.0]], "confidence": 0.7785841226577759, "text": "477h", "bbox": [357.0, 186.0, 382.0, 198.0]}, {"polygon": [[221.0, 188.0], [231.0, 188.0], [231.0, 198.0], [221.0, 198.0]], "confidence": 0.08788838237524033, "text": ".", "bbox": [221.0, 188.0, 231.0, 198.0]}, {"polygon": [[409.0, 193.0], [517.0, 193.0], [517.0, 208.0], [409.0, 208.0]], "confidence": 0.9361869096755981, "text": "2. Related Work", "bbox": [409.0, 193.0, 517.0, 208.0]}, {"polygon": [[94.0, 198.0], [174.0, 198.0], [174.0, 211.0], [94.0, 211.0]], "confidence": 0.9327643513679504, "text": "HowTo-AD (ours)", "bbox": [94.0, 198.0, 174.0, 211.0]}, {"polygon": [[221.0, 198.0], [232.0, 198.0], [232.0, 210.0], [221.0, 210.0]], "confidence": 0.11826710402965546, "text": ",", "bbox": [221.0, 198.0, 232.0, 210.0]}, {"polygon": [[254.0, 198.0], [322.0, 198.0], [322.0, 210.0], [254.0, 210.0]], "confidence": 0.9041128158569336, "text": "180,034* 3.4M", "bbox": [254.0, 198.0, 322.0, 210.0]}, {"polygon": [[348.0, 198.0], [382.0, 198.0], [382.0, 210.0], [348.0, 210.0]], "confidence": 0.8498703241348267, "text": "23652h", "bbox": [348.0, 198.0, 382.0, 210.0]}, {"polygon": [[63.0, 220.0], [382.0, 220.0], [382.0, 234.0], [63.0, 234.0]], "confidence": 0.9770172834396362, "text": "Table 1. Statistics of Movie AD datasets. Only a small number", "bbox": [63.0, 220.0, 382.0, 234.0]}, {"polygon": [[409.0, 221.0], [729.0, 221.0], [729.0, 234.0], [409.0, 234.0]], "confidence": 0.9816672205924988, "text": "Dense video captioning. With the availability of large-", "bbox": [409.0, 221.0, 729.0, 234.0]}, {"polygon": [[63.0, 235.0], [382.0, 235.0], [382.0, 249.0], [63.0, 249.0]], "confidence": 0.9840477108955383, "text": "of movie datasets with AD are available, and they have different", "bbox": [63.0, 235.0, 382.0, 249.0]}, {"polygon": [[408.0, 236.0], [729.0, 236.0], [729.0, 251.0], [408.0, 251.0]], "confidence": 0.9813452959060669, "text": "scale data, the field has made significant progress in", "bbox": [408.0, 236.0, 729.0, 251.0]}, {"polygon": [[64.0, 250.0], [383.0, 250.0], [383.0, 263.0], [64.0, 263.0]], "confidence": 0.984272837638855, "text": "limitations: MovieNet only provides keyframes, LSMDC is short in", "bbox": [64.0, 250.0, 383.0, 263.0]}, {"polygon": [[408.0, 251.0], [728.0, 251.0], [728.0, 266.0], [408.0, 266.0]], "confidence": 0.9821242094039917, "text": "captioning images [ 31 , 32 , 41 , 71 ], and trimmed short", "bbox": [408.0, 251.0, 728.0, 266.0]}, {"polygon": [[63.0, 264.0], [383.0, 264.0], [383.0, 277.0], [63.0, 277.0]], "confidence": 0.9832051396369934, "text": "duration, MAD only provides frame-level visual features, and CMD", "bbox": [63.0, 264.0, 383.0, 277.0]}, {"polygon": [[409.0, 267.0], [728.0, 267.0], [728.0, 281.0], [409.0, 281.0]], "confidence": 0.9821473360061646, "text": "video segments [ 35 , 38 , 48 , 49 ].  Movie AD generation is", "bbox": [409.0, 267.0, 728.0, 281.0]}, {"polygon": [[63.0, 278.0], [382.0, 278.0], [382.0, 291.0], [63.0, 291.0]], "confidence": 0.9830983877182007, "text": "does not have corresponding ADs. We propose two new datasets for", "bbox": [63.0, 278.0, 382.0, 291.0]}, {"polygon": [[409.0, 283.0], [728.0, 283.0], [728.0, 297.0], [409.0, 297.0]], "confidence": 0.9805970191955566, "text": "more related to the task of dense video captioning, where", "bbox": [409.0, 283.0, 728.0, 297.0]}, {"polygon": [[63.0, 292.0], [382.0, 292.0], [382.0, 306.0], [63.0, 306.0]], "confidence": 0.9808485507965088, "text": "AD generation task: CMD-AD and HowTo-AD. *: strictly they are", "bbox": [63.0, 292.0, 382.0, 306.0]}, {"polygon": [[408.0, 299.0], [728.0, 299.0], [728.0, 312.0], [408.0, 312.0]], "confidence": 0.9822635054588318, "text": "the goal is to concurrently address temporal localization", "bbox": [408.0, 299.0, 728.0, 312.0]}, {"polygon": [[64.0, 307.0], [207.0, 307.0], [207.0, 319.0], [64.0, 319.0]], "confidence": 0.9681954979896545, "text": "long videos rather than movies.", "bbox": [64.0, 307.0, 207.0, 319.0]}, {"polygon": [[409.0, 314.0], [729.0, 314.0], [729.0, 329.0], [409.0, 329.0]], "confidence": 0.9820140600204468, "text": "and to describe each identified interval in an untrimmed", "bbox": [409.0, 314.0, 729.0, 329.0]}, {"polygon": [[480.0, 330.0], [728.0, 330.0], [728.0, 344.0], [480.0, 344.0]], "confidence": 0.9754139184951782, "text": "The approach to dense captioning has been", "bbox": [480.0, 330.0, 728.0, 344.0]}, {"polygon": [[410.0, 331.0], [480.0, 331.0], [480.0, 343.0], [410.0, 343.0]], "confidence": 0.8999463319778442, "text": "video [ 30 ].", "bbox": [410.0, 331.0, 480.0, 343.0]}, {"polygon": [[63.0, 332.0], [382.0, 332.0], [382.0, 346.0], [63.0, 346.0]], "confidence": 0.984100341796875, "text": "videos with an associated character bank, and text descriptions", "bbox": [63.0, 332.0, 382.0, 346.0]}, {"polygon": [[408.0, 345.0], [728.0, 345.0], [728.0, 359.0], [408.0, 359.0]], "confidence": 0.9844957590103149, "text": "explored either in two stages [ 25 , 26 , 30 , 62 , 64 ] or a single", "bbox": [408.0, 345.0, 728.0, 359.0]}, {"polygon": [[63.0, 348.0], [382.0, 348.0], [382.0, 361.0], [63.0, 361.0]], "confidence": 0.9839075803756714, "text": "of the visual content that also names the person performing the", "bbox": [63.0, 348.0, 382.0, 361.0]}, {"polygon": [[409.0, 360.0], [728.0, 360.0], [728.0, 374.0], [409.0, 374.0]], "confidence": 0.9855021238327026, "text": "stage [ 7 , 9 , 14 , 34 , 42 , 45 , 50 , 51 , 62 , 65 , 68 , 83 ], depending on", "bbox": [409.0, 360.0, 728.0, 374.0]}, {"polygon": [[63.0, 363.0], [382.0, 363.0], [382.0, 377.0], [63.0, 377.0]], "confidence": 0.9840776920318604, "text": "actions. While this dataset is not a ground-truth AD dataset, we", "bbox": [63.0, 363.0, 382.0, 377.0]}, {"polygon": [[409.0, 376.0], [729.0, 376.0], [729.0, 389.0], [409.0, 389.0]], "confidence": 0.9842990040779114, "text": "whether localization and captioning are jointly addressed. Stan-", "bbox": [409.0, 376.0, 729.0, 389.0]}, {"polygon": [[63.0, 379.0], [382.0, 379.0], [382.0, 393.0], [63.0, 393.0]], "confidence": 0.9827232956886292, "text": "show that the pseudo ground-truth annotations are a valuable", "bbox": [63.0, 379.0, 382.0, 393.0]}, {"polygon": [[408.0, 391.0], [728.0, 391.0], [728.0, 405.0], [408.0, 405.0]], "confidence": 0.9821287989616394, "text": "dard evaluation benchmarks for dense captioning consist of web", "bbox": [408.0, 391.0, 728.0, 405.0]}, {"polygon": [[63.0, 394.0], [382.0, 394.0], [382.0, 408.0], [63.0, 408.0]], "confidence": 0.977495014667511, "text": "source of training data for AD. The statistics of these two new", "bbox": [63.0, 394.0, 382.0, 408.0]}, {"polygon": [[408.0, 406.0], [728.0, 406.0], [728.0, 421.0], [408.0, 421.0]], "confidence": 0.9832592010498047, "text": "videos such as YouCook2 [ 82 ], ActivityNet Captions [ 30 ], and", "bbox": [408.0, 406.0, 728.0, 421.0]}, {"polygon": [[63.0, 410.0], [384.0, 410.0], [384.0, 424.0], [63.0, 424.0]], "confidence": 0.9690070152282715, "text": "datasets are given in Table 1, and visually illustrated in Figure 1 .", "bbox": [63.0, 410.0, 384.0, 424.0]}, {"polygon": [[409.0, 422.0], [728.0, 422.0], [728.0, 436.0], [409.0, 436.0]], "confidence": 0.9686992764472961, "text": "VITT [ 23 ]. Recently, Vid2Seq [ 68 ] repurposed the narrated web", "bbox": [409.0, 422.0, 728.0, 436.0]}, {"polygon": [[79.0, 425.0], [383.0, 425.0], [383.0, 439.0], [79.0, 439.0]], "confidence": 0.9807519912719727, "text": "Our second contribution is a new architecture for AD", "bbox": [79.0, 425.0, 383.0, 439.0]}, {"polygon": [[409.0, 437.0], [728.0, 437.0], [728.0, 452.0], [409.0, 452.0]], "confidence": 0.9824024438858032, "text": "videos YT-Temporal-1B [ 75 ], using transcribed speech as the", "bbox": [409.0, 437.0, 728.0, 452.0]}, {"polygon": [[63.0, 440.0], [382.0, 440.0], [382.0, 455.0], [63.0, 455.0]], "confidence": 0.9771718978881836, "text": "generation that directly inputs a video clip and character bank.", "bbox": [63.0, 440.0, 382.0, 455.0]}, {"polygon": [[409.0, 454.0], [728.0, 454.0], [728.0, 467.0], [409.0, 467.0]], "confidence": 0.9827486872673035, "text": "supervision source. In a similar spirit, HowToCaption [ 53 ] was", "bbox": [409.0, 454.0, 728.0, 467.0]}, {"polygon": [[63.0, 457.0], [356.0, 457.0], [356.0, 470.0], [63.0, 470.0]], "confidence": 0.9657683968544006, "text": "proposals, and outputs a character-aware description.", "bbox": [63.0, 457.0, 356.0, 470.0]}, {"polygon": [[358.0, 459.0], [382.0, 459.0], [382.0, 469.0], [358.0, 469.0]], "confidence": 0.746736466884613, "text": "The", "bbox": [358.0, 459.0, 382.0, 469.0]}, {"polygon": [[408.0, 469.0], [728.0, 469.0], [728.0, 483.0], [408.0, 483.0]], "confidence": 0.9816686511039734, "text": "collected by transforming the narrations of HowTo100M [ 40 ]", "bbox": [408.0, 469.0, 728.0, 483.0]}, {"polygon": [[63.0, 472.0], [382.0, 472.0], [382.0, 486.0], [63.0, 486.0]], "confidence": 0.9801507592201233, "text": "model is based on the Q-former architecture of BLIP-2 [ 32 ] that", "bbox": [63.0, 472.0, 382.0, 486.0]}, {"polygon": [[409.0, 484.0], [728.0, 484.0], [728.0, 498.0], [409.0, 498.0]], "confidence": 0.9829546809196472, "text": "into caption-like descriptions using LLMs, and LaVila [ 80 ] cap-", "bbox": [409.0, 484.0, 728.0, 498.0]}, {"polygon": [[63.0, 487.0], [382.0, 487.0], [382.0, 501.0], [63.0, 501.0]], "confidence": 0.9843732118606567, "text": "bridges the visual space with the language space, then generates", "bbox": [63.0, 487.0, 382.0, 501.0]}, {"polygon": [[409.0, 499.0], [729.0, 499.0], [729.0, 513.0], [409.0, 513.0]], "confidence": 0.9838601350784302, "text": "tioned long videos to enable large-scale video-text pretraining,", "bbox": [409.0, 499.0, 729.0, 513.0]}, {"polygon": [[63.0, 503.0], [382.0, 503.0], [382.0, 516.0], [63.0, 516.0]], "confidence": 0.9834980964660645, "text": "textual outputs with a large language model [ 72 , 77 , 84 ]. Our", "bbox": [63.0, 503.0, 382.0, 516.0]}, {"polygon": [[409.0, 515.0], [729.0, 515.0], [729.0, 529.0], [409.0, 529.0]], "confidence": 0.9827520847320557, "text": "also leveraging LLMs. The distinction between AD generation", "bbox": [409.0, 515.0, 729.0, 529.0]}, {"polygon": [[63.0, 518.0], [382.0, 518.0], [382.0, 533.0], [63.0, 533.0]], "confidence": 0.9825267195701599, "text": "architecture is different from BLIP-2 [ 32 ] in that (i) it takes", "bbox": [63.0, 518.0, 382.0, 533.0]}, {"polygon": [[409.0, 531.0], [728.0, 531.0], [728.0, 544.0], [409.0, 544.0]], "confidence": 0.9832457900047302, "text": "and dense captioning lies in the former's focus on character", "bbox": [409.0, 531.0, 728.0, 544.0]}, {"polygon": [[63.0, 534.0], [382.0, 534.0], [382.0, 548.0], [63.0, 548.0]], "confidence": 0.9824636578559875, "text": "multi-frame movie clips as visual inputs, and (ii) it incorporates", "bbox": [63.0, 534.0, 382.0, 548.0]}, {"polygon": [[409.0, 546.0], [729.0, 546.0], [729.0, 561.0], [409.0, 561.0]], "confidence": 0.9826028347015381, "text": "names, story relevance, and avoidance of interference with", "bbox": [409.0, 546.0, 729.0, 561.0]}, {"polygon": [[63.0, 549.0], [383.0, 549.0], [383.0, 563.0], [63.0, 563.0]], "confidence": 0.983028769493103, "text": "character bank information both from the face exemplars and", "bbox": [63.0, 549.0, 383.0, 563.0]}, {"polygon": [[409.0, 563.0], [650.0, 563.0], [650.0, 576.0], [409.0, 576.0]], "confidence": 0.9781253337860107, "text": "important audio content ( e.g. character speech).", "bbox": [409.0, 563.0, 650.0, 576.0]}, {"polygon": [[63.0, 564.0], [167.0, 564.0], [167.0, 579.0], [63.0, 579.0]], "confidence": 0.9500709772109985, "text": "the character names.", "bbox": [63.0, 564.0, 167.0, 579.0]}, {"polygon": [[409.0, 578.0], [729.0, 578.0], [729.0, 593.0], [409.0, 593.0]], "confidence": 0.9812989234924316, "text": "Movie understanding datasets. For movie understanding,", "bbox": [409.0, 578.0, 729.0, 593.0]}, {"polygon": [[79.0, 580.0], [382.0, 580.0], [382.0, 593.0], [79.0, 593.0]], "confidence": 0.9734556078910828, "text": "Our third contribution is on evaluation. Previous methods", "bbox": [79.0, 580.0, 382.0, 593.0]}, {"polygon": [[409.0, 594.0], [728.0, 594.0], [728.0, 608.0], [409.0, 608.0]], "confidence": 0.983411967754364, "text": "current datasets facilitate a range of computer vision tasks", "bbox": [409.0, 594.0, 728.0, 608.0]}, {"polygon": [[63.0, 595.0], [384.0, 595.0], [384.0, 610.0], [63.0, 610.0]], "confidence": 0.9835814833641052, "text": "use a small test set of only 10 movies for evaluation. We intro-", "bbox": [63.0, 595.0, 384.0, 610.0]}, {"polygon": [[409.0, 609.0], [728.0, 609.0], [728.0, 623.0], [409.0, 623.0]], "confidence": 0.9828780889511108, "text": "including metadata classification [ 43 ], VQA [ 58 ], and visual", "bbox": [409.0, 609.0, 728.0, 623.0]}, {"polygon": [[63.0, 612.0], [383.0, 612.0], [383.0, 625.0], [63.0, 625.0]], "confidence": 0.983196496963501, "text": "duce an evaluation dataset of 100 movies, based on our aligned", "bbox": [63.0, 612.0, 383.0, 625.0]}, {"polygon": [[409.0, 625.0], [728.0, 625.0], [728.0, 639.0], [409.0, 639.0]], "confidence": 0.9807872772216797, "text": "character grounding [ 47 ]. These tasks often rely on auxiliary", "bbox": [409.0, 625.0, 728.0, 639.0]}, {"polygon": [[63.0, 627.0], [384.0, 627.0], [384.0, 641.0], [63.0, 641.0]], "confidence": 0.9815941452980042, "text": "movie clips with AD from AudioVault. This has far more diver-", "bbox": [63.0, 627.0, 384.0, 641.0]}, {"polygon": [[409.0, 640.0], [728.0, 640.0], [728.0, 655.0], [409.0, 655.0]], "confidence": 0.9828630089759827, "text": "data such as movie plots [ 67 ], book adaptations [ 57 ], or", "bbox": [409.0, 640.0, 728.0, 655.0]}, {"polygon": [[63.0, 643.0], [384.0, 643.0], [384.0, 657.0], [63.0, 657.0]], "confidence": 0.9832706451416016, "text": "sity than the previous test sets used, covering, e.g. science fiction,", "bbox": [63.0, 643.0, 384.0, 657.0]}, {"polygon": [[409.0, 656.0], [728.0, 656.0], [728.0, 670.0], [409.0, 670.0]], "confidence": 0.9797684550285339, "text": "AD [ 54 ] for dense annotations.  However, due to copyright", "bbox": [409.0, 656.0, 728.0, 670.0]}, {"polygon": [[63.0, 658.0], [384.0, 658.0], [384.0, 672.0], [63.0, 672.0]], "confidence": 0.9843716621398926, "text": "westerns, action, horror, cartoon, and romance. As well as intro-", "bbox": [63.0, 658.0, 384.0, 672.0]}, {"polygon": [[409.0, 672.0], [728.0, 672.0], [728.0, 685.0], [409.0, 685.0]], "confidence": 0.9848092794418335, "text": "constraints, many datasets are limited to offering visual features", "bbox": [409.0, 672.0, 728.0, 685.0]}, {"polygon": [[63.0, 674.0], [384.0, 674.0], [384.0, 688.0], [63.0, 688.0]], "confidence": 0.9842457175254822, "text": "ducing a new test set, we also adopt two new evaluation methods.", "bbox": [63.0, 674.0, 384.0, 688.0]}, {"polygon": [[408.0, 687.0], [728.0, 687.0], [728.0, 702.0], [408.0, 702.0]], "confidence": 0.9727328419685364, "text": "(MAD [ 54 ]) or sparse keyframes (MovieNet [ 24 ]). CMD [ 4 ]", "bbox": [408.0, 687.0, 728.0, 702.0]}, {"polygon": [[63.0, 689.0], [384.0, 689.0], [384.0, 703.0], [63.0, 703.0]], "confidence": 0.9834515452384949, "text": "For AD, the gold standard evaluation is to compare the gener-", "bbox": [63.0, 689.0, 384.0, 703.0]}, {"polygon": [[409.0, 703.0], [729.0, 703.0], [729.0, 717.0], [409.0, 717.0]], "confidence": 0.9836677312850952, "text": "circumvents this by providing urls to licensed YouTube clips.", "bbox": [409.0, 703.0, 729.0, 717.0]}, {"polygon": [[63.0, 704.0], [384.0, 704.0], [384.0, 718.0], [63.0, 718.0]], "confidence": 0.9831944704055786, "text": "ated AD with that provided by humans. For model development,", "bbox": [63.0, 704.0, 384.0, 718.0]}, {"polygon": [[409.0, 718.0], [729.0, 717.0], [729.0, 731.0], [409.0, 733.0]], "confidence": 0.9802967309951782, "text": "AutoAD [ 20 ] improves on automatic AD collection, and", "bbox": [409.0, 718.0, 729.0, 731.0]}, {"polygon": [[63.0, 720.0], [383.0, 720.0], [383.0, 733.0], [63.0, 733.0]], "confidence": 0.9841645359992981, "text": "however, an automatic scalable evaluation is required. Previous", "bbox": [63.0, 720.0, 383.0, 733.0]}, {"polygon": [[409.0, 734.0], [590.0, 734.0], [590.0, 748.0], [409.0, 748.0]], "confidence": 0.9681050777435303, "text": "provides large-scale audio AD data.", "bbox": [409.0, 734.0, 590.0, 748.0]}, {"polygon": [[63.0, 735.0], [382.0, 735.0], [382.0, 748.0], [63.0, 748.0]], "confidence": 0.979329526424408, "text": "works have used captioning metrics such as CIDEr [ 61 ] but these", "bbox": [63.0, 735.0, 382.0, 748.0]}, {"polygon": [[63.0, 750.0], [383.0, 750.0], [383.0, 764.0], [63.0, 764.0]], "confidence": 0.9817164540290833, "text": "have severe limitations since they essentially measure n-gram", "bbox": [63.0, 750.0, 383.0, 764.0]}, {"polygon": [[409.0, 750.0], [728.0, 750.0], [728.0, 764.0], [409.0, 764.0]], "confidence": 0.9814693927764893, "text": "Improvements in VLMs for images and videos. The recent", "bbox": [409.0, 750.0, 728.0, 764.0]}, {"polygon": [[409.0, 765.0], [728.0, 765.0], [728.0, 780.0], [409.0, 780.0]], "confidence": 0.9835407733917236, "text": "success of LLMs [ 12 , 59 , 60 , 78 ] and vision encoders [ 15 , 44 ]", "bbox": [409.0, 765.0, 728.0, 780.0]}, {"polygon": [[63.0, 767.0], [383.0, 767.0], [383.0, 780.0], [63.0, 780.0]], "confidence": 0.9834206104278564, "text": "accuracy, and the same semantic AD can be presented in multi-", "bbox": [63.0, 767.0, 383.0, 780.0]}, {"polygon": [[63.0, 781.0], [382.0, 781.0], [382.0, 795.0], [63.0, 795.0]], "confidence": 0.9843471646308899, "text": "ple equivalent ways. To deal with this problem, [ 21 ] introduced", "bbox": [63.0, 781.0, 382.0, 795.0]}, {"polygon": [[409.0, 781.0], [730.0, 781.0], [730.0, 795.0], [409.0, 795.0]], "confidence": 0.9830942153930664, "text": "has led to an explosion of multimodal (vision and language)", "bbox": [409.0, 781.0, 730.0, 795.0]}, {"polygon": [[63.0, 797.0], [382.0, 797.0], [382.0, 811.0], [63.0, 811.0]], "confidence": 0.9737622737884521, "text": "a retrieval-based assessment, evaluating how often we can pick.", "bbox": [63.0, 797.0, 382.0, 811.0]}, {"polygon": [[409.0, 797.0], [729.0, 797.0], [729.0, 811.0], [409.0, 811.0]], "confidence": 0.9836609363555908, "text": "models that can jointly understand both vision and text data.", "bbox": [409.0, 797.0, 729.0, 811.0]}, {"polygon": [[63.0, 813.0], [384.0, 813.0], [384.0, 826.0], [63.0, 826.0]], "confidence": 0.9832624197006226, "text": "out the correct AD out of multiple neighboring ADs by compar-", "bbox": [63.0, 813.0, 384.0, 826.0]}, {"polygon": [[409.0, 813.0], [728.0, 813.0], [728.0, 826.0], [409.0, 826.0]], "confidence": 0.982549250125885, "text": "These methods largely work by mapping frozen image encoders", "bbox": [409.0, 813.0, 728.0, 826.0]}, {"polygon": [[63.0, 828.0], [383.0, 828.0], [383.0, 843.0], [63.0, 843.0]], "confidence": 0.9827228784561157, "text": "ing them to the generated AD using BertScore [ 79 ] semantic text", "bbox": [63.0, 828.0, 383.0, 843.0]}, {"polygon": [[408.0, 828.0], [728.0, 828.0], [728.0, 843.0], [408.0, 843.0]], "confidence": 0.9775850772857666, "text": "(e.g. CLIP [ 44 ], EVA-CLIP [ 16 ]) to the textual embedding", "bbox": [408.0, 828.0, 728.0, 843.0]}, {"polygon": [[409.0, 843.0], [730.0, 843.0], [730.0, 858.0], [409.0, 858.0]], "confidence": 0.9815539717674255, "text": "space of frozen LLMs [ 59 , 60 , 78 ], for example Flamingo [ 1 ],", "bbox": [409.0, 843.0, 730.0, 858.0]}, {"polygon": [[63.0, 844.0], [382.0, 844.0], [382.0, 858.0], [63.0, 858.0]], "confidence": 0.9827082753181458, "text": "similarity. In this work we adopt two new measures. The first", "bbox": [63.0, 844.0, 382.0, 858.0]}, {"polygon": [[63.0, 859.0], [384.0, 859.0], [384.0, 873.0], [63.0, 873.0]], "confidence": 0.9829601645469666, "text": "called CRITIC, addresses one essential element of AD that dis-", "bbox": [63.0, 859.0, 384.0, 873.0]}, {"polygon": [[408.0, 859.0], [730.0, 859.0], [730.0, 873.0], [408.0, 873.0]], "confidence": 0.9829165935516357, "text": "which does so via a Perceiver resampler [ 27 ], or BLIP2 [ 32 ],", "bbox": [408.0, 859.0, 730.0, 873.0]}, {"polygon": [[63.0, 874.0], [382.0, 874.0], [382.0, 888.0], [63.0, 888.0]], "confidence": 0.9788875579833984, "text": "tinguishes it from standard video captioning – that it must name", "bbox": [63.0, 874.0, 382.0, 888.0]}, {"polygon": [[409.0, 874.0], [729.0, 874.0], [729.0, 888.0], [409.0, 888.0]], "confidence": 0.9737494587898254, "text": "which uses a Q-former to achieve a similar mapping.  Video-", "bbox": [409.0, 874.0, 729.0, 888.0]}, {"polygon": [[63.0, 890.0], [383.0, 890.0], [383.0, 903.0], [63.0, 903.0]], "confidence": 0.9833216667175293, "text": "the characters involved. The second measure follows the recent", "bbox": [63.0, 890.0, 383.0, 903.0]}, {"polygon": [[408.0, 890.0], [729.0, 890.0], [729.0, 903.0], [408.0, 903.0]], "confidence": 0.9813250303268433, "text": "LLama [ 77 ] extends this idea to the audiovisual domain, by us-", "bbox": [408.0, 890.0, 729.0, 903.0]}, {"polygon": [[63.0, 905.0], [384.0, 905.0], [384.0, 920.0], [63.0, 920.0]], "confidence": 0.9846106171607971, "text": "trend in using LLMs to assess the veracity of captioning [ 8 , 11 ,", "bbox": [63.0, 905.0, 384.0, 920.0]}, {"polygon": [[408.0, 905.0], [728.0, 905.0], [728.0, 920.0], [408.0, 920.0]], "confidence": 0.9814097881317139, "text": "ing the multimodal ImageBind [ 18 ] encoder in conjunction with", "bbox": [408.0, 905.0, 728.0, 920.0]}, {"polygon": [[65.0, 922.0], [384.0, 922.0], [384.0, 935.0], [65.0, 935.0]], "confidence": 0.9833787679672241, "text": "39 , 55 , 81 ] As an exemplar of the usefulness of these new mea-", "bbox": [65.0, 922.0, 384.0, 935.0]}, {"polygon": [[409.0, 922.0], [728.0, 922.0], [728.0, 935.0], [409.0, 935.0]], "confidence": 0.978386402130127, "text": "video and audio Q-formers. While MV-GPT [ 48 ] finetunes a", "bbox": [409.0, 922.0, 728.0, 935.0]}, {"polygon": [[63.0, 937.0], [382.0, 937.0], [382.0, 950.0], [63.0, 950.0]], "confidence": 0.9814755916595459, "text": "sures we also use them to assess inter-rater consistency where", "bbox": [63.0, 937.0, 382.0, 950.0]}, {"polygon": [[409.0, 937.0], [728.0, 937.0], [728.0, 951.0], [409.0, 951.0]], "confidence": 0.98017817735672, "text": "native video backbone [ 3 ] for the task of video captioning, most", "bbox": [409.0, 937.0, 728.0, 951.0]}], "languages": ["en"], "image_bbox": [0.0, 0.0, 816.0, 1056.0], "page": 2}, {"text_lines": [{"polygon": [[63.0, 96.0], [384.0, 96.0], [384.0, 111.0], [63.0, 111.0]], "confidence": 0.9836693406105042, "text": "works adapt image encoders to the costly video domain via tem-", "bbox": [63.0, 96.0, 384.0, 111.0]}, {"polygon": [[514.0, 100.0], [536.0, 100.0], [536.0, 111.0], [514.0, 111.0]], "confidence": 0.8235368132591248, "text": "Split", "bbox": [514.0, 100.0, 536.0, 111.0]}, {"polygon": [[564.0, 100.0], [605.0, 100.0], [605.0, 111.0], [564.0, 111.0]], "confidence": 0.8702473044395447, "text": "# movies", "bbox": [564.0, 100.0, 605.0, 111.0]}, {"polygon": [[618.0, 100.0], [646.0, 100.0], [646.0, 111.0], [618.0, 111.0]], "confidence": 0.7392697930335999, "text": "# AD", "bbox": [618.0, 100.0, 646.0, 111.0]}, {"polygon": [[63.0, 112.0], [383.0, 112.0], [383.0, 127.0], [63.0, 127.0]], "confidence": 0.98457932472229, "text": "porally sampling a few frames with large strides [ 10 , 63 ], or by", "bbox": [63.0, 112.0, 383.0, 127.0]}, {"polygon": [[489.0, 117.0], [560.0, 117.0], [560.0, 129.0], [489.0, 129.0]], "confidence": 0.9151748418807983, "text": "CMD-AD-Train", "bbox": [489.0, 117.0, 560.0, 129.0]}, {"polygon": [[582.0, 117.0], [605.0, 117.0], [605.0, 129.0], [582.0, 129.0]], "confidence": 0.6875411868095398, "text": "1332", "bbox": [582.0, 117.0, 605.0, 129.0]}, {"polygon": [[613.0, 117.0], [645.0, 117.0], [645.0, 129.0], [613.0, 129.0]], "confidence": 0.8279208540916443, "text": "93,952", "bbox": [613.0, 117.0, 645.0, 129.0]}, {"polygon": [[63.0, 128.0], [382.0, 128.0], [382.0, 142.0], [63.0, 142.0]], "confidence": 0.9844161868095398, "text": "representing each frame by a single token [ 65 , 68 , 83 ]. Given the", "bbox": [63.0, 128.0, 382.0, 142.0]}, {"polygon": [[490.0, 131.0], [558.0, 131.0], [558.0, 143.0], [490.0, 143.0]], "confidence": 0.9085305333137512, "text": "CMD-AD-Eval", "bbox": [490.0, 131.0, 558.0, 143.0]}, {"polygon": [[586.0, 132.0], [605.0, 132.0], [605.0, 142.0], [586.0, 142.0]], "confidence": 0.6944026947021484, "text": "100", "bbox": [586.0, 132.0, 605.0, 142.0]}, {"polygon": [[618.0, 132.0], [644.0, 132.0], [644.0, 142.0], [618.0, 142.0]], "confidence": 0.8295527100563049, "text": "7,316", "bbox": [618.0, 132.0, 644.0, 142.0]}, {"polygon": [[582.0, 143.0], [644.0, 143.0], [644.0, 155.0], [582.0, 155.0]], "confidence": 0.9171575307846069, "text": "1432 101,268", "bbox": [582.0, 143.0, 644.0, 155.0]}, {"polygon": [[63.0, 144.0], [382.0, 144.0], [382.0, 157.0], [63.0, 157.0]], "confidence": 0.9843610525131226, "text": "impressive generalisation capabilities of these works made of up", "bbox": [63.0, 144.0, 382.0, 157.0]}, {"polygon": [[514.0, 145.0], [535.0, 145.0], [535.0, 155.0], [514.0, 155.0]], "confidence": 0.7868773937225342, "text": "total", "bbox": [514.0, 145.0, 535.0, 155.0]}, {"polygon": [[63.0, 159.0], [384.0, 159.0], [384.0, 174.0], [63.0, 174.0]], "confidence": 0.9844302535057068, "text": "strong frozen components [ 73 ], we also adopt a similar approach,", "bbox": [63.0, 159.0, 384.0, 174.0]}, {"polygon": [[465.0, 163.0], [671.0, 163.0], [671.0, 177.0], [465.0, 177.0]], "confidence": 0.9736467599868774, "text": "Table 2. Statistics of the CMD-AD dataset.", "bbox": [465.0, 163.0, 671.0, 177.0]}, {"polygon": [[63.0, 175.0], [383.0, 175.0], [383.0, 189.0], [63.0, 189.0]], "confidence": 0.9771447777748108, "text": "leveraging Video-LLama [ 77 ] and BLIP- 2 [ 32 ] models as our", "bbox": [63.0, 175.0, 383.0, 189.0]}, {"polygon": [[409.0, 183.0], [729.0, 185.0], [729.0, 200.0], [409.0, 198.0]], "confidence": 0.9813944697380066, "text": "soundtracks from AudioVault audio files have been modified", "bbox": [409.0, 183.0, 729.0, 200.0]}, {"polygon": [[63.0, 190.0], [383.0, 190.0], [383.0, 205.0], [63.0, 205.0]], "confidence": 0.9836366176605225, "text": "backbone, with the key addition that we also integrate charac-", "bbox": [63.0, 190.0, 383.0, 205.0]}, {"polygon": [[409.0, 200.0], [728.0, 200.0], [728.0, 213.0], [409.0, 213.0]], "confidence": 0.9825579524040222, "text": "and re-encoded to add the AD, therefore the audio signals from", "bbox": [409.0, 200.0, 728.0, 213.0]}, {"polygon": [[63.0, 205.0], [384.0, 205.0], [384.0, 220.0], [63.0, 220.0]], "confidence": 0.9822656512260437, "text": "ter information. More recent works such as MiniGPT-4 [ 84 ],", "bbox": [63.0, 205.0, 384.0, 220.0]}, {"polygon": [[410.0, 215.0], [728.0, 215.0], [728.0, 230.0], [410.0, 230.0]], "confidence": 0.98371821641922, "text": "AudioVault files and CMD movie clips are not identical; second,", "bbox": [410.0, 215.0, 728.0, 230.0]}, {"polygon": [[63.0, 221.0], [384.0, 221.0], [384.0, 235.0], [63.0, 235.0]], "confidence": 0.9810107350349426, "text": "MovieChat [ 55 ] and VideoBLIP [ 72 ] use stronger instruction-", "bbox": [63.0, 221.0, 384.0, 235.0]}, {"polygon": [[410.0, 232.0], [728.0, 232.0], [728.0, 245.0], [410.0, 245.0]], "confidence": 0.9843217134475708, "text": "AudioVault audio files cover the full movie duration (e.g. around", "bbox": [410.0, 232.0, 728.0, 245.0]}, {"polygon": [[63.0, 236.0], [327.0, 236.0], [327.0, 251.0], [63.0, 251.0]], "confidence": 0.9806737899780273, "text": "tuned LLMs, enabling further zero-shot capabilities.", "bbox": [63.0, 236.0, 327.0, 251.0]}, {"polygon": [[409.0, 247.0], [729.0, 247.0], [729.0, 261.0], [409.0, 261.0]], "confidence": 0.9808293581008911, "text": "90 minutes), whilst a CMD clip covers only 2 minutes, and per-", "bbox": [409.0, 247.0, 729.0, 261.0]}, {"polygon": [[63.0, 253.0], [383.0, 253.0], [383.0, 266.0], [63.0, 266.0]], "confidence": 0.9833197593688965, "text": "Captioning evaluation. Human evaluation is the gold standard", "bbox": [63.0, 253.0, 383.0, 266.0]}, {"polygon": [[409.0, 262.0], [728.0, 262.0], [728.0, 277.0], [409.0, 277.0]], "confidence": 0.9839280843734741, "text": "forming precise alignment over the extent of the movie has the", "bbox": [409.0, 262.0, 728.0, 277.0]}, {"polygon": [[63.0, 268.0], [384.0, 268.0], [384.0, 282.0], [63.0, 282.0]], "confidence": 0.9844965934753418, "text": "for judging caption quality, however it requires multiple annota-", "bbox": [63.0, 268.0, 384.0, 282.0]}, {"polygon": [[409.0, 278.0], [729.0, 278.0], [729.0, 291.0], [409.0, 291.0]], "confidence": 0.9836266040802002, "text": "potential for many erroneous matches across the search space;", "bbox": [409.0, 278.0, 729.0, 291.0]}, {"polygon": [[63.0, 284.0], [384.0, 284.0], [384.0, 298.0], [63.0, 298.0]], "confidence": 0.984420657157898, "text": "tors for consistency, is expensive and exceptionally slow. Exist-", "bbox": [63.0, 284.0, 384.0, 298.0]}, {"polygon": [[409.0, 293.0], [728.0, 293.0], [728.0, 308.0], [409.0, 308.0]], "confidence": 0.9833798408508301, "text": "third, the same movie published in different locations might", "bbox": [409.0, 293.0, 728.0, 308.0]}, {"polygon": [[63.0, 299.0], [384.0, 299.0], [384.0, 313.0], [63.0, 313.0]], "confidence": 0.9774560332298279, "text": "ing automatic metrics, such as BLEU, ROUGE and CIDEr [ 61 ],", "bbox": [63.0, 299.0, 384.0, 313.0]}, {"polygon": [[409.0, 308.0], [729.0, 308.0], [729.0, 322.0], [409.0, 322.0]], "confidence": 0.9831944108009338, "text": "have been recorded at different speeds (e.g. NTSC 29.97 fps vs.", "bbox": [409.0, 308.0, 729.0, 322.0]}, {"polygon": [[63.0, 315.0], [383.0, 315.0], [383.0, 329.0], [63.0, 329.0]], "confidence": 0.9820080995559692, "text": "all primarily measure n-gram overlap (however have different", "bbox": [63.0, 315.0, 383.0, 329.0]}, {"polygon": [[409.0, 324.0], [729.0, 322.0], [729.0, 337.0], [409.0, 339.0]], "confidence": 0.9782695174217224, "text": "PAL 25 fps 3 ), introducing another unknown into the alignment.", "bbox": [409.0, 324.0, 729.0, 337.0]}, {"polygon": [[63.0, 330.0], [383.0, 330.0], [383.0, 344.0], [63.0, 344.0]], "confidence": 0.9827455878257751, "text": "weighting schemes between n-grams, and across precision/re-", "bbox": [63.0, 330.0, 383.0, 344.0]}, {"polygon": [[425.0, 339.0], [728.0, 339.0], [728.0, 353.0], [425.0, 353.0]], "confidence": 0.9808928370475769, "text": "We propose a two-stage alignment pipeline to overcome", "bbox": [425.0, 339.0, 728.0, 353.0]}, {"polygon": [[63.0, 345.0], [384.0, 345.0], [384.0, 359.0], [63.0, 359.0]], "confidence": 0.9841196537017822, "text": "call), and do not capture the inherent subjectivity of the task,", "bbox": [63.0, 345.0, 384.0, 359.0]}, {"polygon": [[409.0, 355.0], [728.0, 355.0], [728.0, 369.0], [409.0, 369.0]], "confidence": 0.9829849600791931, "text": "these challenges and get precise temporal alignment between", "bbox": [409.0, 355.0, 728.0, 369.0]}, {"polygon": [[63.0, 361.0], [383.0, 361.0], [383.0, 375.0], [63.0, 375.0]], "confidence": 0.983549177646637, "text": "where different phrasing is often equally valid. Other metrics", "bbox": [63.0, 361.0, 383.0, 375.0]}, {"polygon": [[408.0, 370.0], [728.0, 370.0], [728.0, 385.0], [408.0, 385.0]], "confidence": 0.9813500046730042, "text": "hour-long AudioVault audio files and non-contiguous short", "bbox": [408.0, 370.0, 728.0, 385.0]}, {"polygon": [[63.0, 376.0], [382.0, 376.0], [382.0, 390.0], [63.0, 390.0]], "confidence": 0.9809162020683289, "text": "include SPICE [ 2 ] (adds action and object relationships), while", "bbox": [63.0, 376.0, 382.0, 390.0]}, {"polygon": [[408.0, 387.0], [729.0, 387.0], [729.0, 400.0], [408.0, 400.0]], "confidence": 0.9808094501495361, "text": "CMD movie clips from the same movie.  To achieve this,", "bbox": [408.0, 387.0, 729.0, 400.0]}, {"polygon": [[63.0, 392.0], [383.0, 392.0], [383.0, 406.0], [63.0, 406.0]], "confidence": 0.982554018497467, "text": "model-based metrics using earlier language models or image-", "bbox": [63.0, 392.0, 383.0, 406.0]}, {"polygon": [[409.0, 402.0], [728.0, 402.0], [728.0, 415.0], [409.0, 415.0]], "confidence": 0.973150908946991, "text": "we use two quasi-independent modalities:  (i) the transcribed", "bbox": [409.0, 402.0, 728.0, 415.0]}, {"polygon": [[63.0, 408.0], [383.0, 408.0], [383.0, 421.0], [63.0, 421.0]], "confidence": 0.9784575700759888, "text": "language models include BERT-Score [ 79 ], BERT-Score++ [ 70 ]", "bbox": [63.0, 408.0, 383.0, 421.0]}, {"polygon": [[409.0, 417.0], [728.0, 417.0], [728.0, 431.0], [409.0, 431.0]], "confidence": 0.9838337302207947, "text": "spoken text from the characters (not the AD), and (ii) the raw", "bbox": [409.0, 417.0, 728.0, 431.0]}, {"polygon": [[63.0, 423.0], [384.0, 423.0], [384.0, 438.0], [63.0, 438.0]], "confidence": 0.977477490901947, "text": "(fine-tunes BERT for image captioning), LEIC [ 13 ] and NU-", "bbox": [63.0, 423.0, 384.0, 438.0]}, {"polygon": [[409.0, 432.0], [728.0, 432.0], [728.0, 446.0], [409.0, 446.0]], "confidence": 0.9828305244445801, "text": "audio signal containing both non-speech sounds (music, sound", "bbox": [409.0, 432.0, 728.0, 446.0]}, {"polygon": [[63.0, 439.0], [384.0, 439.0], [384.0, 453.0], [63.0, 453.0]], "confidence": 0.9706351161003113, "text": "BIA [ 29 ] (custom trained models for image caption evaluation),", "bbox": [63.0, 439.0, 384.0, 453.0]}, {"polygon": [[408.0, 447.0], [529.0, 447.0], [529.0, 462.0], [408.0, 462.0]], "confidence": 0.9589889645576477, "text": "effects) and the speech.", "bbox": [408.0, 447.0, 529.0, 462.0]}, {"polygon": [[63.0, 454.0], [382.0, 454.0], [382.0, 469.0], [63.0, 469.0]], "confidence": 0.9780095219612122, "text": "TIGEr [ 28 ], CLIPScore [ 22 ], and EMScore [ 52 ].  Given the", "bbox": [63.0, 454.0, 382.0, 469.0]}, {"polygon": [[63.0, 469.0], [382.0, 469.0], [382.0, 484.0], [63.0, 484.0]], "confidence": 0.9821745157241821, "text": "explosion of LLMs, however, recent works explore the use", "bbox": [63.0, 469.0, 382.0, 484.0]}, {"polygon": [[409.0, 470.0], [728.0, 470.0], [728.0, 484.0], [409.0, 484.0]], "confidence": 0.9782528281211853, "text": "Stage1: Text-text alignment. The aim in this stage is to first", "bbox": [409.0, 470.0, 728.0, 484.0]}, {"polygon": [[63.0, 485.0], [382.0, 485.0], [382.0, 498.0], [63.0, 498.0]], "confidence": 0.9821031093597412, "text": "of state-of-the-art LLMs, such as GPT-4, as a surrogate for", "bbox": [63.0, 485.0, 382.0, 498.0]}, {"polygon": [[408.0, 486.0], [728.0, 486.0], [728.0, 500.0], [408.0, 500.0]], "confidence": 0.9816406965255737, "text": "roughly localize the CMD movie clip with the AudioVault", "bbox": [408.0, 486.0, 728.0, 500.0]}, {"polygon": [[63.0, 500.0], [384.0, 500.0], [384.0, 514.0], [63.0, 514.0]], "confidence": 0.9749904274940491, "text": "humans.  Because these models are often trained with RLHF,", "bbox": [63.0, 500.0, 384.0, 514.0]}, {"polygon": [[409.0, 502.0], [728.0, 502.0], [728.0, 515.0], [409.0, 515.0]], "confidence": 0.9827482104301453, "text": "audio to reduce the search space. In detail, we use WhisperX [ 5 ]", "bbox": [409.0, 502.0, 728.0, 515.0]}, {"polygon": [[63.0, 515.0], [382.0, 515.0], [382.0, 529.0], [63.0, 529.0]], "confidence": 0.9830008745193481, "text": "they already exhibit strong human alignment [ 6 ], and can be", "bbox": [63.0, 515.0, 382.0, 529.0]}, {"polygon": [[409.0, 517.0], [728.0, 517.0], [728.0, 530.0], [409.0, 530.0]], "confidence": 0.9822016954421997, "text": "with the diarization module to separate the AD narration", "bbox": [409.0, 517.0, 728.0, 530.0]}, {"polygon": [[63.0, 531.0], [382.0, 531.0], [382.0, 545.0], [63.0, 545.0]], "confidence": 0.9838181734085083, "text": "used to assess text quality well (LLM-as-a-judge). [ 11 , 81 ] show", "bbox": [63.0, 531.0, 382.0, 545.0]}, {"polygon": [[409.0, 532.0], [728.0, 532.0], [728.0, 547.0], [409.0, 547.0]], "confidence": 0.983315110206604, "text": "from the character speech, and obtain movie 'subtitles' with", "bbox": [409.0, 532.0, 728.0, 547.0]}, {"polygon": [[63.0, 546.0], [382.0, 546.0], [382.0, 561.0], [63.0, 561.0]], "confidence": 0.9830919504165649, "text": "that using strong LLMs as judges (such as GPT-4) aligns highly", "bbox": [63.0, 546.0, 382.0, 561.0]}, {"polygon": [[409.0, 549.0], [728.0, 549.0], [728.0, 562.0], [409.0, 562.0]], "confidence": 0.9799666404724121, "text": "timestamps for both AudioVault audio and CMD movie", "bbox": [409.0, 549.0, 728.0, 562.0]}, {"polygon": [[63.0, 562.0], [382.0, 562.0], [382.0, 576.0], [63.0, 576.0]], "confidence": 0.9811514616012573, "text": "with human preferences on a range of standard language-based", "bbox": [63.0, 562.0, 382.0, 576.0]}, {"polygon": [[408.0, 564.0], [729.0, 564.0], [729.0, 579.0], [408.0, 579.0]], "confidence": 0.9384890794754028, "text": "clips. These are denoted as S AV = {( s 1 ,t 1 ),...,( s m,t m )} and", "bbox": [408.0, 564.0, 729.0, 579.0]}, {"polygon": [[63.0, 577.0], [383.0, 577.0], [383.0, 591.0], [63.0, 591.0]], "confidence": 0.9777610301971436, "text": "tasks, such as conversational instruction following. CLAIR [ 8 ]", "bbox": [63.0, 577.0, 383.0, 591.0]}, {"polygon": [[409.0, 578.0], [728.0, 578.0], [728.0, 594.0], [409.0, 594.0]], "confidence": 0.9231085777282715, "text": "S CMD = {( s ′ 1 , t ′ 1 ),...,( s ′ n , t ′ n )} , where each s i denotes subtitle", "bbox": [409.0, 578.0, 728.0, 594.0]}, {"polygon": [[63.0, 593.0], [382.0, 593.0], [382.0, 608.0], [63.0, 608.0]], "confidence": 0.9836532473564148, "text": "extends this idea to image captioning, showing similar strong", "bbox": [63.0, 593.0, 382.0, 608.0]}, {"polygon": [[409.0, 594.0], [729.0, 594.0], [729.0, 608.0], [409.0, 608.0]], "confidence": 0.9781050086021423, "text": "strings and t i denotes the temporal extent of this subtitle.", "bbox": [409.0, 594.0, 729.0, 608.0]}, {"polygon": [[63.0, 608.0], [382.0, 608.0], [382.0, 622.0], [63.0, 622.0]], "confidence": 0.9830074310302734, "text": "correlations to human preferences on visual-language datasets", "bbox": [63.0, 608.0, 382.0, 622.0]}, {"polygon": [[409.0, 609.0], [728.0, 609.0], [728.0, 624.0], [409.0, 624.0]], "confidence": 0.9687454700469971, "text": "Note that n ≪ m because CMD movie clips are much shorter", "bbox": [409.0, 609.0, 728.0, 624.0]}, {"polygon": [[63.0, 623.0], [382.0, 623.0], [382.0, 638.0], [63.0, 638.0]], "confidence": 0.9790090918540955, "text": "such as MS-COCO and Flickr8K, while VideoChatGPT [ 39 ]", "bbox": [63.0, 623.0, 382.0, 638.0]}, {"polygon": [[409.0, 626.0], [728.0, 626.0], [728.0, 639.0], [409.0, 639.0]], "confidence": 0.9838216304779053, "text": "than the entire movie, also the subtitles from the two sources", "bbox": [409.0, 626.0, 728.0, 639.0]}, {"polygon": [[63.0, 639.0], [383.0, 639.0], [383.0, 653.0], [63.0, 653.0]], "confidence": 0.9819180965423584, "text": "and MovieChat [ 55 ] use LLM-assisted evaluation for video", "bbox": [63.0, 639.0, 383.0, 653.0]}, {"polygon": [[409.0, 641.0], [728.0, 641.0], [728.0, 655.0], [409.0, 655.0]], "confidence": 0.9830999970436096, "text": "are different because of arbitrary sentence partitioning by", "bbox": [409.0, 641.0, 728.0, 655.0]}, {"polygon": [[63.0, 655.0], [223.0, 655.0], [223.0, 669.0], [63.0, 669.0]], "confidence": 0.9671359062194824, "text": "tasks such as videoQA as well.", "bbox": [63.0, 655.0, 223.0, 669.0]}, {"polygon": [[409.0, 656.0], [728.0, 656.0], [728.0, 670.0], [409.0, 670.0]], "confidence": 0.9831975698471069, "text": "WhisperX or possible diarization errors. To localize the CMD", "bbox": [409.0, 656.0, 728.0, 670.0]}, {"polygon": [[408.0, 672.0], [728.0, 672.0], [728.0, 685.0], [408.0, 685.0]], "confidence": 0.9830459356307983, "text": "clip on the AudioVault movie time axis, we compute a simple", "bbox": [408.0, 672.0, 728.0, 685.0]}, {"polygon": [[63.0, 687.0], [276.0, 687.0], [276.0, 702.0], [63.0, 702.0]], "confidence": 0.9691293239593506, "text": "3. New Datasets for Pixels to AD", "bbox": [63.0, 687.0, 276.0, 702.0]}, {"polygon": [[409.0, 687.0], [728.0, 687.0], [728.0, 702.0], [409.0, 702.0]], "confidence": 0.9809585213661194, "text": "word-error-rate (WER) using a sliding window approach as", "bbox": [409.0, 687.0, 728.0, 702.0]}, {"polygon": [[409.0, 703.0], [728.0, 703.0], [728.0, 717.0], [409.0, 717.0]], "confidence": 0.9762474298477173, "text": "follows: we combine the CMD subtitles into a paragraph", "bbox": [409.0, 703.0, 728.0, 717.0]}, {"polygon": [[63.0, 714.0], [383.0, 714.0], [383.0, 727.0], [63.0, 727.0]], "confidence": 0.9838623404502869, "text": "In this section, we describe our two new datasets that contain", "bbox": [63.0, 714.0, 383.0, 727.0]}, {"polygon": [[409.0, 719.0], [729.0, 716.0], [729.0, 731.0], [410.0, 734.0]], "confidence": 0.9029933214187622, "text": "Pcmd = [ S ′ 1 ;…;s ′ n ], then compute WER with AudioVault sub-", "bbox": [409.0, 719.0, 729.0, 731.0]}, {"polygon": [[63.0, 729.0], [383.0, 729.0], [383.0, 743.0], [63.0, 743.0]], "confidence": 0.9737545251846313, "text": "raw video pixels mapped to AD annotation:  CMD-AD", "bbox": [63.0, 729.0, 383.0, 743.0]}, {"polygon": [[409.0, 733.0], [727.0, 735.0], [727.0, 751.0], [409.0, 750.0]], "confidence": 0.8901935815811157, "text": "titles within a chunk size of n . Formally, let P A i ′ = [ s i ;...; s i + n ]", "bbox": [409.0, 733.0, 727.0, 751.0]}, {"polygon": [[63.0, 744.0], [383.0, 744.0], [383.0, 759.0], [63.0, 759.0]], "confidence": 0.9722850918769836, "text": "(Sec. 3.1 ) which is based on CMD [ 4 ], and HowTo-AD", "bbox": [63.0, 744.0, 383.0, 759.0]}, {"polygon": [[409.0, 752.0], [728.0, 752.0], [728.0, 766.0], [409.0, 766.0]], "confidence": 0.9796610474586487, "text": "denote the AudioVault subtitle paragraph consisting of n", "bbox": [409.0, 752.0, 728.0, 766.0]}, {"polygon": [[63.0, 760.0], [261.0, 760.0], [261.0, 774.0], [63.0, 774.0]], "confidence": 0.9687077403068542, "text": "(Sec. 3.2 ) based on HowTo100M [ 40 ].", "bbox": [63.0, 760.0, 261.0, 774.0]}, {"polygon": [[409.0, 767.0], [728.0, 767.0], [728.0, 782.0], [409.0, 782.0]], "confidence": 0.9564076066017151, "text": "continuous subtitle entries starting from -th subtitles. For a", "bbox": [409.0, 767.0, 728.0, 782.0]}, {"polygon": [[409.0, 783.0], [709.0, 783.0], [709.0, 797.0], [409.0, 797.0]], "confidence": 0.9827691912651062, "text": "particular CMD clip, the objective of text-text alignment is", "bbox": [409.0, 783.0, 709.0, 797.0]}, {"polygon": [[63.0, 786.0], [330.0, 786.0], [330.0, 802.0], [63.0, 802.0]], "confidence": 0.9684386253356934, "text": "3.1. CMD-AD – Pixels from Aligned CMD", "bbox": [63.0, 786.0, 330.0, 802.0]}, {"polygon": [[468.0, 811.0], [665.0, 811.0], [665.0, 828.0], [468.0, 828.0]], "confidence": 0.8259115815162659, "text": "T ti-align = argmin  WER( P CMD , P ( V )", "bbox": [468.0, 811.0, 665.0, 828.0]}, {"polygon": [[63.0, 813.0], [383.0, 813.0], [383.0, 826.0], [63.0, 826.0]], "confidence": 0.9805875420570374, "text": "The AudioVault website provides human annotated Audio", "bbox": [63.0, 813.0, 383.0, 826.0]}, {"polygon": [[709.0, 813.0], [730.0, 813.0], [730.0, 827.0], [709.0, 827.0]], "confidence": 0.6975652575492859, "text": "(1)", "bbox": [709.0, 813.0, 730.0, 827.0]}, {"polygon": [[63.0, 828.0], [383.0, 828.0], [383.0, 842.0], [63.0, 842.0]], "confidence": 0.980853796005249, "text": "Descriptions in the form of audio files with the spoken AD", "bbox": [63.0, 828.0, 383.0, 842.0]}, {"polygon": [[63.0, 843.0], [351.0, 843.0], [351.0, 858.0], [63.0, 858.0]], "confidence": 0.9799492955207825, "text": "added to the original movie soundtrack (no video).", "bbox": [63.0, 843.0, 351.0, 858.0]}, {"polygon": [[410.0, 843.0], [728.0, 843.0], [728.0, 858.0], [410.0, 858.0]], "confidence": 0.9819157719612122, "text": "The text-text alignment is not accurate when the CMD movie", "bbox": [410.0, 843.0, 728.0, 858.0]}, {"polygon": [[358.0, 845.0], [382.0, 845.0], [382.0, 857.0], [358.0, 857.0]], "confidence": 0.744975209236145, "text": "The", "bbox": [358.0, 845.0, 382.0, 857.0]}, {"polygon": [[408.0, 858.0], [728.0, 858.0], [728.0, 872.0], [408.0, 872.0]], "confidence": 0.9774261116981506, "text": "clip does not have many dialogues, e.g . in action movies.  In", "bbox": [408.0, 858.0, 728.0, 872.0]}, {"polygon": [[63.0, 859.0], [384.0, 858.0], [384.0, 872.0], [63.0, 874.0]], "confidence": 0.9781896471977234, "text": "CMD dataset [ 4 ] consists of short (about 2 minutes long)", "bbox": [63.0, 859.0, 384.0, 872.0]}, {"polygon": [[409.0, 873.0], [728.0, 873.0], [728.0, 888.0], [409.0, 888.0]], "confidence": 0.9847708344459534, "text": "practice, we find it gives reliable rough time points for more than", "bbox": [409.0, 873.0, 728.0, 888.0]}, {"polygon": [[63.0, 874.0], [383.0, 874.0], [383.0, 888.0], [63.0, 888.0]], "confidence": 0.9798983335494995, "text": "non-contiguous movie clips in the form of video files on", "bbox": [63.0, 874.0, 383.0, 888.0]}, {"polygon": [[409.0, 889.0], [729.0, 889.0], [729.0, 903.0], [409.0, 903.0]], "confidence": 0.9825106263160706, "text": "90% of CMD clips by randomly checking 10+ movies manually.", "bbox": [409.0, 889.0, 729.0, 903.0]}, {"polygon": [[63.0, 890.0], [382.0, 890.0], [382.0, 903.0], [63.0, 903.0]], "confidence": 0.9779241681098938, "text": "YouTube (around 10 clips per movie).  Although there are", "bbox": [63.0, 890.0, 382.0, 903.0]}, {"polygon": [[63.0, 905.0], [383.0, 905.0], [383.0, 920.0], [63.0, 920.0]], "confidence": 0.9824302196502686, "text": "about 2000 movies overlapping between these two data sources,", "bbox": [63.0, 905.0, 383.0, 920.0]}, {"polygon": [[409.0, 911.0], [728.0, 911.0], [728.0, 926.0], [409.0, 926.0]], "confidence": 0.979093611240387, "text": "Stage2: Audio-audio alignment. Given the rough alignment", "bbox": [409.0, 911.0, 728.0, 926.0]}, {"polygon": [[63.0, 921.0], [383.0, 921.0], [383.0, 935.0], [63.0, 935.0]], "confidence": 0.9821676015853882, "text": "temporally aligning the AD with the movie clips from CMD", "bbox": [63.0, 921.0, 383.0, 935.0]}, {"polygon": [[63.0, 937.0], [382.0, 937.0], [382.0, 950.0], [63.0, 950.0]], "confidence": 0.9841771125793457, "text": "is a non-trivial task due to several challenges: First, the movie", "bbox": [63.0, 937.0, 382.0, 950.0]}, {"polygon": [[427.0, 938.0], [720.0, 938.0], [720.0, 952.0], [427.0, 952.0]], "confidence": 0.9536774158477783, "text": "https://en.wikipedia.org/wiki/5761#PAL_speed-up", "bbox": [427.0, 938.0, 720.0, 952.0]}, {"polygon": [[392.0, 976.0], [401.0, 976.0], [401.0, 989.0], [392.0, 989.0]], "confidence": 0.4543657600879669, "text": "3", "bbox": [392.0, 976.0, 401.0, 989.0]}], "languages": ["en"], "image_bbox": [0.0, 0.0, 816.0, 1056.0], "page": 3}, {"text_lines": [{"polygon": [[408.0, 96.0], [728.0, 96.0], [728.0, 111.0], [408.0, 111.0]], "confidence": 0.9683586359024048, "text": "0.8 < W ′ < 1.25 and empirically choose mean-square-error", "bbox": [408.0, 96.0, 728.0, 111.0]}, {"polygon": [[409.0, 112.0], [728.0, 112.0], [728.0, 126.0], [409.0, 126.0]], "confidence": 0.9712953567504883, "text": "MSE < 100. We find these two conditions give very decisive", "bbox": [409.0, 112.0, 728.0, 126.0]}, {"polygon": [[408.0, 128.0], [728.0, 128.0], [728.0, 142.0], [408.0, 142.0]], "confidence": 0.9793465733528137, "text": "boundaries for confident RANSAC output. For instance, the", "bbox": [408.0, 128.0, 728.0, 142.0]}, {"polygon": [[409.0, 144.0], [729.0, 144.0], [729.0, 158.0], [409.0, 158.0]], "confidence": 0.9745915532112122, "text": "successful RANSAC fitting at Figure 2 has an MSE of 0.68,", "bbox": [409.0, 144.0, 729.0, 158.0]}, {"polygon": [[409.0, 159.0], [677.0, 159.0], [677.0, 174.0], [409.0, 174.0]], "confidence": 0.9786878824234009, "text": "whereas failed fittings typically have an MSE > 500.", "bbox": [409.0, 159.0, 677.0, 174.0]}, {"polygon": [[408.0, 183.0], [728.0, 183.0], [728.0, 198.0], [408.0, 198.0]], "confidence": 0.9813515543937683, "text": "Summary. With this two-stage method, we obtain accurate", "bbox": [408.0, 183.0, 728.0, 198.0]}, {"polygon": [[408.0, 199.0], [728.0, 199.0], [728.0, 213.0], [408.0, 213.0]], "confidence": 0.9823566675186157, "text": "temporal alignment between AudioVault audio and CMD movie", "bbox": [408.0, 199.0, 728.0, 213.0]}, {"polygon": [[408.0, 214.0], [728.0, 214.0], [728.0, 227.0], [408.0, 227.0]], "confidence": 0.9824260473251343, "text": "clips, therefore we can map the AudioVault AD annotations", "bbox": [408.0, 214.0, 728.0, 227.0]}, {"polygon": [[408.0, 229.0], [728.0, 229.0], [728.0, 244.0], [408.0, 244.0]], "confidence": 0.9810945987701416, "text": "onto the CMD time axis to get video-text annotations.  This", "bbox": [408.0, 229.0, 728.0, 244.0]}, {"polygon": [[409.0, 245.0], [728.0, 245.0], [728.0, 259.0], [409.0, 259.0]], "confidence": 0.9812326431274414, "text": "gives us the dataset CMD-AD (statistics are provided in", "bbox": [409.0, 245.0, 728.0, 259.0]}, {"polygon": [[262.0, 253.0], [369.0, 253.0], [369.0, 265.0], [262.0, 265.0]], "confidence": 0.8336977958679199, "text": "y = 0.959 : - 0.672", "bbox": [262.0, 253.0, 369.0, 265.0]}, {"polygon": [[83.0, 257.0], [97.0, 257.0], [97.0, 265.0], [83.0, 265.0]], "confidence": 0.7468286156654358, "text": "..", "bbox": [83.0, 257.0, 97.0, 265.0]}, {"polygon": [[410.0, 260.0], [728.0, 260.0], [728.0, 274.0], [410.0, 274.0]], "confidence": 0.9772622585296631, "text": "Table 2 ), consisting of 101k AD segments spanning 1,432", "bbox": [410.0, 260.0, 728.0, 274.0]}, {"polygon": [[409.0, 276.0], [728.0, 276.0], [728.0, 290.0], [409.0, 290.0]], "confidence": 0.9801835417747498, "text": "movies.  Note that the total number of overlapping movies", "bbox": [409.0, 276.0, 728.0, 290.0]}, {"polygon": [[63.0, 281.0], [382.0, 281.0], [382.0, 294.0], [63.0, 294.0]], "confidence": 0.9721828699111938, "text": "Figure 2. Audio-audio alignment between two sources. (left): For", "bbox": [63.0, 281.0, 382.0, 294.0]}, {"polygon": [[408.0, 292.0], [728.0, 292.0], [728.0, 305.0], [408.0, 305.0]], "confidence": 0.9833933115005493, "text": "between the two datasets is 1,803, which means an 80% success", "bbox": [408.0, 292.0, 728.0, 305.0]}, {"polygon": [[63.0, 296.0], [382.0, 296.0], [382.0, 308.0], [63.0, 308.0]], "confidence": 0.9795271158218384, "text": "each small audio segment on AudioVault, we find the best-matching", "bbox": [63.0, 296.0, 382.0, 308.0]}, {"polygon": [[409.0, 307.0], [729.0, 307.0], [729.0, 322.0], [409.0, 322.0]], "confidence": 0.9840553402900696, "text": "rate of precise alignment. A higher success rate can be achieved", "bbox": [409.0, 307.0, 729.0, 322.0]}, {"polygon": [[64.0, 310.0], [384.0, 310.0], [384.0, 322.0], [64.0, 322.0]], "confidence": 0.9841229915618896, "text": "audio segment on CMD clip, and plot two timestamps as scatters;", "bbox": [64.0, 310.0, 384.0, 322.0]}, {"polygon": [[408.0, 323.0], [728.0, 323.0], [728.0, 337.0], [408.0, 337.0]], "confidence": 0.9824361205101013, "text": "by using a larger search window or an iterative alignment", "bbox": [408.0, 323.0, 728.0, 337.0]}, {"polygon": [[63.0, 324.0], [383.0, 324.0], [383.0, 337.0], [63.0, 337.0]], "confidence": 0.9801455736160278, "text": "( right ): Fitting a straight line with RANSAC we can get the precise", "bbox": [63.0, 324.0, 383.0, 337.0]}, {"polygon": [[409.0, 338.0], [615.0, 338.0], [615.0, 352.0], [409.0, 352.0]], "confidence": 0.9752624034881592, "text": "pipeline, which we leave as future work.", "bbox": [409.0, 338.0, 615.0, 352.0]}, {"polygon": [[63.0, 339.0], [382.0, 339.0], [382.0, 352.0], [63.0, 352.0]], "confidence": 0.9831726551055908, "text": "mapping function between two sources. The slope of the fitted line", "bbox": [63.0, 339.0, 382.0, 352.0]}, {"polygon": [[63.0, 352.0], [383.0, 352.0], [383.0, 367.0], [63.0, 367.0]], "confidence": 0.9813826680183411, "text": "0.959 < 1 indicates this CMD clip plays slightly faster than the", "bbox": [63.0, 352.0, 383.0, 367.0]}, {"polygon": [[425.0, 354.0], [730.0, 354.0], [730.0, 368.0], [425.0, 368.0]], "confidence": 0.9813209176063538, "text": "We use 1332 movies for training and 100 movies for eval-", "bbox": [425.0, 354.0, 730.0, 368.0]}, {"polygon": [[63.0, 366.0], [219.0, 366.0], [219.0, 381.0], [63.0, 381.0]], "confidence": 0.9678319096565247, "text": "corresponding AudioVault chunk.", "bbox": [63.0, 366.0, 219.0, 381.0]}, {"polygon": [[409.0, 369.0], [729.0, 369.0], [729.0, 384.0], [409.0, 384.0]], "confidence": 0.9798020124435425, "text": "uation, naming the splits CMD-AD-Train and CMD-AD-Eval", "bbox": [409.0, 369.0, 729.0, 384.0]}, {"polygon": [[409.0, 385.0], [499.0, 385.0], [499.0, 399.0], [409.0, 399.0]], "confidence": 0.947746753692627, "text": "sets, respectively.", "bbox": [409.0, 385.0, 499.0, 399.0]}, {"polygon": [[63.0, 397.0], [384.0, 397.0], [384.0, 412.0], [63.0, 412.0]], "confidence": 0.9820907115936279, "text": "(which may be noisy) provided by Stage 1, this stage aims to", "bbox": [63.0, 397.0, 384.0, 412.0]}, {"polygon": [[409.0, 412.0], [679.0, 412.0], [679.0, 428.0], [409.0, 428.0]], "confidence": 0.9700408577919006, "text": "3.2. HowTo-AD – Pixels from HowTo100M", "bbox": [409.0, 412.0, 679.0, 428.0]}, {"polygon": [[63.0, 414.0], [382.0, 414.0], [382.0, 428.0], [63.0, 428.0]], "confidence": 0.9826061725616455, "text": "verify the match, and obtain a precise temporal alignment by", "bbox": [63.0, 414.0, 382.0, 428.0]}, {"polygon": [[63.0, 429.0], [383.0, 429.0], [383.0, 442.0], [63.0, 442.0]], "confidence": 0.983069658279419, "text": "comparing audio signals from the two sources. The objective", "bbox": [63.0, 429.0, 383.0, 442.0]}, {"polygon": [[408.0, 438.0], [728.0, 438.0], [728.0, 453.0], [408.0, 453.0]], "confidence": 0.9837621450424194, "text": "Our second dataset is based on the large-scale instructional video", "bbox": [408.0, 438.0, 728.0, 453.0]}, {"polygon": [[63.0, 444.0], [371.0, 444.0], [371.0, 459.0], [63.0, 459.0]], "confidence": 0.9828429818153381, "text": "is to get a precise linear mapping for each CMD movie clip:", "bbox": [63.0, 444.0, 371.0, 459.0]}, {"polygon": [[409.0, 454.0], [729.0, 454.0], [729.0, 468.0], [409.0, 468.0]], "confidence": 0.9823470711708069, "text": "dataset HowTo100M [ 40 ], that contains over 1.2M videos with", "bbox": [409.0, 454.0, 729.0, 468.0]}, {"polygon": [[140.0, 468.0], [307.0, 468.0], [307.0, 484.0], [140.0, 484.0]], "confidence": 0.9163454174995422, "text": "f : { T AV → T CMD } = W · t AV + B ,", "bbox": [140.0, 468.0, 307.0, 484.0]}, {"polygon": [[365.0, 469.0], [384.0, 469.0], [384.0, 483.0], [365.0, 483.0]], "confidence": 0.7157939672470093, "text": "(2)", "bbox": [365.0, 469.0, 384.0, 483.0]}, {"polygon": [[409.0, 469.0], [729.0, 469.0], [729.0, 484.0], [409.0, 484.0]], "confidence": 0.977042555809021, "text": "ASR subtitles from YouTube. At first glance, the ASR tran-", "bbox": [409.0, 469.0, 729.0, 484.0]}, {"polygon": [[409.0, 485.0], [728.0, 485.0], [728.0, 498.0], [409.0, 498.0]], "confidence": 0.9844233393669128, "text": "scripts of these videos may look drastically different from that", "bbox": [409.0, 485.0, 728.0, 498.0]}, {"polygon": [[63.0, 492.0], [383.0, 492.0], [383.0, 507.0], [63.0, 507.0]], "confidence": 0.9731751084327698, "text": "where W is the speed rate between the AudioVault and CMD", "bbox": [63.0, 492.0, 383.0, 507.0]}, {"polygon": [[408.0, 500.0], [728.0, 500.0], [728.0, 515.0], [408.0, 515.0]], "confidence": 0.9829965829849243, "text": "of AD in movies, since the spoken words are primarily aimed", "bbox": [408.0, 500.0, 728.0, 515.0]}, {"polygon": [[63.0, 508.0], [383.0, 508.0], [383.0, 522.0], [63.0, 522.0]], "confidence": 0.9821881055831909, "text": "movie sources which might not be 1.0 due to different movie", "bbox": [63.0, 508.0, 383.0, 522.0]}, {"polygon": [[409.0, 516.0], [720.0, 516.0], [720.0, 530.0], [409.0, 530.0]], "confidence": 0.9842000603675842, "text": "to instruct the viewer on how to carry out various daily tasks.", "bbox": [409.0, 516.0, 720.0, 530.0]}, {"polygon": [[63.0, 524.0], [383.0, 524.0], [383.0, 537.0], [63.0, 537.0]], "confidence": 0.9759927988052368, "text": "fps, and B is the time shift. The key idea here is that even", "bbox": [63.0, 524.0, 383.0, 537.0]}, {"polygon": [[424.0, 532.0], [729.0, 532.0], [729.0, 547.0], [424.0, 547.0]], "confidence": 0.9795280694961548, "text": "However, we can transform the instruction ASR into", "bbox": [424.0, 532.0, 729.0, 547.0]}, {"polygon": [[63.0, 539.0], [383.0, 539.0], [383.0, 553.0], [63.0, 553.0]], "confidence": 0.9820776581764221, "text": "though the individual CMD clips are matched locally, the", "bbox": [63.0, 539.0, 383.0, 553.0]}, {"polygon": [[409.0, 548.0], [728.0, 548.0], [728.0, 562.0], [409.0, 562.0]], "confidence": 0.9825815558433533, "text": "pseudo-AD in two steps. The first step is to adopt the captions", "bbox": [409.0, 548.0, 728.0, 562.0]}, {"polygon": [[63.0, 555.0], [384.0, 555.0], [384.0, 569.0], [63.0, 569.0]], "confidence": 0.9797571897506714, "text": "parameters W and B can be assumed to be global (i.e. constant)", "bbox": [63.0, 555.0, 384.0, 569.0]}, {"polygon": [[410.0, 564.0], [729.0, 564.0], [729.0, 578.0], [410.0, 578.0]], "confidence": 0.9820051789283752, "text": "generated from HowToCaption [ 53 ], where the ASR transcripts", "bbox": [410.0, 564.0, 729.0, 578.0]}, {"polygon": [[63.0, 570.0], [384.0, 570.0], [384.0, 584.0], [63.0, 584.0]], "confidence": 0.9844924211502075, "text": "across the movie. Hence, matches can be verified as they will lie", "bbox": [63.0, 570.0, 384.0, 584.0]}, {"polygon": [[409.0, 579.0], [728.0, 579.0], [728.0, 593.0], [409.0, 593.0]], "confidence": 0.9815865159034729, "text": "have been transformed into concise and descriptive captions", "bbox": [409.0, 579.0, 728.0, 593.0]}, {"polygon": [[63.0, 586.0], [383.0, 586.0], [383.0, 600.0], [63.0, 600.0]], "confidence": 0.9774923324584961, "text": "on a line specified by W and B , and this line can be obtained by", "bbox": [63.0, 586.0, 383.0, 600.0]}, {"polygon": [[409.0, 594.0], [728.0, 594.0], [728.0, 608.0], [409.0, 608.0]], "confidence": 0.9802373051643372, "text": "with large language models (LLMs).  To improve caption", "bbox": [409.0, 594.0, 728.0, 608.0]}, {"polygon": [[63.0, 601.0], [380.0, 601.0], [380.0, 615.0], [63.0, 615.0]], "confidence": 0.9777050614356995, "text": "a standard robust fitting method. Here we use RANSAC [ 17 ].", "bbox": [63.0, 601.0, 380.0, 615.0]}, {"polygon": [[409.0, 609.0], [729.0, 609.0], [729.0, 623.0], [409.0, 623.0]], "confidence": 0.9831277132034302, "text": "temporal alignment with the corresponding video timestamps,", "bbox": [409.0, 609.0, 729.0, 623.0]}, {"polygon": [[81.0, 616.0], [383.0, 616.0], [383.0, 630.0], [81.0, 630.0]], "confidence": 0.9818769097328186, "text": "To obtain precise audio alignment, we perform alignment", "bbox": [81.0, 616.0, 383.0, 630.0]}, {"polygon": [[409.0, 625.0], [728.0, 625.0], [728.0, 639.0], [409.0, 639.0]], "confidence": 0.9809544086456299, "text": "the authors employ an off-the-shelf Temporal Alignment", "bbox": [409.0, 625.0, 728.0, 639.0]}, {"polygon": [[63.0, 632.0], [382.0, 632.0], [382.0, 646.0], [63.0, 646.0]], "confidence": 0.9822626709938049, "text": "on low-level audio representation mel-spectrogram. First, we", "bbox": [63.0, 632.0, 382.0, 646.0]}, {"polygon": [[409.0, 640.0], [728.0, 640.0], [728.0, 654.0], [409.0, 654.0]], "confidence": 0.9826945662498474, "text": "Network [ 19 ], while also discarding non-alignable subtitles", "bbox": [409.0, 640.0, 728.0, 654.0]}, {"polygon": [[63.0, 647.0], [383.0, 647.0], [383.0, 661.0], [63.0, 661.0]], "confidence": 0.9817351698875427, "text": "compute mel-spectrogram for both AudioVault audio and the", "bbox": [63.0, 647.0, 383.0, 661.0]}, {"polygon": [[408.0, 655.0], [729.0, 655.0], [729.0, 670.0], [408.0, 670.0]], "confidence": 0.9807356595993042, "text": "(such as \"Hello, welcome to my channel!\"). The second step", "bbox": [408.0, 655.0, 729.0, 670.0]}, {"polygon": [[63.0, 661.0], [383.0, 661.0], [383.0, 677.0], [63.0, 677.0]], "confidence": 0.954717755317688, "text": "CMD movie clip, denoted as MAV and MCMD . We only take", "bbox": [63.0, 661.0, 383.0, 677.0]}, {"polygon": [[409.0, 672.0], [728.0, 672.0], [728.0, 685.0], [409.0, 685.0]], "confidence": 0.9835954308509827, "text": "addresses the key difference between descriptive captions and", "bbox": [409.0, 672.0, 728.0, 685.0]}, {"polygon": [[63.0, 678.0], [383.0, 678.0], [383.0, 692.0], [63.0, 692.0]], "confidence": 0.9831888675689697, "text": "a short AudioVault audio chunk based on the previous text-text", "bbox": [63.0, 678.0, 383.0, 692.0]}, {"polygon": [[409.0, 687.0], [729.0, 687.0], [729.0, 702.0], [409.0, 702.0]], "confidence": 0.9836732745170593, "text": "audio descriptions, that is, character names do not appear in", "bbox": [409.0, 687.0, 729.0, 702.0]}, {"polygon": [[63.0, 694.0], [383.0, 694.0], [383.0, 708.0], [63.0, 708.0]], "confidence": 0.9827598929405212, "text": "alignment result. Second, we mask out mel-spectrogram regions", "bbox": [63.0, 694.0, 383.0, 708.0]}, {"polygon": [[409.0, 704.0], [728.0, 704.0], [728.0, 717.0], [409.0, 717.0]], "confidence": 0.9770150780677795, "text": "the captions.  For this transformation, we detect the subjects", "bbox": [409.0, 704.0, 728.0, 717.0]}, {"polygon": [[63.0, 710.0], [383.0, 710.0], [383.0, 724.0], [63.0, 724.0]], "confidence": 0.9828975200653076, "text": "of Audio Descriptions based on the timestamps obtained from", "bbox": [63.0, 710.0, 383.0, 724.0]}, {"polygon": [[408.0, 718.0], [729.0, 718.0], [729.0, 732.0], [408.0, 732.0]], "confidence": 0.982521116733551, "text": "of description sentences and uniformly replace them with a", "bbox": [408.0, 718.0, 729.0, 732.0]}, {"polygon": [[63.0, 725.0], [383.0, 725.0], [383.0, 738.0], [63.0, 738.0]], "confidence": 0.9819696545600891, "text": "WhisperX, as the AD signal only exists in AudioVault and", "bbox": [63.0, 725.0, 383.0, 738.0]}, {"polygon": [[409.0, 733.0], [729.0, 733.0], [729.0, 748.0], [409.0, 748.0]], "confidence": 0.977598249912262, "text": "randomly chosen character name, e.g . transforming ' a man is", "bbox": [409.0, 733.0, 729.0, 748.0]}, {"polygon": [[63.0, 740.0], [382.0, 740.0], [382.0, 755.0], [63.0, 755.0]], "confidence": 0.9825345277786255, "text": "not in the CMD movie clip. Next, we perform sliding window", "bbox": [63.0, 740.0, 382.0, 755.0]}, {"polygon": [[409.0, 749.0], [728.0, 749.0], [728.0, 762.0], [409.0, 762.0]], "confidence": 0.9827055335044861, "text": "pouring wine' into ' John is pouring wine'. This completes the", "bbox": [409.0, 749.0, 728.0, 762.0]}, {"polygon": [[63.0, 755.0], [383.0, 755.0], [383.0, 770.0], [63.0, 770.0]], "confidence": 0.9730985760688782, "text": "matching with a window size w = 1.6s. For each 1.6-second", "bbox": [63.0, 755.0, 383.0, 770.0]}, {"polygon": [[409.0, 764.0], [728.0, 764.0], [728.0, 779.0], [409.0, 779.0]], "confidence": 0.981430172920227, "text": "transformation from HowTo100M captions to the HowTo-AD.", "bbox": [409.0, 764.0, 728.0, 779.0]}, {"polygon": [[63.0, 771.0], [384.0, 770.0], [384.0, 785.0], [63.0, 786.0]], "confidence": 0.9790542125701904, "text": "audio chunk on AudioVault starting from t 1 to t 2 , we find", "bbox": [63.0, 771.0, 384.0, 785.0]}, {"polygon": [[425.0, 781.0], [728.0, 781.0], [728.0, 794.0], [425.0, 794.0]], "confidence": 0.9819275736808777, "text": "Additionally, to mimic having a character bank as external", "bbox": [425.0, 781.0, 728.0, 794.0]}, {"polygon": [[63.0, 785.0], [383.0, 785.0], [383.0, 800.0], [63.0, 800.0]], "confidence": 0.9806775450706482, "text": "the corresponding timestamps on CMD audio which has a", "bbox": [63.0, 785.0, 383.0, 800.0]}, {"polygon": [[409.0, 796.0], [728.0, 796.0], [728.0, 811.0], [409.0, 811.0]], "confidence": 0.9850360751152039, "text": "knowledge as in [ 21 , 36 , 69 ], we also provide each instructional", "bbox": [409.0, 796.0, 728.0, 811.0]}, {"polygon": [[63.0, 802.0], [180.0, 802.0], [180.0, 816.0], [63.0, 816.0]], "confidence": 0.9294015765190125, "text": "mximum correlation:", "bbox": [63.0, 802.0, 180.0, 816.0]}, {"polygon": [[409.0, 812.0], [729.0, 812.0], [729.0, 826.0], [409.0, 826.0]], "confidence": 0.9763578772544861, "text": "video with a pseudo-character bank that includes: the chosen", "bbox": [409.0, 812.0, 729.0, 826.0]}, {"polygon": [[104.0, 821.0], [325.0, 821.0], [325.0, 846.0], [104.0, 846.0]], "confidence": 0.8895265460014343, "text": "y 1,y 2 = argmax { cor ( M [ t 1,t 2 ] , M [ t i ,t i + w ] ,", "bbox": [104.0, 821.0, 325.0, 846.0]}, {"polygon": [[365.0, 827.0], [384.0, 827.0], [384.0, 841.0], [365.0, 841.0]], "confidence": 0.7177563905715942, "text": "(3)", "bbox": [365.0, 827.0, 384.0, 841.0]}, {"polygon": [[408.0, 827.0], [729.0, 827.0], [729.0, 841.0], [408.0, 841.0]], "confidence": 0.9834242463111877, "text": "character name and the character portrait face extracted from", "bbox": [408.0, 827.0, 729.0, 841.0]}, {"polygon": [[149.0, 842.0], [185.0, 842.0], [185.0, 850.0], [149.0, 850.0]], "confidence": 0.665521502494812, "text": "t i , t i + m .", "bbox": [149.0, 842.0, 185.0, 850.0]}, {"polygon": [[408.0, 843.0], [728.0, 843.0], [728.0, 857.0], [408.0, 857.0]], "confidence": 0.9833266735076904, "text": "the instructional video, and a few face exemplars sampled from", "bbox": [408.0, 843.0, 728.0, 857.0]}, {"polygon": [[409.0, 858.0], [728.0, 858.0], [728.0, 872.0], [409.0, 872.0]], "confidence": 0.9785643219947815, "text": "other videos to mimic off-screen characters.  An overview of", "bbox": [409.0, 858.0, 728.0, 872.0]}, {"polygon": [[63.0, 859.0], [384.0, 859.0], [384.0, 874.0], [63.0, 874.0]], "confidence": 0.983080267906189, "text": "These matches can be thought of as points on a scatter plot", "bbox": [63.0, 859.0, 384.0, 874.0]}, {"polygon": [[409.0, 873.0], [665.0, 873.0], [665.0, 888.0], [409.0, 888.0]], "confidence": 0.9718379378318787, "text": "the pipeline with an example is shown in Figure 3 .", "bbox": [409.0, 873.0, 665.0, 888.0]}, {"polygon": [[63.0, 875.0], [383.0, 874.0], [383.0, 888.0], [63.0, 890.0]], "confidence": 0.9745786190032959, "text": "from ( t 1 , y 1 ) to ( t 2 , y 2 ) for a series of small windows from", "bbox": [63.0, 875.0, 383.0, 888.0]}, {"polygon": [[63.0, 890.0], [382.0, 890.0], [382.0, 904.0], [63.0, 904.0]], "confidence": 0.9745057225227356, "text": "AudioVault, as shown in Figure 2 . Finally, we use a RANSAC", "bbox": [63.0, 890.0, 382.0, 904.0]}, {"polygon": [[424.0, 890.0], [728.0, 890.0], [728.0, 903.0], [424.0, 903.0]], "confidence": 0.9812121391296387, "text": "Because of the noisy nature of YouTube videos and the", "bbox": [424.0, 890.0, 728.0, 903.0]}, {"polygon": [[63.0, 905.0], [383.0, 905.0], [383.0, 920.0], [63.0, 920.0]], "confidence": 0.9821608662605286, "text": "algorithm to fit a line through these match points over all clips", "bbox": [63.0, 905.0, 383.0, 920.0]}, {"polygon": [[409.0, 905.0], [728.0, 905.0], [728.0, 920.0], [409.0, 920.0]], "confidence": 0.9834298491477966, "text": "abundance of data in the HowTo100M dataset, we filter out less", "bbox": [409.0, 905.0, 728.0, 920.0]}, {"polygon": [[409.0, 921.0], [729.0, 921.0], [729.0, 935.0], [409.0, 935.0]], "confidence": 0.9834452867507935, "text": "preferable videos by the quality of subject detection in How-", "bbox": [409.0, 921.0, 729.0, 935.0]}, {"polygon": [[63.0, 922.0], [383.0, 922.0], [383.0, 935.0], [63.0, 935.0]], "confidence": 0.9816240668296814, "text": "to obtain the mapping in Equation ( 2 ).  Based on the ratio", "bbox": [63.0, 922.0, 383.0, 935.0]}, {"polygon": [[63.0, 936.0], [382.0, 936.0], [382.0, 951.0], [63.0, 951.0]], "confidence": 0.9804250597953796, "text": "between common movie fps, we filter RANSAC output by", "bbox": [63.0, 936.0, 382.0, 951.0]}, {"polygon": [[410.0, 937.0], [728.0, 935.0], [728.0, 950.0], [410.0, 952.0]], "confidence": 0.9833675026893616, "text": "ToCaption, the frequency of names in ASR, and the quality of", "bbox": [410.0, 937.0, 728.0, 950.0]}], "languages": ["en"], "image_bbox": [0.0, 0.0, 816.0, 1056.0], "page": 4}, {"text_lines": [{"polygon": [[348.0, 178.0], [416.0, 178.0], [416.0, 184.0], [348.0, 184.0]], "confidence": 0.8618419766426086, "text": "10. 181 John is por", "bbox": [348.0, 178.0, 416.0, 184.0]}, {"polygon": [[567.0, 187.0], [629.0, 189.0], [629.0, 198.0], [567.0, 197.0]], "confidence": 0.9291208982467651, "text": "[10, 18] A man is", "bbox": [567.0, 187.0, 629.0, 198.0]}, {"polygon": [[471.0, 195.0], [529.0, 195.0], [529.0, 203.0], [471.0, 203.0]], "confidence": 0.9242038130760193, "text": "Find subjects", "bbox": [471.0, 195.0, 529.0, 203.0]}, {"polygon": [[565.0, 200.0], [655.0, 200.0], [655.0, 208.0], [565.0, 208.0]], "confidence": 0.9610309600830078, "text": "pouring wine into a glass", "bbox": [565.0, 200.0, 655.0, 208.0]}, {"polygon": [[487.0, 205.0], [531.0, 205.0], [531.0, 214.0], [487.0, 214.0]], "confidence": 0.8906879425048828, "text": "A man, He", "bbox": [487.0, 205.0, 531.0, 214.0]}, {"polygon": [[567.0, 212.0], [655.0, 212.0], [655.0, 220.0], [567.0, 220.0]], "confidence": 0.9523445963859558, "text": "[20, 25] He takes a sniff", "bbox": [567.0, 212.0, 655.0, 220.0]}, {"polygon": [[348.0, 216.0], [400.0, 216.0], [400.0, 222.0], [348.0, 222.0]], "confidence": 0.9078746438026428, "text": "niff of the win", "bbox": [348.0, 216.0, 400.0, 222.0]}, {"polygon": [[565.0, 222.0], [603.0, 222.0], [603.0, 230.0], [565.0, 230.0]], "confidence": 0.9041996002197266, "text": "of the win", "bbox": [565.0, 222.0, 603.0, 230.0]}, {"polygon": [[250.0, 241.0], [342.0, 241.0], [342.0, 251.0], [250.0, 251.0]], "confidence": 0.9081113934516907, "text": "nstruct character bank", "bbox": [250.0, 241.0, 342.0, 251.0]}, {"polygon": [[353.0, 241.0], [435.0, 241.0], [435.0, 251.0], [353.0, 251.0]], "confidence": 0.9494131803512573, "text": "Rewrite description", "bbox": [353.0, 241.0, 435.0, 251.0]}, {"polygon": [[523.0, 243.0], [669.0, 243.0], [669.0, 255.0], [523.0, 255.0]], "confidence": 0.9576377868652344, "text": "Descriptions from HowToCaption .", "bbox": [523.0, 243.0, 669.0, 255.0]}, {"polygon": [[309.0, 256.0], [369.0, 256.0], [369.0, 265.0], [309.0, 265.0]], "confidence": 0.8746890425682068, "text": "HowTo-AD", "bbox": [309.0, 256.0, 369.0, 265.0]}, {"polygon": [[63.0, 274.0], [730.0, 274.0], [730.0, 287.0], [63.0, 287.0]], "confidence": 0.9789682030677795, "text": "Figure 3. HowTo-AD dataset . We convert the LLM rewritten video descriptions (from HowToCaption) to fit movie audio descriptions by ( i )", "bbox": [63.0, 274.0, 730.0, 287.0]}, {"polygon": [[64.0, 289.0], [728.0, 289.0], [728.0, 302.0], [64.0, 302.0]], "confidence": 0.9882100224494934, "text": "uniformly replacing the subjects in descriptions with a randomly sampled name, i.e. John , and (2) constructing a character bank by providing", "bbox": [64.0, 289.0, 728.0, 302.0]}, {"polygon": [[63.0, 303.0], [658.0, 303.0], [658.0, 316.0], [63.0, 316.0]], "confidence": 0.9900191426277161, "text": "a frame with the instructor and the randomly sampled name. The video sample is from https://youtu.be/aRbQb19v2JI .", "bbox": [63.0, 303.0, 658.0, 316.0]}, {"polygon": [[63.0, 327.0], [383.0, 327.0], [383.0, 342.0], [63.0, 342.0]], "confidence": 0.9840474128723145, "text": "character portrait faces; details are in the Appendix. As shown", "bbox": [63.0, 327.0, 383.0, 342.0]}, {"polygon": [[408.0, 327.0], [728.0, 327.0], [728.0, 341.0], [408.0, 341.0]], "confidence": 0.9810869097709656, "text": "then finetuned on CMD-AD-Train. We pretrain on HowTo-AD", "bbox": [408.0, 327.0, 728.0, 341.0]}, {"polygon": [[63.0, 343.0], [382.0, 343.0], [382.0, 357.0], [63.0, 357.0]], "confidence": 0.9755619168281555, "text": "in Table 1 , the HowTo-AD dataset ends up with a subset of 180k", "bbox": [63.0, 343.0, 382.0, 357.0]}, {"polygon": [[409.0, 342.0], [728.0, 342.0], [728.0, 357.0], [409.0, 357.0]], "confidence": 0.9811617732048035, "text": "for 1 epoch and finetune on CMD-AD-Train for 2 epochs. We", "bbox": [409.0, 342.0, 728.0, 357.0]}, {"polygon": [[64.0, 359.0], [383.0, 359.0], [383.0, 372.0], [64.0, 372.0]], "confidence": 0.9764702320098877, "text": "YouTube videos from the original HowTo100M dataset – which", "bbox": [64.0, 359.0, 383.0, 372.0]}, {"polygon": [[409.0, 359.0], [728.0, 359.0], [728.0, 372.0], [409.0, 372.0]], "confidence": 0.973726212978363, "text": "find finetuning beyond 2 epochs leads to overfitting. We use", "bbox": [409.0, 359.0, 728.0, 372.0]}, {"polygon": [[63.0, 374.0], [383.0, 374.0], [383.0, 388.0], [63.0, 388.0]], "confidence": 0.9775410294532776, "text": "is about 20% of the full HowTo100M – and 3.4M transformed", "bbox": [63.0, 374.0, 383.0, 388.0]}, {"polygon": [[408.0, 374.0], [729.0, 374.0], [729.0, 388.0], [408.0, 388.0]], "confidence": 0.9801420569419861, "text": "a batch size of 8 AD samples, an AdamW optimizer [ 37 ] with", "bbox": [408.0, 374.0, 729.0, 388.0]}, {"polygon": [[64.0, 389.0], [370.0, 389.0], [370.0, 403.0], [64.0, 403.0]], "confidence": 0.9813284277915955, "text": "AD segments with timestamps from HowToCaption dataset.", "bbox": [64.0, 389.0, 370.0, 403.0]}, {"polygon": [[408.0, 389.0], [728.0, 389.0], [728.0, 403.0], [408.0, 403.0]], "confidence": 0.9674292206764221, "text": "3 × 10 − 5 learning rate and a cosine decay schedule. For both", "bbox": [408.0, 389.0, 728.0, 403.0]}, {"polygon": [[408.0, 405.0], [728.0, 405.0], [728.0, 418.0], [408.0, 418.0]], "confidence": 0.9839506149291992, "text": "the pretraining and finetuning stages, the training pipeline fits", "bbox": [408.0, 405.0, 728.0, 418.0]}, {"polygon": [[63.0, 419.0], [209.0, 419.0], [209.0, 437.0], [63.0, 437.0]], "confidence": 0.9533966183662415, "text": "4. Model Architecture", "bbox": [63.0, 419.0, 209.0, 437.0]}, {"polygon": [[408.0, 419.0], [728.0, 419.0], [728.0, 434.0], [408.0, 434.0]], "confidence": 0.9806145429611206, "text": "in a single A40 GPU with 48GB GPU memory. More training", "bbox": [408.0, 419.0, 728.0, 434.0]}, {"polygon": [[409.0, 436.0], [598.0, 436.0], [598.0, 449.0], [409.0, 449.0]], "confidence": 0.9731181263923645, "text": "details are provided in the Appendix.", "bbox": [409.0, 436.0, 598.0, 449.0]}, {"polygon": [[63.0, 449.0], [382.0, 449.0], [382.0, 463.0], [63.0, 463.0]], "confidence": 0.9830101728439331, "text": "With pixel data available, we propose two visual captioning", "bbox": [63.0, 449.0, 382.0, 463.0]}, {"polygon": [[409.0, 463.0], [558.0, 463.0], [558.0, 479.0], [409.0, 479.0]], "confidence": 0.9533399343490601, "text": "5. Evaluation Methods", "bbox": [409.0, 463.0, 558.0, 479.0]}, {"polygon": [[63.0, 464.0], [383.0, 464.0], [383.0, 480.0], [63.0, 480.0]], "confidence": 0.9807887673377991, "text": "models based on BLIP2 [ 32 ] and Llama2 [ 60 ] for movie AD", "bbox": [63.0, 464.0, 383.0, 480.0]}, {"polygon": [[63.0, 480.0], [382.0, 480.0], [382.0, 495.0], [63.0, 495.0]], "confidence": 0.9750459790229797, "text": "generation.  Specifically, we propose two new architectures", "bbox": [63.0, 480.0, 382.0, 495.0]}, {"polygon": [[409.0, 490.0], [729.0, 490.0], [729.0, 505.0], [409.0, 505.0]], "confidence": 0.98221355676651, "text": "We propose two new methods for evaluating movie AD gener-", "bbox": [409.0, 490.0, 729.0, 505.0]}, {"polygon": [[63.0, 496.0], [382.0, 496.0], [382.0, 509.0], [63.0, 509.0]], "confidence": 0.9780978560447693, "text": "called Movie-BLIP2 and Movie-Llama2.  Both of them take", "bbox": [63.0, 496.0, 382.0, 509.0]}, {"polygon": [[409.0, 506.0], [729.0, 506.0], [729.0, 520.0], [409.0, 520.0]], "confidence": 0.9833757877349854, "text": "ation: CRITIC for identifying correct characters, and an LLM-", "bbox": [409.0, 506.0, 729.0, 520.0]}, {"polygon": [[63.0, 511.0], [382.0, 511.0], [382.0, 526.0], [63.0, 526.0]], "confidence": 0.9815563559532166, "text": "8 video frames, resized at 224 × 224 pixels as inputs, then use", "bbox": [63.0, 511.0, 382.0, 526.0]}, {"polygon": [[409.0, 521.0], [717.0, 521.0], [717.0, 535.0], [409.0, 535.0]], "confidence": 0.9829623699188232, "text": "based AD evaluation for assessing holistic semantics of AD.", "bbox": [409.0, 521.0, 717.0, 535.0]}, {"polygon": [[63.0, 527.0], [382.0, 527.0], [382.0, 541.0], [63.0, 541.0]], "confidence": 0.9781149625778198, "text": "EVA-CLIP [ 56 ] to extract dense visual features. Next, we use", "bbox": [63.0, 527.0, 382.0, 541.0]}, {"polygon": [[63.0, 542.0], [384.0, 542.0], [384.0, 556.0], [63.0, 556.0]], "confidence": 0.9821696281433105, "text": "a Q-former to attend to spatial-temporal feature grids to extract", "bbox": [63.0, 542.0, 384.0, 556.0]}, {"polygon": [[408.0, 552.0], [728.0, 552.0], [728.0, 566.0], [408.0, 566.0]], "confidence": 0.9759026169776917, "text": "CRITIC (Co-Referencing In Text for Identifying", "bbox": [408.0, 552.0, 728.0, 566.0]}, {"polygon": [[63.0, 557.0], [383.0, 557.0], [383.0, 572.0], [63.0, 572.0]], "confidence": 0.9827035069465637, "text": "visual descriptors represented by 32 vectors. Both models also", "bbox": [63.0, 557.0, 383.0, 572.0]}, {"polygon": [[488.0, 568.0], [728.0, 568.0], [728.0, 582.0], [488.0, 582.0]], "confidence": 0.9758865237236023, "text": "The CRITIC metric assesses the accuracy of", "bbox": [488.0, 568.0, 728.0, 582.0]}, {"polygon": [[410.0, 570.0], [490.0, 570.0], [490.0, 582.0], [410.0, 582.0]], "confidence": 0.8951797485351562, "text": "Characters).", "bbox": [410.0, 570.0, 490.0, 582.0]}, {"polygon": [[63.0, 573.0], [382.0, 573.0], [382.0, 586.0], [63.0, 586.0]], "confidence": 0.9833863377571106, "text": "processes image inputs from character face exemplars. In this", "bbox": [63.0, 573.0, 382.0, 586.0]}, {"polygon": [[408.0, 584.0], [728.0, 584.0], [728.0, 597.0], [408.0, 597.0]], "confidence": 0.9815021753311157, "text": "character naming in predicted AD against human-generated", "bbox": [408.0, 584.0, 728.0, 597.0]}, {"polygon": [[63.0, 588.0], [383.0, 588.0], [383.0, 603.0], [63.0, 603.0]], "confidence": 0.9828987717628479, "text": "case, they take a single image resized at 224 × 224 pixels, and", "bbox": [63.0, 588.0, 383.0, 603.0]}, {"polygon": [[408.0, 599.0], [730.0, 599.0], [730.0, 614.0], [408.0, 614.0]], "confidence": 0.9737502336502075, "text": "reference AD.  The metric is designed to be robust to (i)", "bbox": [408.0, 599.0, 730.0, 614.0]}, {"polygon": [[63.0, 604.0], [382.0, 604.0], [382.0, 618.0], [63.0, 618.0]], "confidence": 0.9836389422416687, "text": "then use the same EVA-CLIP to extract visual features in spatial", "bbox": [63.0, 604.0, 382.0, 618.0]}, {"polygon": [[409.0, 615.0], [730.0, 615.0], [730.0, 629.0], [409.0, 629.0]], "confidence": 0.9820692539215088, "text": "co-referencing complexities (ii) pronoun usage, and (iii)", "bbox": [409.0, 615.0, 730.0, 629.0]}, {"polygon": [[63.0, 619.0], [382.0, 619.0], [382.0, 633.0], [63.0, 633.0]], "confidence": 0.9832167029380798, "text": "grid, and the same Q-former to attend to this spatial feature", "bbox": [63.0, 619.0, 382.0, 633.0]}, {"polygon": [[408.0, 630.0], [729.0, 630.0], [729.0, 644.0], [408.0, 644.0]], "confidence": 0.9768470525741577, "text": "orthographic variation in character names.  The objective is", "bbox": [408.0, 630.0, 729.0, 644.0]}, {"polygon": [[63.0, 635.0], [383.0, 635.0], [383.0, 649.0], [63.0, 649.0]], "confidence": 0.9830589294433594, "text": "grid and extract 32 vectors as image descriptors. The video and", "bbox": [63.0, 635.0, 383.0, 649.0]}, {"polygon": [[409.0, 645.0], [729.0, 645.0], [729.0, 660.0], [409.0, 660.0]], "confidence": 0.9839472770690918, "text": "to measure the quality of character reference in the generated", "bbox": [409.0, 645.0, 729.0, 660.0]}, {"polygon": [[63.0, 651.0], [382.0, 651.0], [382.0, 664.0], [63.0, 664.0]], "confidence": 0.9833659529685974, "text": "image descriptors are passed to two shallow projection heads", "bbox": [63.0, 651.0, 382.0, 664.0]}, {"polygon": [[409.0, 661.0], [728.0, 661.0], [728.0, 674.0], [409.0, 674.0]], "confidence": 0.980022132396698, "text": "AD compared to the ground-truth AD. For example, the model", "bbox": [409.0, 661.0, 728.0, 674.0]}, {"polygon": [[63.0, 666.0], [384.0, 666.0], [384.0, 681.0], [63.0, 681.0]], "confidence": 0.9837474226951599, "text": "respectively, to project them on the language embedding space.", "bbox": [63.0, 666.0, 384.0, 681.0]}, {"polygon": [[408.0, 676.0], [728.0, 676.0], [728.0, 691.0], [408.0, 691.0]], "confidence": 0.9840978980064392, "text": "might generate AD with the text 'Jack' or pronouns like 'he', the", "bbox": [408.0, 676.0, 728.0, 691.0]}, {"polygon": [[63.0, 682.0], [382.0, 682.0], [382.0, 696.0], [63.0, 696.0]], "confidence": 0.9833846688270569, "text": "Finally, the projected visual outputs together with language", "bbox": [63.0, 682.0, 382.0, 696.0]}, {"polygon": [[408.0, 692.0], [729.0, 692.0], [729.0, 706.0], [408.0, 706.0]], "confidence": 0.9829601049423218, "text": "CRITIC metric aims to evaluate the accuracy of these references.", "bbox": [408.0, 692.0, 729.0, 706.0]}, {"polygon": [[63.0, 696.0], [384.0, 696.0], [384.0, 711.0], [63.0, 711.0]], "confidence": 0.9826934337615967, "text": "prompts are passed to a large language model (OPT for Movie-", "bbox": [63.0, 696.0, 384.0, 711.0]}, {"polygon": [[425.0, 708.0], [729.0, 708.0], [729.0, 722.0], [425.0, 722.0]], "confidence": 0.9806805849075317, "text": "To achieve this, a co-referencing model 4 is applied to", "bbox": [425.0, 708.0, 729.0, 722.0]}, {"polygon": [[63.0, 711.0], [383.0, 711.0], [383.0, 726.0], [63.0, 726.0]], "confidence": 0.9805260896682739, "text": "BLIP2 and Llama2 for Movie-Llama2) to generate movie AD", "bbox": [63.0, 711.0, 383.0, 726.0]}, {"polygon": [[409.0, 724.0], [728.0, 724.0], [728.0, 738.0], [409.0, 738.0]], "confidence": 0.979131817817688, "text": "both predicted and reference AD passages.  Specifically, let", "bbox": [409.0, 724.0, 728.0, 738.0]}, {"polygon": [[63.0, 728.0], [382.0, 728.0], [382.0, 741.0], [63.0, 741.0]], "confidence": 0.9777367115020752, "text": "in text form. An overview of architecture is shown in Figure 4 .", "bbox": [63.0, 728.0, 382.0, 741.0]}, {"polygon": [[408.0, 740.0], [728.0, 737.0], [728.0, 753.0], [408.0, 755.0]], "confidence": 0.9394693970680237, "text": "C = \"c 1 ,c 2 ,...,c n .\" denote the set of official character names", "bbox": [408.0, 740.0, 728.0, 753.0]}, {"polygon": [[80.0, 745.0], [383.0, 745.0], [383.0, 759.0], [80.0, 759.0]], "confidence": 0.9811024069786072, "text": "The Movie-BLIP2 architecture inherits from the original", "bbox": [80.0, 745.0, 383.0, 759.0]}, {"polygon": [[409.0, 754.0], [729.0, 754.0], [729.0, 769.0], [409.0, 769.0]], "confidence": 0.9841423034667969, "text": "from the cast list of a movie, combined into a single sentence.", "bbox": [409.0, 754.0, 729.0, 769.0]}, {"polygon": [[63.0, 760.0], [383.0, 760.0], [383.0, 774.0], [63.0, 774.0]], "confidence": 0.9806126952171326, "text": "Image-based BLIP2 architecture, and it uses OPT [ 78 ] as the", "bbox": [63.0, 760.0, 383.0, 774.0]}, {"polygon": [[409.0, 770.0], [729.0, 770.0], [729.0, 784.0], [409.0, 784.0]], "confidence": 0.9827834963798523, "text": "For a specific movie, we group the predicted and reference au-", "bbox": [409.0, 770.0, 729.0, 784.0]}, {"polygon": [[63.0, 776.0], [383.0, 776.0], [383.0, 791.0], [63.0, 791.0]], "confidence": 0.982032060623169, "text": "language model. The Movie-Llama2 architecture inherits from", "bbox": [63.0, 776.0, 383.0, 791.0]}, {"polygon": [[409.0, 784.0], [729.0, 786.0], [729.0, 800.0], [409.0, 799.0]], "confidence": 0.9782337546348572, "text": "dio descriptions into long paragraphs, denoted as AD pred and", "bbox": [409.0, 784.0, 729.0, 800.0]}, {"polygon": [[63.0, 792.0], [383.0, 792.0], [383.0, 806.0], [63.0, 806.0]], "confidence": 0.9792382121086121, "text": "the image-based MiniGPT-4 [ 84 ] which connects BLIP2's", "bbox": [63.0, 792.0, 383.0, 806.0]}, {"polygon": [[410.0, 800.0], [728.0, 800.0], [728.0, 814.0], [410.0, 814.0]], "confidence": 0.9785704612731934, "text": "AD ref respectively. In order to guide the co-referencing model", "bbox": [410.0, 800.0, 728.0, 814.0]}, {"polygon": [[63.0, 806.0], [383.0, 806.0], [383.0, 821.0], [63.0, 821.0]], "confidence": 0.982489287853241, "text": "visual embedding with Llama2 language embedding [ 60 ]. Our", "bbox": [63.0, 806.0, 383.0, 821.0]}, {"polygon": [[408.0, 816.0], [728.0, 816.0], [728.0, 831.0], [408.0, 831.0]], "confidence": 0.979275107383728, "text": "to detect character names, both AD pred and AD ref are prefixed", "bbox": [408.0, 816.0, 728.0, 831.0]}, {"polygon": [[63.0, 823.0], [382.0, 823.0], [382.0, 836.0], [63.0, 836.0]], "confidence": 0.9812055826187134, "text": "Movie-Llama2 follows the same setup and uses Llama2 as the", "bbox": [63.0, 823.0, 382.0, 836.0]}, {"polygon": [[409.0, 831.0], [730.0, 831.0], [730.0, 846.0], [409.0, 846.0]], "confidence": 0.9768888354301453, "text": "with the character list sentence C , as shown in Figure 5 ( a and c ).", "bbox": [409.0, 831.0, 730.0, 846.0]}, {"polygon": [[63.0, 838.0], [384.0, 838.0], [384.0, 852.0], [63.0, 852.0]], "confidence": 0.9824613332748413, "text": "language model. We take pre-trained checkpoints from open-", "bbox": [63.0, 838.0, 384.0, 852.0]}, {"polygon": [[425.0, 848.0], [729.0, 848.0], [729.0, 861.0], [425.0, 861.0]], "confidence": 0.9817753434181213, "text": "Next, the co-referencing model is applied to both paragraphs", "bbox": [425.0, 848.0, 729.0, 861.0]}, {"polygon": [[63.0, 852.0], [382.0, 852.0], [382.0, 867.0], [63.0, 867.0]], "confidence": 0.9793874025344849, "text": "sourced projects [ 72 ] and [ 77 ].  Details of these architectures", "bbox": [63.0, 852.0, 382.0, 867.0]}, {"polygon": [[409.0, 863.0], [727.0, 863.0], [727.0, 878.0], [409.0, 878.0]], "confidence": 0.9828157424926758, "text": "from prediction and reference, yielding sets of identities E pred", "bbox": [409.0, 863.0, 727.0, 878.0]}, {"polygon": [[63.0, 869.0], [384.0, 869.0], [384.0, 882.0], [63.0, 882.0]], "confidence": 0.984076738357544, "text": "are in the Appendix. Following previous works [ 84 ], by default,", "bbox": [63.0, 869.0, 384.0, 882.0]}, {"polygon": [[409.0, 879.0], [728.0, 879.0], [728.0, 893.0], [409.0, 893.0]], "confidence": 0.957930326461792, "text": "and E ret . Each identity E includes references and pronouns", "bbox": [409.0, 879.0, 728.0, 893.0]}, {"polygon": [[63.0, 884.0], [382.0, 884.0], [382.0, 899.0], [63.0, 899.0]], "confidence": 0.9826507568359375, "text": "all the visual backbone, language model, and the Q-former are", "bbox": [63.0, 884.0, 382.0, 899.0]}, {"polygon": [[409.0, 894.0], [729.0, 894.0], [729.0, 908.0], [409.0, 908.0]], "confidence": 0.984502375125885, "text": "linked to a single entity. We only keep identities containing ex-", "bbox": [409.0, 894.0, 729.0, 908.0]}, {"polygon": [[63.0, 900.0], [299.0, 900.0], [299.0, 914.0], [63.0, 914.0]], "confidence": 0.9787106513977051, "text": "frozen, and we only train the projection heads.", "bbox": [63.0, 900.0, 299.0, 914.0]}, {"polygon": [[409.0, 909.0], [728.0, 909.0], [728.0, 924.0], [409.0, 924.0]], "confidence": 0.9819757342338562, "text": "actly one character name from C , ensuring distinct association", "bbox": [409.0, 909.0, 728.0, 924.0]}, {"polygon": [[64.0, 921.0], [382.0, 921.0], [382.0, 935.0], [64.0, 935.0]], "confidence": 0.982650101184845, "text": "Training details. By default, we adopt a two-stage training", "bbox": [64.0, 921.0, 382.0, 935.0]}, {"polygon": [[63.0, 937.0], [382.0, 937.0], [382.0, 952.0], [63.0, 952.0]], "confidence": 0.9733755588531494, "text": "strategy.  The model is firstly pretrained on HowTo-AD and", "bbox": [63.0, 937.0, 382.0, 952.0]}, {"polygon": [[428.0, 939.0], [656.0, 939.0], [656.0, 951.0], [428.0, 951.0]], "confidence": 0.9759379625320435, "text": "https://github.com/shon-otmazgin/fastcoref", "bbox": [428.0, 939.0, 656.0, 951.0]}], "languages": ["en"], "image_bbox": [0.0, 0.0, 816.0, 1056.0], "page": 5}, {"text_lines": [{"polygon": [[229.0, 190.0], [360.0, 192.0], [360.0, 204.0], [229.0, 203.0]], "confidence": 0.9455687999725342, "text": "Prompt : Possible characters:", "bbox": [229.0, 190.0, 360.0, 204.0]}, {"polygon": [[229.0, 205.0], [418.0, 205.0], [418.0, 219.0], [229.0, 219.0]], "confidence": 0.9424024820327759, "text": "Jack Foley < Image >", "bbox": [229.0, 205.0, 418.0, 219.0]}, {"polygon": [[229.0, 220.0], [335.0, 220.0], [335.0, 231.0], [229.0, 231.0]], "confidence": 0.9394092559814453, "text": "Karen Sisco <Image>", "bbox": [229.0, 220.0, 335.0, 231.0]}, {"polygon": [[371.0, 220.0], [419.0, 220.0], [419.0, 231.0], [371.0, 231.0]], "confidence": 0.8652723431587219, "text": "]</Image>", "bbox": [371.0, 220.0, 419.0, 231.0]}, {"polygon": [[229.0, 234.0], [295.0, 235.0], [295.0, 244.0], [229.0, 243.0]], "confidence": 0.9376680254936218, "text": "< Video > ", "bbox": [229.0, 234.0, 295.0, 244.0]}, {"polygon": [[297.0, 234.0], [354.0, 233.0], [354.0, 243.0], [297.0, 245.0]], "confidence": 0.8057315349578857, "text": "■ </Videoi", "bbox": [297.0, 234.0, 354.0, 243.0]}, {"polygon": [[229.0, 248.0], [482.0, 246.0], [482.0, 259.0], [229.0, 260.0]], "confidence": 0.981375515460968, "text": "Please provide a detailed description of this movie clip.", "bbox": [229.0, 248.0, 482.0, 259.0]}, {"polygon": [[340.0, 285.0], [578.0, 285.0], [578.0, 297.0], [340.0, 297.0]], "confidence": 0.9794504642486572, "text": "Target: As Karen stares gloomily out of the window", "bbox": [340.0, 285.0, 578.0, 297.0]}, {"polygon": [[221.0, 291.0], [305.0, 291.0], [305.0, 303.0], [221.0, 303.0]], "confidence": 0.874441921710968, "text": "➪ OPT / Llama2", "bbox": [221.0, 291.0, 305.0, 303.0]}, {"polygon": [[338.0, 299.0], [512.0, 299.0], [512.0, 311.0], [338.0, 311.0]], "confidence": 0.9740009903907776, "text": "Jack approaches toying with a lighter.", "bbox": [338.0, 299.0, 512.0, 311.0]}, {"polygon": [[63.0, 320.0], [729.0, 320.0], [729.0, 334.0], [63.0, 334.0]], "confidence": 0.9883420467376709, "text": "Figure 4. Architecture overview. Our model takes as input movie frames and movie character bank from IMDb including face exemplars and", "bbox": [63.0, 320.0, 729.0, 334.0]}, {"polygon": [[63.0, 335.0], [728.0, 335.0], [728.0, 348.0], [63.0, 348.0]], "confidence": 0.988562822341919, "text": "character names, and produces character-aware audio descriptions. The input images/videos are first fed to a frozen visual feature extractor to", "bbox": [63.0, 335.0, 728.0, 348.0]}, {"polygon": [[63.0, 349.0], [728.0, 349.0], [728.0, 362.0], [63.0, 362.0]], "confidence": 0.983785092830658, "text": "obtain spatial or spatial­temporal visual features. Then it uses a shared Q­former to process the visual information and project them to the language", "bbox": [63.0, 349.0, 728.0, 362.0]}, {"polygon": [[63.0, 363.0], [556.0, 363.0], [556.0, 376.0], [63.0, 376.0]], "confidence": 0.9900326132774353, "text": "embedding space, to leverage frozen large language models(LLM) like OPT and Llama2 for text generation.", "bbox": [63.0, 363.0, 556.0, 376.0]}, {"polygon": [[408.0, 387.0], [728.0, 387.0], [728.0, 402.0], [408.0, 402.0]], "confidence": 0.9817942976951599, "text": "whether the name is correct. The CRITIC score has a range", "bbox": [408.0, 387.0, 728.0, 402.0]}, {"polygon": [[409.0, 403.0], [605.0, 403.0], [605.0, 418.0], [409.0, 418.0]], "confidence": 0.972073495388031, "text": "between 0 and 1, with 1 being perfect.", "bbox": [409.0, 403.0, 605.0, 418.0]}, {"polygon": [[408.0, 430.0], [728.0, 430.0], [728.0, 445.0], [408.0, 445.0]], "confidence": 0.9761762022972107, "text": "LLM-AD-eval. We also adopt the LLM as a judge [ 11 , 81 ]", "bbox": [408.0, 430.0, 728.0, 445.0]}, {"polygon": [[409.0, 446.0], [728.0, 446.0], [728.0, 460.0], [409.0, 460.0]], "confidence": 0.980589747428894, "text": "procedure for AD quality assessment.  Following previous", "bbox": [409.0, 446.0, 728.0, 460.0]}, {"polygon": [[409.0, 461.0], [728.0, 461.0], [728.0, 475.0], [409.0, 475.0]], "confidence": 0.9799659252166748, "text": "works [ 39 ], we use a 'gpt-3.5-turbo' API from OpenAI and", "bbox": [409.0, 461.0, 728.0, 475.0]}, {"polygon": [[409.0, 476.0], [728.0, 476.0], [728.0, 491.0], [409.0, 491.0]], "confidence": 0.9712446331977844, "text": "an open-sourced 'llama-2-7b-chat' model [ 60 ].  prompting", "bbox": [409.0, 476.0, 728.0, 491.0]}, {"polygon": [[409.0, 492.0], [728.0, 492.0], [728.0, 506.0], [409.0, 506.0]], "confidence": 0.9819770455360413, "text": "the model to assess the matching quality between a pair of", "bbox": [409.0, 492.0, 728.0, 506.0]}, {"polygon": [[409.0, 507.0], [729.0, 507.0], [729.0, 521.0], [409.0, 521.0]], "confidence": 0.9810844659805298, "text": "predicted AD and ground-truth AD by a score from 1 to 5,", "bbox": [409.0, 507.0, 729.0, 521.0]}, {"polygon": [[409.0, 523.0], [728.0, 523.0], [728.0, 537.0], [409.0, 537.0]], "confidence": 0.9777702689170837, "text": "where 5 indicates the best matching and 1 indicates the worst", "bbox": [409.0, 523.0, 728.0, 537.0]}, {"polygon": [[409.0, 538.0], [728.0, 538.0], [728.0, 552.0], [409.0, 552.0]], "confidence": 0.9776681661605835, "text": "matching.  To be complementary to the CRITIC metric, for", "bbox": [409.0, 538.0, 728.0, 552.0]}, {"polygon": [[113.0, 547.0], [174.0, 547.0], [174.0, 554.0], [113.0, 554.0]], "confidence": 0.9131501913070679, "text": "He tums and smiles", "bbox": [113.0, 547.0, 174.0, 554.0]}, {"polygon": [[409.0, 554.0], [728.0, 554.0], [728.0, 568.0], [409.0, 568.0]], "confidence": 0.9809789657592773, "text": "LLM-AD-eval we instruct the LLM to (1) consider pronouns", "bbox": [409.0, 554.0, 728.0, 568.0]}, {"polygon": [[116.0, 567.0], [382.0, 567.0], [382.0, 582.0], [116.0, 582.0]], "confidence": 0.9791377186775208, "text": "Illustration of the CRITIC metric. The paragraphs", "bbox": [116.0, 567.0, 382.0, 582.0]}, {"polygon": [[64.0, 570.0], [119.0, 570.0], [119.0, 581.0], [64.0, 581.0]], "confidence": 0.8886203765869141, "text": "Figure 5.", "bbox": [64.0, 570.0, 119.0, 581.0]}, {"polygon": [[409.0, 570.0], [728.0, 570.0], [728.0, 583.0], [409.0, 583.0]], "confidence": 0.9822549819946289, "text": "as valid matches and ignore character names, and (2) focus", "bbox": [409.0, 570.0, 728.0, 583.0]}, {"polygon": [[63.0, 584.0], [382.0, 584.0], [382.0, 596.0], [63.0, 596.0]], "confidence": 0.9833583831787109, "text": "consisting character list and AD (a,c) are fed into a co-referencing", "bbox": [63.0, 584.0, 382.0, 596.0]}, {"polygon": [[409.0, 585.0], [728.0, 585.0], [728.0, 599.0], [409.0, 599.0]], "confidence": 0.9769256711006165, "text": "on human actions, objects and interactions.  The customized", "bbox": [409.0, 585.0, 728.0, 599.0]}, {"polygon": [[63.0, 598.0], [382.0, 598.0], [382.0, 610.0], [63.0, 610.0]], "confidence": 0.9801070094108582, "text": "model to get co-referencing identities (b,d).  The CRITIC metric", "bbox": [63.0, 598.0, 382.0, 610.0]}, {"polygon": [[409.0, 601.0], [607.0, 601.0], [607.0, 615.0], [409.0, 615.0]], "confidence": 0.9732741117477417, "text": "prompts are provided in the Appendix.", "bbox": [409.0, 601.0, 607.0, 615.0]}, {"polygon": [[63.0, 611.0], [382.0, 611.0], [382.0, 624.0], [63.0, 624.0]], "confidence": 0.9830650687217712, "text": "computes an IoU between the identities in the prediction vs. the", "bbox": [63.0, 611.0, 382.0, 624.0]}, {"polygon": [[63.0, 625.0], [196.0, 625.0], [196.0, 638.0], [63.0, 638.0]], "confidence": 0.9666634202003479, "text": "identities from the reference.", "bbox": [63.0, 625.0, 196.0, 638.0]}, {"polygon": [[408.0, 630.0], [511.0, 630.0], [511.0, 645.0], [408.0, 645.0]], "confidence": 0.9318223595619202, "text": "6. Experiments", "bbox": [408.0, 630.0, 511.0, 645.0]}, {"polygon": [[63.0, 651.0], [383.0, 651.0], [383.0, 666.0], [63.0, 666.0]], "confidence": 0.9829949736595154, "text": "with individual characters. Importantly, we remove pronouns", "bbox": [63.0, 651.0, 383.0, 666.0]}, {"polygon": [[409.0, 655.0], [728.0, 655.0], [728.0, 670.0], [409.0, 670.0]], "confidence": 0.9773532152175903, "text": "We outline the datasets (Sec. 6.1 ) and evaluation measures", "bbox": [409.0, 655.0, 728.0, 670.0]}, {"polygon": [[63.0, 666.0], [384.0, 666.0], [384.0, 681.0], [63.0, 681.0]], "confidence": 0.9829857349395752, "text": "like 'he', 'she', and 'they' in each co-referencing identity to", "bbox": [63.0, 666.0, 384.0, 681.0]}, {"polygon": [[408.0, 671.0], [728.0, 671.0], [728.0, 685.0], [408.0, 685.0]], "confidence": 0.9746313691139221, "text": "(Sec. 6.2 ) employed in our experiments, and provide an", "bbox": [408.0, 671.0, 728.0, 685.0]}, {"polygon": [[63.0, 682.0], [383.0, 682.0], [383.0, 696.0], [63.0, 696.0]], "confidence": 0.9794923663139343, "text": "exclude ambiguous pronoun matching.  Next, we map each", "bbox": [63.0, 682.0, 383.0, 696.0]}, {"polygon": [[409.0, 687.0], [728.0, 687.0], [728.0, 700.0], [409.0, 700.0]], "confidence": 0.981369137763977, "text": "analysis on inter-rater agreement between AD annotation", "bbox": [409.0, 687.0, 728.0, 700.0]}, {"polygon": [[63.0, 697.0], [384.0, 697.0], [384.0, 712.0], [63.0, 712.0]], "confidence": 0.9834076762199402, "text": "sentence to its corresponding set of co-referencing identities,", "bbox": [63.0, 697.0, 384.0, 712.0]}, {"polygon": [[409.0, 702.0], [728.0, 702.0], [728.0, 716.0], [409.0, 716.0]], "confidence": 0.9824720025062561, "text": "versions (Sec. 6.3 ). We report quantitative results, ablating the", "bbox": [409.0, 702.0, 728.0, 716.0]}, {"polygon": [[63.0, 713.0], [384.0, 713.0], [384.0, 727.0], [63.0, 727.0]], "confidence": 0.9832897782325745, "text": "which may be empty (when no names are recognized), singular,", "bbox": [63.0, 713.0, 384.0, 727.0]}, {"polygon": [[409.0, 718.0], [728.0, 718.0], [728.0, 731.0], [409.0, 731.0]], "confidence": 0.9828320145606995, "text": "architectural design and the effect of HowToAD pretraining", "bbox": [409.0, 718.0, 728.0, 731.0]}, {"polygon": [[63.0, 728.0], [296.0, 728.0], [296.0, 742.0], [63.0, 742.0]], "confidence": 0.9688579440116882, "text": "or multiple, as depicted in Figure 5 ( b and d ).", "bbox": [63.0, 728.0, 296.0, 742.0]}, {"polygon": [[408.0, 733.0], [728.0, 733.0], [728.0, 748.0], [408.0, 748.0]], "confidence": 0.9795696139335632, "text": "(Sec. 6.4 ), followed by qualitative results thanks to our pixel", "bbox": [408.0, 733.0, 728.0, 748.0]}, {"polygon": [[80.0, 744.0], [384.0, 744.0], [384.0, 759.0], [80.0, 759.0]], "confidence": 0.9761365652084351, "text": "The CRITIC metric M CRITIC is then calculated as an IoU,", "bbox": [80.0, 744.0, 384.0, 759.0]}, {"polygon": [[409.0, 748.0], [522.0, 748.0], [522.0, 762.0], [409.0, 762.0]], "confidence": 0.9478999972343445, "text": "movie data (Sec. 6.5).", "bbox": [409.0, 748.0, 522.0, 762.0]}, {"polygon": [[63.0, 760.0], [338.0, 760.0], [338.0, 774.0], [63.0, 774.0]], "confidence": 0.9635353684425354, "text": "for -th AD reference (with valid character identities):", "bbox": [63.0, 760.0, 338.0, 774.0]}, {"polygon": [[409.0, 772.0], [490.0, 772.0], [490.0, 787.0], [409.0, 787.0]], "confidence": 0.926209568977356, "text": "6.1. Datasets", "bbox": [409.0, 772.0, 490.0, 787.0]}, {"polygon": [[222.0, 785.0], [282.0, 785.0], [282.0, 800.0], [222.0, 800.0]], "confidence": 0.9111308455467224, "text": "Epred ", "bbox": [222.0, 785.0, 282.0, 800.0]}, {"polygon": [[161.0, 793.0], [216.0, 793.0], [216.0, 808.0], [161.0, 808.0]], "confidence": 0.8483774065971375, "text": "M CRITIC =", "bbox": [161.0, 793.0, 216.0, 808.0]}, {"polygon": [[365.0, 793.0], [385.0, 793.0], [385.0, 807.0], [365.0, 807.0]], "confidence": 0.724286675453186, "text": "(4)", "bbox": [365.0, 793.0, 385.0, 807.0]}, {"polygon": [[410.0, 797.0], [730.0, 797.0], [730.0, 812.0], [410.0, 812.0]], "confidence": 0.9710819721221924, "text": "AudioVault-8k is the dataset collected from https :", "bbox": [410.0, 797.0, 730.0, 812.0]}, {"polygon": [[219.0, 803.0], [284.0, 803.0], [284.0, 818.0], [219.0, 818.0]], "confidence": 0.7113795280456543, "text": "[ Epred UEref ]", "bbox": [219.0, 803.0, 284.0, 818.0]}, {"polygon": [[410.0, 813.0], [728.0, 813.0], [728.0, 827.0], [410.0, 827.0]], "confidence": 0.9798664450645447, "text": "//audiovault.net/ by [ 21 ] that contains full-movie AD", "bbox": [410.0, 813.0, 728.0, 827.0]}, {"polygon": [[63.0, 827.0], [383.0, 827.0], [383.0, 843.0], [63.0, 843.0]], "confidence": 0.9423775672912598, "text": "where | Eprod ∩ E ref | is the count of matching identities between", "bbox": [63.0, 827.0, 383.0, 843.0]}, {"polygon": [[409.0, 828.0], [728.0, 828.0], [728.0, 843.0], [409.0, 843.0]], "confidence": 0.9826588034629822, "text": "and subtitles transcribed from user-uploaded audio description", "bbox": [409.0, 828.0, 728.0, 843.0]}, {"polygon": [[63.0, 843.0], [382.0, 843.0], [382.0, 858.0], [63.0, 858.0]], "confidence": 0.9527962803840637, "text": "predicted and reference ADs, and | Epred∪E ref | is the total count", "bbox": [63.0, 843.0, 382.0, 858.0]}, {"polygon": [[409.0, 843.0], [728.0, 843.0], [728.0, 858.0], [409.0, 858.0]], "confidence": 0.9788942933082581, "text": "files covering 7800 movies. CMD (Condensed Movie", "bbox": [409.0, 843.0, 728.0, 858.0]}, {"polygon": [[63.0, 859.0], [384.0, 859.0], [384.0, 874.0], [63.0, 874.0]], "confidence": 0.9838781952857971, "text": "of unique identities in both the prediction and the reference.", "bbox": [63.0, 859.0, 384.0, 874.0]}, {"polygon": [[409.0, 859.0], [728.0, 859.0], [728.0, 873.0], [409.0, 873.0]], "confidence": 0.9801250100135803, "text": "Dataset) [ 4 ] contains movie clips collected from YouTube", "bbox": [409.0, 859.0, 728.0, 873.0]}, {"polygon": [[63.0, 874.0], [384.0, 874.0], [384.0, 888.0], [63.0, 888.0]], "confidence": 0.9824580550193787, "text": "The CRITIC score is averaged across all the AD references.", "bbox": [63.0, 874.0, 384.0, 888.0]}, {"polygon": [[409.0, 874.0], [729.0, 874.0], [729.0, 888.0], [409.0, 888.0]], "confidence": 0.9812042117118835, "text": "for more than 3k movies.  On average each movie has 10", "bbox": [409.0, 874.0, 729.0, 888.0]}, {"polygon": [[63.0, 890.0], [383.0, 890.0], [383.0, 904.0], [63.0, 904.0]], "confidence": 0.98325115442276, "text": "Intuitively, if the predicted AD includes a name, the CRITIC", "bbox": [63.0, 890.0, 383.0, 904.0]}, {"polygon": [[409.0, 890.0], [728.0, 890.0], [728.0, 904.0], [409.0, 904.0]], "confidence": 0.9826498031616211, "text": "non-contiguous clips and each clip spans for a few minutes.", "bbox": [409.0, 890.0, 728.0, 904.0]}, {"polygon": [[63.0, 905.0], [384.0, 905.0], [384.0, 920.0], [63.0, 920.0]], "confidence": 0.9838897585868835, "text": "score verifies whether the name refers to the correct identity;", "bbox": [63.0, 905.0, 384.0, 920.0]}, {"polygon": [[409.0, 905.0], [728.0, 905.0], [728.0, 920.0], [409.0, 920.0]], "confidence": 0.9800803065299988, "text": "HowTo100M [ 40 ] contains 1M YouTube long videos with", "bbox": [409.0, 905.0, 728.0, 920.0]}, {"polygon": [[63.0, 921.0], [382.0, 921.0], [382.0, 935.0], [63.0, 935.0]], "confidence": 0.9789867997169495, "text": "if the prediction includes a pronoun like 'he', the CRITIC", "bbox": [63.0, 921.0, 382.0, 935.0]}, {"polygon": [[409.0, 921.0], [728.0, 921.0], [728.0, 935.0], [409.0, 935.0]], "confidence": 0.9828970432281494, "text": "more than 100M ASR segments. It is typically used for video", "bbox": [409.0, 921.0, 728.0, 935.0]}, {"polygon": [[63.0, 936.0], [382.0, 936.0], [382.0, 951.0], [63.0, 951.0]], "confidence": 0.9827373027801514, "text": "score first resolves the identity by co-referencing, then verifies", "bbox": [63.0, 936.0, 382.0, 951.0]}, {"polygon": [[409.0, 936.0], [728.0, 936.0], [728.0, 951.0], [409.0, 951.0]], "confidence": 0.9823441505432129, "text": "pretraining. CMD-AD is the new movie AD dataset introduced", "bbox": [409.0, 936.0, 728.0, 951.0]}], "languages": ["en"], "image_bbox": [0.0, 0.0, 816.0, 1056.0], "page": 6}, {"text_lines": [{"polygon": [[110.0, 96.0], [180.0, 96.0], [180.0, 108.0], [110.0, 108.0]], "confidence": 0.9354626536369324, "text": "AudioVault #10435", "bbox": [110.0, 96.0, 180.0, 108.0]}, {"polygon": [[265.0, 96.0], [337.0, 96.0], [337.0, 108.0], [265.0, 108.0]], "confidence": 0.9406393766403198, "text": "AudioVault #16387", "bbox": [265.0, 96.0, 337.0, 108.0]}, {"polygon": [[417.0, 98.0], [449.0, 98.0], [449.0, 109.0], [417.0, 109.0]], "confidence": 0.853276789188385, "text": "Method", "bbox": [417.0, 98.0, 449.0, 109.0]}, {"polygon": [[566.0, 97.0], [722.0, 97.0], [722.0, 110.0], [566.0, 110.0]], "confidence": 0.885191023349762, "text": "| CIDEr R@1/5 CRITIC LLM-AD-eval |", "bbox": [566.0, 97.0, 722.0, 110.0]}, {"polygon": [[475.0, 99.0], [515.0, 99.0], [515.0, 109.0], [475.0, 109.0]], "confidence": 0.8639166355133057, "text": "V-model", "bbox": [475.0, 99.0, 515.0, 109.0]}, {"polygon": [[517.0, 99.0], [556.0, 99.0], [556.0, 110.0], [517.0, 110.0]], "confidence": 0.8602277636528015, "text": "L-model", "bbox": [517.0, 99.0, 556.0, 110.0]}, {"polygon": [[417.0, 113.0], [461.0, 113.0], [461.0, 124.0], [417.0, 124.0]], "confidence": 0.877476692199707, "text": "AutoAD-II", "bbox": [417.0, 113.0, 461.0, 124.0]}, {"polygon": [[474.0, 112.0], [545.0, 112.0], [545.0, 124.0], [474.0, 124.0]], "confidence": 0.8888360261917114, "text": "CLIP-B-32 GPT2", "bbox": [474.0, 112.0, 545.0, 124.0]}, {"polygon": [[573.0, 113.0], [592.0, 113.0], [592.0, 124.0], [573.0, 124.0]], "confidence": 0.7401819229125977, "text": "13.5", "bbox": [573.0, 113.0, 592.0, 124.0]}, {"polygon": [[604.0, 113.0], [622.0, 113.0], [622.0, 124.0], [604.0, 124.0]], "confidence": 0.7606056928634644, "text": "26.1", "bbox": [604.0, 113.0, 622.0, 124.0]}, {"polygon": [[638.0, 113.0], [652.0, 113.0], [652.0, 123.0], [638.0, 123.0]], "confidence": 0.5190157294273376, "text": "88.2", "bbox": [638.0, 113.0, 652.0, 123.0]}, {"polygon": [[674.0, 113.0], [714.0, 113.0], [714.0, 124.0], [674.0, 124.0]], "confidence": 0.8853769302368164, "text": "1.53 / 2.08", "bbox": [674.0, 113.0, 714.0, 124.0]}, {"polygon": [[74.0, 114.0], [200.0, 114.0], [200.0, 125.0], [74.0, 125.0]], "confidence": 0.9530148506164551, "text": "[00:42:07.287, 00:42:09.369]", "bbox": [74.0, 114.0, 200.0, 125.0]}, {"polygon": [[227.0, 115.0], [354.0, 115.0], [354.0, 125.0], [227.0, 125.0]], "confidence": 0.9618996977806091, "text": "[00:42:07.126, 00:42:09.428]", "bbox": [227.0, 115.0, 354.0, 125.0]}, {"polygon": [[417.0, 124.0], [471.0, 124.0], [471.0, 135.0], [417.0, 135.0]], "confidence": 0.8980919718742371, "text": "Movie-BLIP2", "bbox": [417.0, 124.0, 471.0, 135.0]}, {"polygon": [[473.0, 124.0], [560.0, 124.0], [560.0, 135.0], [473.0, 135.0]], "confidence": 0.9264065623283386, "text": "Eva-CLIP OPT-2.7B", "bbox": [473.0, 124.0, 560.0, 135.0]}, {"polygon": [[573.0, 124.0], [592.0, 124.0], [592.0, 135.0], [573.0, 135.0]], "confidence": 0.699191689491272, "text": "21.2", "bbox": [573.0, 124.0, 592.0, 135.0]}, {"polygon": [[604.0, 124.0], [622.0, 124.0], [622.0, 135.0], [604.0, 135.0]], "confidence": 0.7275866866111755, "text": "29.3", "bbox": [604.0, 124.0, 622.0, 135.0]}, {"polygon": [[637.0, 124.0], [654.0, 124.0], [654.0, 135.0], [637.0, 135.0]], "confidence": 0.6990076303482056, "text": "24.5", "bbox": [637.0, 124.0, 654.0, 135.0]}, {"polygon": [[673.0, 124.0], [714.0, 124.0], [714.0, 135.0], [673.0, 135.0]], "confidence": 0.8551250100135803, "text": "2.13 / 2.66", "bbox": [673.0, 124.0, 714.0, 135.0]}, {"polygon": [[74.0, 128.0], [197.0, 128.0], [197.0, 139.0], [74.0, 139.0]], "confidence": 0.962042510509491, "text": "Audience members look at a", "bbox": [74.0, 128.0, 197.0, 139.0]}, {"polygon": [[227.0, 128.0], [366.0, 128.0], [366.0, 139.0], [227.0, 139.0]], "confidence": 0.9675630927085876, "text": "All eyes turn to a red-haired boy", "bbox": [227.0, 128.0, 366.0, 139.0]}, {"polygon": [[417.0, 135.0], [571.0, 135.0], [571.0, 146.0], [417.0, 146.0]], "confidence": 0.945823073387146, "text": "Movie-Llama2 Eva-CLIP LLama2-7B", "bbox": [417.0, 135.0, 571.0, 146.0]}, {"polygon": [[572.0, 135.0], [593.0, 135.0], [593.0, 146.0], [572.0, 146.0]], "confidence": 0.7163019776344299, "text": "21.7", "bbox": [572.0, 135.0, 593.0, 146.0]}, {"polygon": [[604.0, 135.0], [623.0, 135.0], [623.0, 146.0], [604.0, 146.0]], "confidence": 0.7590359449386597, "text": "30.0", "bbox": [604.0, 135.0, 623.0, 146.0]}, {"polygon": [[637.0, 135.0], [654.0, 135.0], [654.0, 146.0], [637.0, 146.0]], "confidence": 0.7221741080284119, "text": "25.2", "bbox": [637.0, 135.0, 654.0, 146.0]}, {"polygon": [[673.0, 135.0], [714.0, 135.0], [714.0, 146.0], [673.0, 146.0]], "confidence": 0.857673168182373, "text": "2.05 / 2.85", "bbox": [673.0, 135.0, 714.0, 146.0]}, {"polygon": [[74.0, 141.0], [148.0, 141.0], [148.0, 153.0], [74.0, 153.0]], "confidence": 0.9349157214164734, "text": "by in the crowd.", "bbox": [74.0, 141.0, 148.0, 153.0]}, {"polygon": [[227.0, 141.0], [291.0, 141.0], [291.0, 153.0], [227.0, 153.0]], "confidence": 0.935528576374054, "text": "in the audience", "bbox": [227.0, 141.0, 291.0, 153.0]}, {"polygon": [[410.0, 153.0], [727.0, 151.0], [727.0, 164.0], [410.0, 166.0]], "confidence": 0.9785951375961304, "text": "Table 4. Architecture experiments on CMD-AD-Eval. We compare", "bbox": [410.0, 153.0, 727.0, 164.0]}, {"polygon": [[74.0, 158.0], [200.0, 158.0], [200.0, 169.0], [74.0, 169.0]], "confidence": 0.9632428288459778, "text": "[01:02:05.836, 01:02:09.097]", "bbox": [74.0, 158.0, 200.0, 169.0]}, {"polygon": [[227.0, 157.0], [354.0, 157.0], [354.0, 169.0], [227.0, 169.0]], "confidence": 0.9616414308547974, "text": "[01:02:05.955, 01:02:09.958]", "bbox": [227.0, 157.0, 354.0, 169.0]}, {"polygon": [[409.0, 167.0], [728.0, 167.0], [728.0, 179.0], [409.0, 179.0]], "confidence": 0.974646806716919, "text": "the proposed two architectures with AutoAD-II. All of them take", "bbox": [409.0, 167.0, 728.0, 179.0]}, {"polygon": [[74.0, 170.0], [213.0, 170.0], [213.0, 183.0], [74.0, 183.0]], "confidence": 0.9652261734008789, "text": "Jamie grabs Lipton, hurls him to", "bbox": [74.0, 170.0, 213.0, 183.0]}, {"polygon": [[227.0, 170.0], [368.0, 170.0], [368.0, 183.0], [227.0, 183.0]], "confidence": 0.9700109958648682, "text": "Jamie shoves the detective to the", "bbox": [227.0, 170.0, 368.0, 183.0]}, {"polygon": [[409.0, 181.0], [728.0, 181.0], [728.0, 193.0], [409.0, 193.0]], "confidence": 0.9670673608779907, "text": "character bank inputs.  † : from 'gpt-3.5-turbo' / 'llama-2-7b-chat'", "bbox": [409.0, 181.0, 728.0, 193.0]}, {"polygon": [[72.0, 184.0], [187.0, 184.0], [187.0, 197.0], [72.0, 197.0]], "confidence": 0.9637885689735413, "text": "the floor, and runs outside.", "bbox": [72.0, 184.0, 187.0, 197.0]}, {"polygon": [[227.0, 185.0], [369.0, 183.0], [369.0, 196.0], [227.0, 198.0]], "confidence": 0.9696060419082642, "text": "ground and runs out of the house.", "bbox": [227.0, 185.0, 369.0, 196.0]}, {"polygon": [[409.0, 195.0], [727.0, 195.0], [727.0, 207.0], [409.0, 207.0]], "confidence": 0.9850937724113464, "text": "respectively. Note, AutoAD-II is trained with averaged frame features", "bbox": [409.0, 195.0, 727.0, 207.0]}, {"polygon": [[73.0, 200.0], [201.0, 200.0], [201.0, 212.0], [73.0, 212.0]], "confidence": 0.9636130928993225, "text": "[01:06:17.747, 01:06:21.069]", "bbox": [73.0, 200.0, 201.0, 212.0]}, {"polygon": [[227.0, 200.0], [354.0, 200.0], [354.0, 212.0], [227.0, 212.0]], "confidence": 0.9630165696144104, "text": "[01:06:17.670, 01:06:21.073]", "bbox": [227.0, 200.0, 354.0, 212.0]}, {"polygon": [[410.0, 209.0], [691.0, 209.0], [691.0, 221.0], [410.0, 221.0]], "confidence": 0.9827711582183838, "text": "to mimic its original setting on the feature-only MAD dataset.", "bbox": [410.0, 209.0, 691.0, 221.0]}, {"polygon": [[72.0, 214.0], [208.0, 214.0], [208.0, 227.0], [72.0, 227.0]], "confidence": 0.9692699313163757, "text": "Lipton holds the lantern up, his", "bbox": [72.0, 214.0, 208.0, 227.0]}, {"polygon": [[227.0, 214.0], [350.0, 214.0], [350.0, 227.0], [227.0, 227.0]], "confidence": 0.9648151397705078, "text": "As the detective watches, he", "bbox": [227.0, 214.0, 350.0, 227.0]}, {"polygon": [[72.0, 227.0], [180.0, 227.0], [180.0, 240.0], [72.0, 240.0]], "confidence": 0.9556285738945007, "text": "brow pinched in a frown,", "bbox": [72.0, 227.0, 180.0, 240.0]}, {"polygon": [[227.0, 227.0], [367.0, 227.0], [367.0, 240.0], [227.0, 240.0]], "confidence": 0.9707002639770508, "text": "raises the lantern at arm's length.", "bbox": [227.0, 227.0, 367.0, 240.0]}, {"polygon": [[656.0, 236.0], [722.0, 238.0], [722.0, 250.0], [656.0, 248.0]], "confidence": 0.8827316761016846, "text": "LLM-AD-eval†", "bbox": [656.0, 236.0, 722.0, 250.0]}, {"polygon": [[418.0, 237.0], [452.0, 237.0], [452.0, 249.0], [418.0, 249.0]], "confidence": 0.8132106065750122, "text": "Method.", "bbox": [418.0, 237.0, 452.0, 249.0]}, {"polygon": [[549.0, 237.0], [657.0, 237.0], [657.0, 249.0], [549.0, 249.0]], "confidence": 0.8183576464653015, "text": "CIDEr R @ 1/5 CRITIC", "bbox": [549.0, 237.0, 657.0, 249.0]}, {"polygon": [[498.0, 240.0], [532.0, 238.0], [532.0, 249.0], [499.0, 251.0]], "confidence": 0.8863633275032043, "text": "pretrain", "bbox": [498.0, 240.0, 532.0, 249.0]}, {"polygon": [[65.0, 250.0], [382.0, 249.0], [382.0, 262.0], [65.0, 263.0]], "confidence": 0.9772594571113586, "text": "Figure 6. An example of inter-rater evaluation. Some movies", "bbox": [65.0, 250.0, 382.0, 262.0]}, {"polygon": [[418.0, 254.0], [473.0, 254.0], [473.0, 265.0], [418.0, 265.0]], "confidence": 0.9070217609405518, "text": "Movie-BLIP2", "bbox": [418.0, 254.0, 473.0, 265.0]}, {"polygon": [[557.0, 254.0], [574.0, 254.0], [574.0, 264.0], [557.0, 264.0]], "confidence": 0.7216652631759644, "text": "21.2", "bbox": [557.0, 254.0, 574.0, 264.0]}, {"polygon": [[592.0, 254.0], [610.0, 254.0], [610.0, 264.0], [592.0, 264.0]], "confidence": 0.5786170959472656, "text": "29.3", "bbox": [592.0, 254.0, 610.0, 264.0]}, {"polygon": [[629.0, 254.0], [646.0, 254.0], [646.0, 264.0], [629.0, 264.0]], "confidence": 0.6770318746566772, "text": "24.5", "bbox": [629.0, 254.0, 646.0, 264.0]}, {"polygon": [[670.0, 254.0], [711.0, 254.0], [711.0, 265.0], [670.0, 265.0]], "confidence": 0.8929367065429688, "text": "2.13 / 2.66", "bbox": [670.0, 254.0, 711.0, 265.0]}, {"polygon": [[64.0, 264.0], [382.0, 263.0], [382.0, 276.0], [64.0, 278.0]], "confidence": 0.9764912724494934, "text": "on AudioVault have multiple available ADs.  Two versions of", "bbox": [64.0, 264.0, 382.0, 276.0]}, {"polygon": [[669.0, 264.0], [713.0, 264.0], [713.0, 277.0], [669.0, 277.0]], "confidence": 0.8604739308357239, "text": "2.25 / 2.78", "bbox": [669.0, 264.0, 713.0, 277.0]}, {"polygon": [[418.0, 265.0], [473.0, 265.0], [473.0, 277.0], [418.0, 277.0]], "confidence": 0.9052074551582336, "text": "Movie-BLIP2", "bbox": [418.0, 265.0, 473.0, 277.0]}, {"polygon": [[490.0, 265.0], [539.0, 265.0], [539.0, 277.0], [490.0, 277.0]], "confidence": 0.8747789859771729, "text": "HowTo-AD", "bbox": [490.0, 265.0, 539.0, 277.0]}, {"polygon": [[556.0, 265.0], [575.0, 265.0], [575.0, 277.0], [556.0, 277.0]], "confidence": 0.6055130958557129, "text": "2.3", "bbox": [556.0, 265.0, 575.0, 277.0]}, {"polygon": [[591.0, 265.0], [610.0, 265.0], [610.0, 277.0], [591.0, 277.0]], "confidence": 0.7050753831863403, "text": "29.8", "bbox": [591.0, 265.0, 610.0, 277.0]}, {"polygon": [[629.0, 265.0], [647.0, 265.0], [647.0, 277.0], [629.0, 277.0]], "confidence": 0.7314761877059937, "text": "30.2", "bbox": [629.0, 265.0, 647.0, 277.0]}, {"polygon": [[64.0, 278.0], [382.0, 278.0], [382.0, 290.0], [64.0, 290.0]], "confidence": 0.9831257462501526, "text": "human-annotated ADs for the same visual scene are shown here.", "bbox": [64.0, 278.0, 382.0, 290.0]}, {"polygon": [[418.0, 282.0], [473.0, 282.0], [473.0, 293.0], [418.0, 293.0]], "confidence": 0.8495082259178162, "text": "Movie-Llama'.", "bbox": [418.0, 282.0, 473.0, 293.0]}, {"polygon": [[556.0, 282.0], [575.0, 282.0], [575.0, 293.0], [556.0, 293.0]], "confidence": 0.7391650080680847, "text": "21.7", "bbox": [556.0, 282.0, 575.0, 293.0]}, {"polygon": [[591.0, 282.0], [610.0, 282.0], [610.0, 293.0], [591.0, 293.0]], "confidence": 0.7481127381324768, "text": "30.0", "bbox": [591.0, 282.0, 610.0, 293.0]}, {"polygon": [[628.0, 282.0], [646.0, 282.0], [646.0, 293.0], [628.0, 293.0]], "confidence": 0.7059110999107361, "text": "25.2", "bbox": [628.0, 282.0, 646.0, 293.0]}, {"polygon": [[670.0, 282.0], [711.0, 282.0], [711.0, 293.0], [670.0, 293.0]], "confidence": 0.8825922012329102, "text": "2.05 / 2.85", "bbox": [670.0, 282.0, 711.0, 293.0]}, {"polygon": [[65.0, 293.0], [382.0, 293.0], [382.0, 305.0], [65.0, 305.0]], "confidence": 0.9837578535079956, "text": "These ADs are filtered with a tIoU threshold of 0.9, and from the", "bbox": [65.0, 293.0, 382.0, 305.0]}, {"polygon": [[418.0, 293.0], [479.0, 293.0], [479.0, 305.0], [418.0, 305.0]], "confidence": 0.9006017446517944, "text": "Movie-Llama2", "bbox": [418.0, 293.0, 479.0, 305.0]}, {"polygon": [[482.0, 293.0], [544.0, 293.0], [544.0, 305.0], [482.0, 305.0]], "confidence": 0.8828558921813965, "text": "HowToCaption]", "bbox": [482.0, 293.0, 544.0, 305.0]}, {"polygon": [[557.0, 293.0], [575.0, 293.0], [575.0, 305.0], [557.0, 305.0]], "confidence": 0.7313047647476196, "text": "20.8", "bbox": [557.0, 293.0, 575.0, 305.0]}, {"polygon": [[592.0, 293.0], [610.0, 293.0], [610.0, 304.0], [592.0, 304.0]], "confidence": 0.5933337807655334, "text": "29.4", "bbox": [592.0, 293.0, 610.0, 304.0]}, {"polygon": [[629.0, 293.0], [647.0, 293.0], [647.0, 304.0], [629.0, 304.0]], "confidence": 0.6067990064620972, "text": "25.6", "bbox": [629.0, 293.0, 647.0, 304.0]}, {"polygon": [[670.0, 293.0], [711.0, 293.0], [711.0, 304.0], [670.0, 304.0]], "confidence": 0.8654963374137878, "text": "2.07 / 2.85", "bbox": [670.0, 293.0, 711.0, 304.0]}, {"polygon": [[669.0, 303.0], [713.0, 305.0], [713.0, 317.0], [669.0, 315.0]], "confidence": 0.8537654876708984, "text": "2.29 / 2.92", "bbox": [669.0, 303.0, 713.0, 317.0]}, {"polygon": [[628.0, 304.0], [646.0, 304.0], [646.0, 315.0], [628.0, 315.0]], "confidence": 0.5365889072418213, "text": "32.7", "bbox": [628.0, 304.0, 646.0, 315.0]}, {"polygon": [[556.0, 305.0], [576.0, 305.0], [576.0, 315.0], [556.0, 315.0]], "confidence": 0.7185567021369934, "text": "25.0", "bbox": [556.0, 305.0, 576.0, 315.0]}, {"polygon": [[591.0, 305.0], [610.0, 305.0], [610.0, 315.0], [591.0, 315.0]], "confidence": 0.7293795347213745, "text": "31.2", "bbox": [591.0, 305.0, 610.0, 315.0]}, {"polygon": [[418.0, 306.0], [477.0, 304.0], [477.0, 314.0], [418.0, 316.0]], "confidence": 0.8991090059280396, "text": "Movie-Llama2", "bbox": [418.0, 306.0, 477.0, 314.0]}, {"polygon": [[490.0, 306.0], [539.0, 306.0], [539.0, 316.0], [490.0, 316.0]], "confidence": 0.8593586683273315, "text": "HowTo-AD", "bbox": [490.0, 306.0, 539.0, 316.0]}, {"polygon": [[65.0, 307.0], [237.0, 307.0], [237.0, 318.0], [65.0, 318.0]], "confidence": 0.9448013305664062, "text": "movie 'Dead Silence'( tt0455760 ).", "bbox": [65.0, 307.0, 237.0, 318.0]}, {"polygon": [[456.0, 325.0], [728.0, 325.0], [728.0, 339.0], [456.0, 339.0]], "confidence": 0.9755489826202393, "text": "Effect of HowTo-AD pretraining on CMD-AD-Eval.", "bbox": [456.0, 325.0, 728.0, 339.0]}, {"polygon": [[410.0, 327.0], [456.0, 327.0], [456.0, 338.0], [410.0, 338.0]], "confidence": 0.8676488399505615, "text": "Table 5.", "bbox": [410.0, 327.0, 456.0, 338.0]}, {"polygon": [[72.0, 337.0], [374.0, 337.0], [374.0, 350.0], [72.0, 350.0]], "confidence": 0.9475980401039124, "text": "tIoU #movies #AD pairs | CIDEr R@1/5 CRITIC LLM-AD-eval †", "bbox": [72.0, 337.0, 374.0, 350.0]}, {"polygon": [[410.0, 340.0], [727.0, 340.0], [727.0, 354.0], [410.0, 354.0]], "confidence": 0.9765142202377319, "text": "†: from 'gpt-3.5-turbo' / 'llama-2-7b-chat', respectively. † uses the", "bbox": [410.0, 340.0, 727.0, 354.0]}, {"polygon": [[110.0, 354.0], [129.0, 354.0], [129.0, 366.0], [110.0, 366.0]], "confidence": 0.6235019564628601, "text": "315", "bbox": [110.0, 354.0, 129.0, 366.0]}, {"polygon": [[153.0, 354.0], [178.0, 354.0], [178.0, 366.0], [153.0, 366.0]], "confidence": 0.6988034248352051, "text": "447", "bbox": [153.0, 354.0, 178.0, 366.0]}, {"polygon": [[232.0, 354.0], [254.0, 354.0], [254.0, 367.0], [232.0, 367.0]], "confidence": 0.7466120719909668, "text": "71.2", "bbox": [232.0, 354.0, 254.0, 367.0]}, {"polygon": [[74.0, 355.0], [93.0, 355.0], [93.0, 366.0], [74.0, 366.0]], "confidence": 0.720376193523407, "text": "0.8", "bbox": [74.0, 355.0, 93.0, 366.0]}, {"polygon": [[194.0, 355.0], [218.0, 355.0], [218.0, 367.0], [194.0, 367.0]], "confidence": 0.638671338558197, "text": "61.5", "bbox": [194.0, 355.0, 218.0, 367.0]}, {"polygon": [[271.0, 355.0], [293.0, 355.0], [293.0, 366.0], [271.0, 366.0]], "confidence": 0.6691542863845825, "text": "42.0", "bbox": [271.0, 355.0, 293.0, 366.0]}, {"polygon": [[315.0, 355.0], [363.0, 353.0], [364.0, 366.0], [316.0, 368.0]], "confidence": 0.8318197131156921, "text": "2.56/3.04", "bbox": [315.0, 355.0, 363.0, 366.0]}, {"polygon": [[409.0, 355.0], [727.0, 355.0], [727.0, 368.0], [409.0, 368.0]], "confidence": 0.9796876907348633, "text": "same 180k-video subset as HowTo-AD, but without constructing", "bbox": [409.0, 355.0, 727.0, 368.0]}, {"polygon": [[74.0, 367.0], [94.0, 367.0], [94.0, 380.0], [74.0, 380.0]], "confidence": 0.7178059816360474, "text": "0.9", "bbox": [74.0, 367.0, 94.0, 380.0]}, {"polygon": [[110.0, 368.0], [128.0, 368.0], [128.0, 380.0], [110.0, 380.0]], "confidence": 0.6249592900276184, "text": "267", "bbox": [110.0, 368.0, 128.0, 380.0]}, {"polygon": [[154.0, 368.0], [175.0, 368.0], [175.0, 379.0], [154.0, 379.0]], "confidence": 0.8599450588226318, "text": "99", "bbox": [154.0, 368.0, 175.0, 379.0]}, {"polygon": [[194.0, 368.0], [218.0, 368.0], [218.0, 380.0], [194.0, 380.0]], "confidence": 0.7604817152023315, "text": "69.69.69.", "bbox": [194.0, 368.0, 218.0, 380.0]}, {"polygon": [[232.0, 368.0], [254.0, 368.0], [254.0, 380.0], [232.0, 380.0]], "confidence": 0.7418511509895325, "text": "80.4", "bbox": [232.0, 368.0, 254.0, 380.0]}, {"polygon": [[270.0, 368.0], [293.0, 368.0], [293.0, 380.0], [270.0, 380.0]], "confidence": 0.7487243413925171, "text": "47.6", "bbox": [270.0, 368.0, 293.0, 380.0]}, {"polygon": [[316.0, 367.0], [363.0, 367.0], [363.0, 381.0], [316.0, 381.0]], "confidence": 0.77918940782547, "text": "3.06/3.53", "bbox": [316.0, 367.0, 363.0, 381.0]}, {"polygon": [[409.0, 369.0], [580.0, 369.0], [580.0, 382.0], [409.0, 382.0]], "confidence": 0.9739506840705872, "text": "character banks or rewriting captions.", "bbox": [409.0, 369.0, 580.0, 382.0]}, {"polygon": [[65.0, 385.0], [383.0, 385.0], [383.0, 399.0], [65.0, 399.0]], "confidence": 0.9790716767311096, "text": "Table 3. Inter-rater agreement on AudioVault AD annotations.", "bbox": [65.0, 385.0, 383.0, 399.0]}, {"polygon": [[408.0, 395.0], [728.0, 395.0], [728.0, 410.0], [408.0, 410.0]], "confidence": 0.9683530926704407, "text": "– CIDEr, Recall@1/5, CRITIC, and LLM-AD-eval. Note, these", "bbox": [408.0, 395.0, 728.0, 410.0]}, {"polygon": [[65.0, 401.0], [382.0, 401.0], [382.0, 413.0], [65.0, 413.0]], "confidence": 0.9803641438484192, "text": "AD from different annotators do not usually synchronize. A higher", "bbox": [65.0, 401.0, 382.0, 413.0]}, {"polygon": [[408.0, 411.0], [728.0, 411.0], [728.0, 425.0], [408.0, 425.0]], "confidence": 0.9841547608375549, "text": "evaluations are carried out directly on the text version of the", "bbox": [408.0, 411.0, 728.0, 425.0]}, {"polygon": [[63.0, 415.0], [383.0, 415.0], [383.0, 427.0], [63.0, 427.0]], "confidence": 0.9843859076499939, "text": "tIoU threshold filters out fewer AD pairs but they are more likely to", "bbox": [63.0, 415.0, 383.0, 427.0]}, {"polygon": [[410.0, 426.0], [564.0, 426.0], [564.0, 441.0], [410.0, 441.0]], "confidence": 0.9666714072227478, "text": "AD, so no pixels are involved.", "bbox": [410.0, 426.0, 564.0, 441.0]}, {"polygon": [[63.0, 428.0], [382.0, 428.0], [382.0, 441.0], [63.0, 441.0]], "confidence": 0.9792224168777466, "text": "describe the exact same visual event. ↑ : LLM-AD-Eval scores are", "bbox": [63.0, 428.0, 382.0, 441.0]}, {"polygon": [[63.0, 441.0], [382.0, 441.0], [382.0, 455.0], [63.0, 455.0]], "confidence": 0.9829546809196472, "text": "computed from 'gpt-3.5-turbo' / 'llama-2-7b-chat' respectively. For", "bbox": [63.0, 441.0, 382.0, 455.0]}, {"polygon": [[425.0, 442.0], [728.0, 442.0], [728.0, 456.0], [425.0, 456.0]], "confidence": 0.9808432459831238, "text": "In the AudioVault-8K dataset, there are 402 movies with", "bbox": [425.0, 442.0, 728.0, 456.0]}, {"polygon": [[63.0, 456.0], [382.0, 456.0], [382.0, 470.0], [63.0, 470.0]], "confidence": 0.9835461378097534, "text": "all the metrics, a higher number indicates better quality. R@1/5 and", "bbox": [63.0, 456.0, 382.0, 470.0]}, {"polygon": [[409.0, 458.0], [653.0, 458.0], [653.0, 472.0], [409.0, 472.0]], "confidence": 0.9630305767059326, "text": "mone than one version of AD annotations.", "bbox": [409.0, 458.0, 653.0, 472.0]}, {"polygon": [[658.0, 458.0], [728.0, 458.0], [728.0, 471.0], [658.0, 471.0]], "confidence": 0.868593156337738, "text": "We conduct", "bbox": [658.0, 458.0, 728.0, 471.0]}, {"polygon": [[63.0, 471.0], [382.0, 471.0], [382.0, 484.0], [63.0, 484.0]], "confidence": 0.9815558195114136, "text": "CRITIC are upperbounded at 100 and LLM-AD-eval is between 1 to 5.", "bbox": [63.0, 471.0, 382.0, 484.0]}, {"polygon": [[409.0, 473.0], [728.0, 473.0], [728.0, 487.0], [409.0, 487.0]], "confidence": 0.9832742214202881, "text": "inter-rater experiments on this subset of AD annotations. Three", "bbox": [409.0, 473.0, 728.0, 487.0]}, {"polygon": [[408.0, 489.0], [728.0, 489.0], [728.0, 503.0], [408.0, 503.0]], "confidence": 0.9826248288154602, "text": "challenges emerge when conducting inter-rater comparisons for", "bbox": [408.0, 489.0, 728.0, 503.0]}, {"polygon": [[63.0, 496.0], [383.0, 496.0], [383.0, 511.0], [63.0, 511.0]], "confidence": 0.9767358899116516, "text": "in Section 3.1 , by aligning AD data with CMD clips [ 4 ].  It", "bbox": [63.0, 496.0, 383.0, 511.0]}, {"polygon": [[409.0, 504.0], [728.0, 504.0], [728.0, 519.0], [409.0, 519.0]], "confidence": 0.9835112690925598, "text": "the same visual scene: (i) Different versions of AD annotations", "bbox": [409.0, 504.0, 728.0, 519.0]}, {"polygon": [[63.0, 511.0], [383.0, 511.0], [383.0, 526.0], [63.0, 526.0]], "confidence": 0.982581377029419, "text": "contains 101k ADs for more than 1432 movies. We split a 100-", "bbox": [63.0, 511.0, 383.0, 526.0]}, {"polygon": [[409.0, 520.0], [728.0, 520.0], [728.0, 534.0], [409.0, 534.0]], "confidence": 0.9835255146026611, "text": "might correspond to different versions of the same movie which", "bbox": [409.0, 520.0, 728.0, 534.0]}, {"polygon": [[63.0, 528.0], [382.0, 528.0], [382.0, 541.0], [63.0, 541.0]], "confidence": 0.9819238185882568, "text": "movie evaluation set named CMD-AD-Eval and use the rest for", "bbox": [63.0, 528.0, 382.0, 541.0]}, {"polygon": [[409.0, 535.0], [728.0, 535.0], [728.0, 549.0], [409.0, 549.0]], "confidence": 0.9780994057655334, "text": "do not naturally synchronize as introduced in Section 3.1 . We", "bbox": [409.0, 535.0, 728.0, 549.0]}, {"polygon": [[63.0, 542.0], [383.0, 542.0], [383.0, 557.0], [63.0, 557.0]], "confidence": 0.9824769496917725, "text": "training. HowTo-AD is the new AD dataset introduced in Sec-", "bbox": [63.0, 542.0, 383.0, 557.0]}, {"polygon": [[409.0, 550.0], [728.0, 550.0], [728.0, 565.0], [409.0, 565.0]], "confidence": 0.9834569692611694, "text": "apply the audio-audio alignment pipeline in 3.1 to synchronize", "bbox": [409.0, 550.0, 728.0, 565.0]}, {"polygon": [[63.0, 557.0], [382.0, 557.0], [382.0, 572.0], [63.0, 572.0]], "confidence": 0.9826282262802124, "text": "tion 3.2 , transformed from HowTo100M [ 40 ]. It contains 180k", "bbox": [63.0, 557.0, 382.0, 572.0]}, {"polygon": [[409.0, 566.0], [728.0, 566.0], [728.0, 580.0], [409.0, 580.0]], "confidence": 0.9787597060203552, "text": "both AD annotations.  (ii) The timing of providing AD is", "bbox": [409.0, 566.0, 728.0, 580.0]}, {"polygon": [[63.0, 573.0], [382.0, 573.0], [382.0, 587.0], [63.0, 587.0]], "confidence": 0.9822121858596802, "text": "YouTube videos with augmented descriptions and character", "bbox": [63.0, 573.0, 382.0, 587.0]}, {"polygon": [[409.0, 582.0], [728.0, 582.0], [728.0, 596.0], [409.0, 596.0]], "confidence": 0.9840899109840393, "text": "subjective and arbitrary within a short time interval [ 21 ], to", "bbox": [409.0, 582.0, 728.0, 596.0]}, {"polygon": [[63.0, 588.0], [362.0, 588.0], [362.0, 603.0], [63.0, 603.0]], "confidence": 0.9824140071868896, "text": "exemplars. We mainly use it for AD generation pertaining.", "bbox": [63.0, 588.0, 362.0, 603.0]}, {"polygon": [[409.0, 598.0], [728.0, 598.0], [728.0, 611.0], [409.0, 611.0]], "confidence": 0.9838125109672546, "text": "obtain different ADs for the exact same visual moment, we have", "bbox": [409.0, 598.0, 728.0, 611.0]}, {"polygon": [[63.0, 612.0], [225.0, 612.0], [225.0, 628.0], [63.0, 628.0]], "confidence": 0.9589110612869263, "text": "6.2. Evaluation Measures", "bbox": [63.0, 612.0, 225.0, 628.0]}, {"polygon": [[409.0, 612.0], [728.0, 612.0], [728.0, 626.0], [409.0, 626.0]], "confidence": 0.9806520938873291, "text": "to filter the time segments of two AD versions with a temporal", "bbox": [409.0, 612.0, 728.0, 626.0]}, {"polygon": [[409.0, 627.0], [729.0, 627.0], [729.0, 642.0], [409.0, 642.0]], "confidence": 0.9797675013542175, "text": "Intersection-over-Union (tIoU). (iii) For about 20% of movies,", "bbox": [409.0, 627.0, 729.0, 642.0]}, {"polygon": [[63.0, 637.0], [383.0, 637.0], [383.0, 651.0], [63.0, 651.0]], "confidence": 0.9824283123016357, "text": "In addition to the two new evaluation measures introduced", "bbox": [63.0, 637.0, 383.0, 651.0]}, {"polygon": [[409.0, 644.0], [728.0, 644.0], [728.0, 657.0], [409.0, 657.0]], "confidence": 0.9833756685256958, "text": "the multiple AD versions from AudioVault are simply narrating", "bbox": [409.0, 644.0, 728.0, 657.0]}, {"polygon": [[63.0, 652.0], [382.0, 652.0], [382.0, 667.0], [63.0, 667.0]], "confidence": 0.9762530326843262, "text": "in Section 5 , CRITIC and LLM-AD-Eval , we also monitor", "bbox": [63.0, 652.0, 382.0, 667.0]}, {"polygon": [[409.0, 659.0], [728.0, 659.0], [728.0, 674.0], [409.0, 674.0]], "confidence": 0.98116135597229, "text": "the same scripts again with minor modifications, which does", "bbox": [409.0, 659.0, 728.0, 674.0]}, {"polygon": [[63.0, 668.0], [384.0, 668.0], [384.0, 682.0], [63.0, 682.0]], "confidence": 0.9794726967811584, "text": "Recall@k/N, CIDEr, and perplexity. Recall@k/N [ 21 ] is a", "bbox": [63.0, 668.0, 384.0, 682.0]}, {"polygon": [[409.0, 674.0], [728.0, 674.0], [728.0, 689.0], [409.0, 689.0]], "confidence": 0.9770342707633972, "text": "not reflect independent inter-rater comparisons. We filter out", "bbox": [409.0, 674.0, 728.0, 689.0]}, {"polygon": [[63.0, 683.0], [383.0, 683.0], [383.0, 697.0], [63.0, 697.0]], "confidence": 0.9848117232322693, "text": "retrieval metric that distinguishes the predicted text among a set", "bbox": [63.0, 683.0, 383.0, 697.0]}, {"polygon": [[409.0, 690.0], [713.0, 689.0], [713.0, 703.0], [409.0, 705.0]], "confidence": 0.9826015830039978, "text": "those movies by checking the exact sentence-matching rate.", "bbox": [409.0, 690.0, 713.0, 703.0]}, {"polygon": [[63.0, 699.0], [383.0, 699.0], [383.0, 713.0], [63.0, 713.0]], "confidence": 0.9809772968292236, "text": "of temporal neighbours. The parameters k and N mean within", "bbox": [63.0, 699.0, 383.0, 713.0]}, {"polygon": [[425.0, 705.0], [728.0, 705.0], [728.0, 719.0], [425.0, 719.0]], "confidence": 0.973360538482666, "text": "The inter-rater evaluation is shown in Table 3 . With a higher", "bbox": [425.0, 705.0, 728.0, 719.0]}, {"polygon": [[63.0, 714.0], [383.0, 714.0], [383.0, 728.0], [63.0, 728.0]], "confidence": 0.982072651386261, "text": "a temporal window of N neighbouring reference ADs, whether", "bbox": [63.0, 714.0, 383.0, 728.0]}, {"polygon": [[409.0, 721.0], [728.0, 721.0], [728.0, 734.0], [409.0, 734.0]], "confidence": 0.9821485877037048, "text": "tIoU threshold, we get fewer AD annotation pairs covering", "bbox": [409.0, 721.0, 728.0, 734.0]}, {"polygon": [[63.0, 730.0], [382.0, 730.0], [382.0, 744.0], [63.0, 744.0]], "confidence": 0.9818428754806519, "text": "the predicted AD can retrieve the corresponding reference", "bbox": [63.0, 730.0, 382.0, 744.0]}, {"polygon": [[409.0, 736.0], [728.0, 736.0], [728.0, 751.0], [409.0, 751.0]], "confidence": 0.9833455085754395, "text": "fewer movies, but the AD annotation pairs are more likely to", "bbox": [409.0, 736.0, 728.0, 751.0]}, {"polygon": [[63.0, 746.0], [382.0, 746.0], [382.0, 759.0], [63.0, 759.0]], "confidence": 0.9720694422721863, "text": "AD at top- k position. We use Recall@ 1/5 on CMD-AD-Eval", "bbox": [63.0, 746.0, 382.0, 759.0]}, {"polygon": [[409.0, 752.0], [728.0, 752.0], [728.0, 766.0], [409.0, 766.0]], "confidence": 0.9825468063354492, "text": "describe the same movie scene. The numbers can be regarded", "bbox": [409.0, 752.0, 728.0, 766.0]}, {"polygon": [[63.0, 761.0], [382.0, 761.0], [382.0, 776.0], [63.0, 776.0]], "confidence": 0.9798799157142639, "text": "and Recall@5/16 on MAD-Eval to compare with previous", "bbox": [63.0, 761.0, 382.0, 776.0]}, {"polygon": [[409.0, 767.0], [728.0, 767.0], [728.0, 781.0], [409.0, 781.0]], "confidence": 0.9824535250663757, "text": "as human-level upperbound. Note that with a lower tIoU (from", "bbox": [409.0, 767.0, 728.0, 781.0]}, {"polygon": [[63.0, 776.0], [384.0, 776.0], [384.0, 791.0], [63.0, 791.0]], "confidence": 0.9826944470405579, "text": "works. We use the official implementation provided by [ 21 ].", "bbox": [63.0, 776.0, 384.0, 791.0]}, {"polygon": [[408.0, 783.0], [728.0, 783.0], [728.0, 797.0], [408.0, 797.0]], "confidence": 0.9822301864624023, "text": "0.9 to 0.8), all the metrics drop significantly, highlighting the", "bbox": [408.0, 783.0, 728.0, 797.0]}, {"polygon": [[63.0, 792.0], [383.0, 792.0], [383.0, 806.0], [63.0, 806.0]], "confidence": 0.9680348038673401, "text": "CIDEr [ 61 ] is a popular text similarity metric that is based on", "bbox": [63.0, 792.0, 383.0, 806.0]}, {"polygon": [[409.0, 799.0], [728.0, 799.0], [728.0, 812.0], [409.0, 812.0]], "confidence": 0.9838941693305969, "text": "temporal sensitivity of AD tasks and the importance of precise", "bbox": [409.0, 799.0, 728.0, 812.0]}, {"polygon": [[63.0, 806.0], [382.0, 806.0], [382.0, 821.0], [63.0, 821.0]], "confidence": 0.981340229511261, "text": "word matching rate. We include Recall@k/N and CIDEr here", "bbox": [63.0, 806.0, 382.0, 821.0]}, {"polygon": [[409.0, 814.0], [728.0, 814.0], [728.0, 828.0], [409.0, 828.0]], "confidence": 0.9781717658042908, "text": "data alignment.  A few pairs of human-annotated AD from", "bbox": [409.0, 814.0, 728.0, 828.0]}, {"polygon": [[63.0, 822.0], [382.0, 822.0], [382.0, 836.0], [63.0, 836.0]], "confidence": 0.9832234382629395, "text": "as they have been used in recent work on AD [ 20 , 21 ] and we", "bbox": [63.0, 822.0, 382.0, 836.0]}, {"polygon": [[410.0, 829.0], [728.0, 829.0], [728.0, 843.0], [410.0, 843.0]], "confidence": 0.9770341515541077, "text": "AudioVault are shown in Figure 6 , where the left and right", "bbox": [410.0, 829.0, 728.0, 843.0]}, {"polygon": [[63.0, 838.0], [312.0, 838.0], [312.0, 852.0], [63.0, 852.0]], "confidence": 0.9797652363777161, "text": "also compare on the test datasets of those works.", "bbox": [63.0, 838.0, 312.0, 852.0]}, {"polygon": [[409.0, 844.0], [700.0, 844.0], [700.0, 858.0], [409.0, 858.0]], "confidence": 0.9824442267417908, "text": "panels are from two annotators for the same visual scene.", "bbox": [409.0, 844.0, 700.0, 858.0]}, {"polygon": [[63.0, 865.0], [238.0, 865.0], [238.0, 880.0], [63.0, 880.0]], "confidence": 0.9622001051902771, "text": "6.3. Inter-rater Evaluations", "bbox": [63.0, 865.0, 238.0, 880.0]}, {"polygon": [[409.0, 865.0], [567.0, 865.0], [567.0, 880.0], [409.0, 880.0]], "confidence": 0.9606133699417114, "text": "6.4. Quantitative Results", "bbox": [409.0, 865.0, 567.0, 880.0]}, {"polygon": [[63.0, 890.0], [384.0, 890.0], [384.0, 904.0], [63.0, 904.0]], "confidence": 0.9806347489356995, "text": "Many of the films in Audiovault have multiple ADs available.", "bbox": [63.0, 890.0, 384.0, 904.0]}, {"polygon": [[409.0, 890.0], [729.0, 890.0], [729.0, 904.0], [409.0, 904.0]], "confidence": 0.9728869199752808, "text": "Architecture Comparisons on Aligned-CMD. In Table 4 ,", "bbox": [409.0, 890.0, 729.0, 904.0]}, {"polygon": [[64.0, 905.0], [383.0, 905.0], [383.0, 920.0], [64.0, 920.0]], "confidence": 0.9745532274246216, "text": "Typically these are UK and US versions. In this section,", "bbox": [64.0, 905.0, 383.0, 920.0]}, {"polygon": [[409.0, 905.0], [728.0, 905.0], [728.0, 920.0], [409.0, 920.0]], "confidence": 0.9783646464347839, "text": "we compare the proposed Movie-BLIP2 and Movie-Llama2", "bbox": [409.0, 905.0, 728.0, 920.0]}, {"polygon": [[63.0, 922.0], [383.0, 922.0], [383.0, 935.0], [63.0, 935.0]], "confidence": 0.9791302680969238, "text": "we use the agreement between the human-provided AD", "bbox": [63.0, 922.0, 383.0, 935.0]}, {"polygon": [[410.0, 922.0], [729.0, 922.0], [729.0, 935.0], [410.0, 935.0]], "confidence": 0.9827467203140259, "text": "architectures with previous methods on the CMD-AD dataset.", "bbox": [410.0, 922.0, 729.0, 935.0]}, {"polygon": [[64.0, 936.0], [382.0, 936.0], [382.0, 950.0], [64.0, 950.0]], "confidence": 0.9844392538070679, "text": "versions to assess the usefulness of the four evaluation metrics", "bbox": [64.0, 936.0, 382.0, 950.0]}, {"polygon": [[410.0, 936.0], [728.0, 936.0], [728.0, 950.0], [410.0, 950.0]], "confidence": 0.9828338623046875, "text": "All of these models are trained on the CMD-AD-Train set and", "bbox": [410.0, 936.0, 728.0, 950.0]}], "languages": ["en"], "image_bbox": [0.0, 0.0, 816.0, 1056.0], "page": 7}, {"text_lines": [{"polygon": [[403.0, 158.0], [569.0, 158.0], [569.0, 167.0], [403.0, 167.0]], "confidence": 0.9782019257545471, "text": "Ground Truth : She starts to walk along the be", "bbox": [403.0, 158.0, 569.0, 167.0]}, {"polygon": [[178.0, 160.0], [267.0, 160.0], [267.0, 166.0], [178.0, 166.0]], "confidence": 0.9359771013259888, "text": "Vientha overhears and loc", "bbox": [178.0, 160.0, 267.0, 166.0]}, {"polygon": [[78.0, 168.0], [234.0, 168.0], [234.0, 176.0], [78.0, 176.0]], "confidence": 0.9764029383659363, "text": "Prediction : Vayentha looks up at the ceiling", "bbox": [78.0, 168.0, 234.0, 176.0]}, {"polygon": [[403.0, 169.0], [528.0, 169.0], [528.0, 177.0], [403.0, 177.0]], "confidence": 0.9722787737846375, "text": "Prediction : She steps onto a ladder", "bbox": [403.0, 169.0, 528.0, 177.0]}, {"polygon": [[80.0, 243.0], [266.0, 244.0], [266.0, 252.0], [80.0, 250.0]], "confidence": 0.9567217230796814, "text": "Ground 'Truth : A tear falls down the princess's chee", "bbox": [80.0, 243.0, 266.0, 252.0]}, {"polygon": [[405.0, 243.0], [626.0, 243.0], [626.0, 251.0], [405.0, 251.0]], "confidence": 0.9826174974441528, "text": "Ground Truth : Other mounted officers watch Bourne steal the", "bbox": [405.0, 243.0, 626.0, 251.0]}, {"polygon": [[79.0, 253.0], [261.0, 253.0], [261.0, 262.0], [79.0, 262.0]], "confidence": 0.9790553450584412, "text": "Prediction : She looks at him with tears in her eyes.", "bbox": [79.0, 253.0, 261.0, 262.0]}, {"polygon": [[403.0, 252.0], [597.0, 252.0], [597.0, 262.0], [403.0, 262.0]], "confidence": 0.9809464812278748, "text": "Prediction : Bourne jumps off the motorcycle and runs", "bbox": [403.0, 252.0, 597.0, 262.0]}, {"polygon": [[63.0, 269.0], [728.0, 269.0], [728.0, 283.0], [63.0, 283.0]], "confidence": 0.9896791577339172, "text": "Figure 7. Qualitative results. AutoAD-III predictions correctly identify the semantics of the scene, by referring to the characters ('Vayentha',", "bbox": [63.0, 269.0, 728.0, 283.0]}, {"polygon": [[64.0, 285.0], [728.0, 285.0], [728.0, 298.0], [64.0, 298.0]], "confidence": 0.9901850819587708, "text": "'Bourne'), their relations ('looks at him'), actions ('steps onto', 'jumps off'), emotions ('tears'), objects ('ladder', 'motorcycle'). The comparison", "bbox": [64.0, 285.0, 728.0, 298.0]}, {"polygon": [[65.0, 300.0], [727.0, 300.0], [727.0, 312.0], [65.0, 312.0]], "confidence": 0.9892799258232117, "text": "with the ground truth further highlights the limitations of the n-gram based metrics since the same meaning can be conveyed with different wordings.", "bbox": [65.0, 300.0, 727.0, 312.0]}, {"polygon": [[212.0, 326.0], [267.0, 326.0], [267.0, 336.0], [212.0, 336.0]], "confidence": 0.9037845730781555, "text": "CMD-AD-Eval", "bbox": [212.0, 326.0, 267.0, 336.0]}, {"polygon": [[327.0, 326.0], [369.0, 326.0], [369.0, 336.0], [327.0, 336.0]], "confidence": 0.8501502275466919, "text": "MAD-Eval", "bbox": [327.0, 326.0, 369.0, 336.0]}, {"polygon": [[409.0, 326.0], [729.0, 326.0], [729.0, 340.0], [409.0, 340.0]], "confidence": 0.9729205369949341, "text": "in this work, and MAD-Eval proposed in [ 20 ] ( Table 6 ).", "bbox": [409.0, 326.0, 729.0, 340.0]}, {"polygon": [[72.0, 331.0], [102.0, 331.0], [102.0, 342.0], [72.0, 342.0]], "confidence": 0.8541516065597534, "text": "Method", "bbox": [72.0, 331.0, 102.0, 342.0]}, {"polygon": [[170.0, 337.0], [191.0, 337.0], [191.0, 346.0], [170.0, 346.0]], "confidence": 0.6501676440238953, "text": "CIDEr", "bbox": [170.0, 337.0, 191.0, 346.0]}, {"polygon": [[193.0, 337.0], [314.0, 335.0], [314.0, 347.0], [193.0, 349.0]], "confidence": 0.86583411693573, "text": "R@1/5 CRITIC LLM-AD-Eval †", "bbox": [193.0, 337.0, 314.0, 347.0]}, {"polygon": [[319.0, 337.0], [376.0, 337.0], [376.0, 346.0], [319.0, 346.0]], "confidence": 0.8870089650154114, "text": "CIDEr R@5/16", "bbox": [319.0, 337.0, 376.0, 346.0]}, {"polygon": [[409.0, 341.0], [728.0, 341.0], [728.0, 355.0], [409.0, 355.0]], "confidence": 0.9803634881973267, "text": "Note that MAD-Eval is a 10-movie subset from LSMDC,", "bbox": [409.0, 341.0, 728.0, 355.0]}, {"polygon": [[72.0, 352.0], [159.0, 352.0], [159.0, 362.0], [72.0, 362.0]], "confidence": 0.9529268741607666, "text": "Video-BLIP2 [ 72 ] (no ft)", "bbox": [72.0, 352.0, 159.0, 362.0]}, {"polygon": [[233.0, 353.0], [245.0, 353.0], [245.0, 360.0], [233.0, 360.0]], "confidence": 0.7183479070663452, "text": "0.0", "bbox": [233.0, 353.0, 245.0, 360.0]}, {"polygon": [[272.0, 352.0], [304.0, 352.0], [304.0, 361.0], [272.0, 361.0]], "confidence": 0.9015428423881531, "text": ".40 / 1.89", "bbox": [272.0, 352.0, 304.0, 361.0]}, {"polygon": [[409.0, 357.0], [729.0, 357.0], [729.0, 371.0], [409.0, 371.0]], "confidence": 0.9775865077972412, "text": "where we can get short movie clips for evaluation. However,", "bbox": [409.0, 357.0, 729.0, 371.0]}, {"polygon": [[72.0, 363.0], [163.0, 363.0], [163.0, 374.0], [72.0, 374.0]], "confidence": 0.9526413679122925, "text": "Video-Llama2 [ 77 ] (no ft)", "bbox": [72.0, 363.0, 163.0, 374.0]}, {"polygon": [[269.0, 363.0], [305.0, 363.0], [305.0, 372.0], [269.0, 372.0]], "confidence": 0.8992100358009338, "text": "1.43 / 1.91", "bbox": [269.0, 363.0, 305.0, 372.0]}, {"polygon": [[172.0, 364.0], [184.0, 364.0], [184.0, 371.0], [172.0, 371.0]], "confidence": 0.7435570955276489, "text": "5.2", "bbox": [172.0, 364.0, 184.0, 371.0]}, {"polygon": [[200.0, 364.0], [215.0, 364.0], [215.0, 372.0], [200.0, 372.0]], "confidence": 0.7899535894393921, "text": "23.6", "bbox": [200.0, 364.0, 215.0, 372.0]}, {"polygon": [[232.0, 364.0], [246.0, 364.0], [246.0, 371.0], [232.0, 371.0]], "confidence": 0.7161538600921631, "text": "0.0", "bbox": [232.0, 364.0, 246.0, 371.0]}, {"polygon": [[325.0, 364.0], [337.0, 364.0], [337.0, 372.0], [325.0, 372.0]], "confidence": 0.5100083351135254, "text": "4.8", "bbox": [325.0, 364.0, 337.0, 372.0]}, {"polygon": [[352.0, 364.0], [367.0, 364.0], [367.0, 371.0], [352.0, 371.0]], "confidence": 0.7268953323364258, "text": "3.8", "bbox": [352.0, 364.0, 367.0, 371.0]}, {"polygon": [[409.0, 373.0], [728.0, 373.0], [728.0, 386.0], [409.0, 386.0]], "confidence": 0.9807215929031372, "text": "we can not perform any training on MAD-Train since no", "bbox": [409.0, 373.0, 728.0, 386.0]}, {"polygon": [[72.0, 377.0], [160.0, 377.0], [160.0, 388.0], [72.0, 388.0]], "confidence": 0.9274634718894958, "text": "MM-Narrator-GPT4 [ 76 ]", "bbox": [72.0, 377.0, 160.0, 388.0]}, {"polygon": [[321.0, 378.0], [337.0, 378.0], [337.0, 387.0], [321.0, 387.0]], "confidence": 0.7792518734931946, "text": "13.9", "bbox": [321.0, 378.0, 337.0, 387.0]}, {"polygon": [[73.0, 386.0], [125.0, 389.0], [125.0, 398.0], [73.0, 396.0]], "confidence": 0.9024456739425659, "text": "AutoAD-I [ 20 ]", "bbox": [73.0, 386.0, 125.0, 398.0]}, {"polygon": [[322.0, 388.0], [337.0, 388.0], [337.0, 396.0], [322.0, 396.0]], "confidence": 0.665300190448761, "text": "14.3", "bbox": [322.0, 388.0, 337.0, 396.0]}, {"polygon": [[352.0, 388.0], [368.0, 388.0], [368.0, 396.0], [352.0, 396.0]], "confidence": 0.731777548789978, "text": "42.1", "bbox": [352.0, 388.0, 368.0, 396.0]}, {"polygon": [[409.0, 388.0], [729.0, 388.0], [729.0, 403.0], [409.0, 403.0]], "confidence": 0.9817115068435669, "text": "pixels are available. The proposed method Movie-BLIP2 and", "bbox": [409.0, 388.0, 729.0, 403.0]}, {"polygon": [[73.0, 398.0], [128.0, 398.0], [128.0, 409.0], [73.0, 409.0]], "confidence": 0.9098247289657593, "text": "AutoAD-II [ 21 ]", "bbox": [73.0, 398.0, 128.0, 409.0]}, {"polygon": [[171.0, 398.0], [187.0, 398.0], [187.0, 407.0], [171.0, 407.0]], "confidence": 0.6048595905303955, "text": "135", "bbox": [171.0, 398.0, 187.0, 407.0]}, {"polygon": [[268.0, 398.0], [306.0, 398.0], [306.0, 409.0], [268.0, 409.0]], "confidence": 0.8835012316703796, "text": "1.53 / 2.08", "bbox": [268.0, 398.0, 306.0, 409.0]}, {"polygon": [[199.0, 399.0], [216.0, 399.0], [216.0, 409.0], [199.0, 409.0]], "confidence": 0.7200977802276611, "text": "26.1", "bbox": [199.0, 399.0, 216.0, 409.0]}, {"polygon": [[232.0, 399.0], [246.0, 399.0], [246.0, 409.0], [232.0, 409.0]], "confidence": 0.4898143410682678, "text": "88.2", "bbox": [232.0, 399.0, 246.0, 409.0]}, {"polygon": [[321.0, 399.0], [337.0, 399.0], [337.0, 407.0], [321.0, 407.0]], "confidence": 0.6693731546401978, "text": "19.2", "bbox": [321.0, 399.0, 337.0, 407.0]}, {"polygon": [[352.0, 399.0], [369.0, 399.0], [369.0, 408.0], [352.0, 408.0]], "confidence": 0.6661291122436523, "text": "51.3", "bbox": [352.0, 399.0, 369.0, 408.0]}, {"polygon": [[409.0, 403.0], [728.0, 403.0], [728.0, 418.0], [409.0, 418.0]], "confidence": 0.9817444086074829, "text": "Movie-Llama2 perform much better than the previous methods", "bbox": [409.0, 403.0, 728.0, 418.0]}, {"polygon": [[72.0, 409.0], [146.0, 409.0], [146.0, 418.0], [72.0, 418.0]], "confidence": 0.9355859160423279, "text": "Movie-BLIP2 (ours)", "bbox": [72.0, 409.0, 146.0, 418.0]}, {"polygon": [[268.0, 409.0], [305.0, 409.0], [305.0, 417.0], [268.0, 417.0]], "confidence": 0.8520938754081726, "text": "2.25 / 2.78", "bbox": [268.0, 409.0, 305.0, 417.0]}, {"polygon": [[232.0, 410.0], [247.0, 410.0], [247.0, 417.0], [232.0, 417.0]], "confidence": 0.7318407893180847, "text": "30.2", "bbox": [232.0, 410.0, 247.0, 417.0]}, {"polygon": [[352.0, 410.0], [371.0, 410.0], [371.0, 417.0], [352.0, 417.0]], "confidence": 0.734325110912323, "text": "52.0*", "bbox": [352.0, 410.0, 371.0, 417.0]}, {"polygon": [[409.0, 418.0], [729.0, 418.0], [729.0, 432.0], [409.0, 432.0]], "confidence": 0.9814123511314392, "text": "including MM-Narrator with GPT4 as the language model.", "bbox": [409.0, 418.0, 729.0, 432.0]}, {"polygon": [[269.0, 419.0], [306.0, 419.0], [306.0, 430.0], [269.0, 430.0]], "confidence": 0.8141339421272278, "text": "2.29 / 2.92", "bbox": [269.0, 419.0, 306.0, 430.0]}, {"polygon": [[72.0, 420.0], [148.0, 420.0], [148.0, 431.0], [72.0, 431.0]], "confidence": 0.9390961527824402, "text": "Movie-Llama2 (ours)", "bbox": [72.0, 420.0, 148.0, 431.0]}, {"polygon": [[171.0, 420.0], [187.0, 420.0], [187.0, 428.0], [171.0, 428.0]], "confidence": 0.7396484613418579, "text": "25.0", "bbox": [171.0, 420.0, 187.0, 428.0]}, {"polygon": [[352.0, 420.0], [372.0, 420.0], [372.0, 429.0], [352.0, 429.0]], "confidence": 0.6297099590301514, "text": "52.88", "bbox": [352.0, 420.0, 372.0, 429.0]}, {"polygon": [[409.0, 434.0], [728.0, 434.0], [728.0, 448.0], [409.0, 448.0]], "confidence": 0.9816449880599976, "text": "We also evaluate video captioning models like Video-BLIP2", "bbox": [409.0, 434.0, 728.0, 448.0]}, {"polygon": [[65.0, 440.0], [109.0, 440.0], [109.0, 451.0], [65.0, 451.0]], "confidence": 0.8661897778511047, "text": "Table 6.", "bbox": [65.0, 440.0, 109.0, 451.0]}, {"polygon": [[112.0, 440.0], [382.0, 440.0], [382.0, 452.0], [112.0, 452.0]], "confidence": 0.9766098856925964, "text": "Comparison with other methods on CMD-AD-Eval", "bbox": [112.0, 440.0, 382.0, 452.0]}, {"polygon": [[409.0, 450.0], [730.0, 450.0], [730.0, 463.0], [409.0, 463.0]], "confidence": 0.9811803698539734, "text": "and Video-Llama2 but neither of them performs well on AD,", "bbox": [409.0, 450.0, 730.0, 463.0]}, {"polygon": [[64.0, 453.0], [382.0, 453.0], [382.0, 466.0], [64.0, 466.0]], "confidence": 0.9793360829353333, "text": "and MAD-Eval. Additionally, we evaluate the out-of-the-box", "bbox": [64.0, 453.0, 382.0, 466.0]}, {"polygon": [[409.0, 465.0], [645.0, 465.0], [645.0, 479.0], [409.0, 479.0]], "confidence": 0.9777639508247375, "text": "highlighting the challenges of Movie AD task.", "bbox": [409.0, 465.0, 645.0, 479.0]}, {"polygon": [[65.0, 468.0], [382.0, 468.0], [382.0, 480.0], [65.0, 480.0]], "confidence": 0.9818782210350037, "text": "Video-BLIP2 and Video-Llama2 video captioning models (without", "bbox": [65.0, 468.0, 382.0, 480.0]}, {"polygon": [[64.0, 481.0], [382.0, 482.0], [382.0, 495.0], [64.0, 494.0]], "confidence": 0.9741159081459045, "text": "any AD finetuning) directly on both datasets. † : from 'gpt-3.5-turbo'", "bbox": [64.0, 481.0, 382.0, 495.0]}, {"polygon": [[409.0, 490.0], [564.0, 490.0], [564.0, 506.0], [409.0, 506.0]], "confidence": 0.9606345891952515, "text": "6.5. Qualitative Analysis", "bbox": [409.0, 490.0, 564.0, 506.0]}, {"polygon": [[63.0, 496.0], [383.0, 496.0], [383.0, 509.0], [63.0, 509.0]], "confidence": 0.9664011597633362, "text": "/ 'Ilama-2-7b-chat' respectively. *: these results are obtained on", "bbox": [63.0, 496.0, 383.0, 509.0]}, {"polygon": [[63.0, 510.0], [286.0, 510.0], [286.0, 523.0], [63.0, 523.0]], "confidence": 0.9429048299789429, "text": "MAD-Eval without any training on MAD-Train.", "bbox": [63.0, 510.0, 286.0, 523.0]}, {"polygon": [[409.0, 514.0], [728.0, 514.0], [728.0, 529.0], [409.0, 529.0]], "confidence": 0.9798740744590759, "text": "Figure 7 illustrates several random examples from the", "bbox": [409.0, 514.0, 728.0, 529.0]}, {"polygon": [[408.0, 531.0], [728.0, 531.0], [728.0, 544.0], [408.0, 544.0]], "confidence": 0.9769861698150635, "text": "CMD-AD-Eval set. For each sample, we display the predictions", "bbox": [408.0, 531.0, 728.0, 544.0]}, {"polygon": [[63.0, 539.0], [383.0, 539.0], [383.0, 554.0], [63.0, 554.0]], "confidence": 0.9816652536392212, "text": "evaluated on the CMD-AD-Eval set. We implement and train", "bbox": [63.0, 539.0, 383.0, 554.0]}, {"polygon": [[408.0, 546.0], [730.0, 546.0], [730.0, 560.0], [408.0, 560.0]], "confidence": 0.9822245240211487, "text": "of our Movie-Llama2 model, as well as the ground truth AD.", "bbox": [408.0, 546.0, 730.0, 560.0]}, {"polygon": [[63.0, 556.0], [384.0, 556.0], [384.0, 569.0], [63.0, 569.0]], "confidence": 0.98089599609375, "text": "AutoAD-II on CMD-AD-Train based on the public codebase.", "bbox": [63.0, 556.0, 384.0, 569.0]}, {"polygon": [[409.0, 562.0], [729.0, 562.0], [729.0, 575.0], [409.0, 575.0]], "confidence": 0.9839978218078613, "text": "We observe that, while different wording than the ground truth,", "bbox": [409.0, 562.0, 729.0, 575.0]}, {"polygon": [[65.0, 571.0], [382.0, 571.0], [382.0, 585.0], [65.0, 585.0]], "confidence": 0.9814040064811707, "text": "The results show that both Movie-BLIP2 and Movie-Llama2", "bbox": [65.0, 571.0, 382.0, 585.0]}, {"polygon": [[409.0, 577.0], [728.0, 577.0], [728.0, 591.0], [409.0, 591.0]], "confidence": 0.9834734201431274, "text": "the semantics of the AD content remain largely similar for Our", "bbox": [409.0, 577.0, 728.0, 591.0]}, {"polygon": [[63.0, 586.0], [383.0, 586.0], [383.0, 600.0], [63.0, 600.0]], "confidence": 0.9812114834785461, "text": "perform much better than AutoAD-II architecture. The stronger", "bbox": [63.0, 586.0, 383.0, 600.0]}, {"polygon": [[409.0, 592.0], [728.0, 592.0], [728.0, 607.0], [409.0, 607.0]], "confidence": 0.9818940162658691, "text": "method. Interestingly, in the first example, the ASR pipeline [ 5 ]", "bbox": [409.0, 592.0, 728.0, 607.0]}, {"polygon": [[63.0, 601.0], [383.0, 601.0], [383.0, 615.0], [63.0, 615.0]], "confidence": 0.978274405002594, "text": "performance is attributed to multiple factors – stronger visual", "bbox": [63.0, 601.0, 383.0, 615.0]}, {"polygon": [[409.0, 608.0], [728.0, 608.0], [728.0, 622.0], [409.0, 622.0]], "confidence": 0.9829386472702026, "text": "transcribed the name incorrectly as 'Vientha' but our model", "bbox": [409.0, 608.0, 728.0, 622.0]}, {"polygon": [[63.0, 616.0], [384.0, 616.0], [384.0, 631.0], [63.0, 631.0]], "confidence": 0.9818840622901917, "text": "backbone, stronger language model, and taking visual grid", "bbox": [63.0, 616.0, 384.0, 631.0]}, {"polygon": [[409.0, 623.0], [728.0, 623.0], [728.0, 637.0], [409.0, 637.0]], "confidence": 0.9809232354164124, "text": "fixed the name through the character bank. More examples can", "bbox": [409.0, 623.0, 728.0, 637.0]}, {"polygon": [[63.0, 633.0], [384.0, 633.0], [384.0, 646.0], [63.0, 646.0]], "confidence": 0.9829058647155762, "text": "feature as input instead of a single vector as in AutoAD-II.", "bbox": [63.0, 633.0, 384.0, 646.0]}, {"polygon": [[409.0, 638.0], [544.0, 638.0], [544.0, 652.0], [409.0, 652.0]], "confidence": 0.9605815410614014, "text": "be found in the Appendix.", "bbox": [409.0, 638.0, 544.0, 652.0]}, {"polygon": [[64.0, 648.0], [383.0, 648.0], [383.0, 662.0], [64.0, 662.0]], "confidence": 0.9793542623519897, "text": "Movie-Llama2 has a much stronger language model than", "bbox": [64.0, 648.0, 383.0, 662.0]}, {"polygon": [[63.0, 663.0], [384.0, 663.0], [384.0, 678.0], [63.0, 678.0]], "confidence": 0.9800398945808411, "text": "Movie-BLIP2 (Llama2-7B vs OPT-2.7B), but it achieves a", "bbox": [63.0, 663.0, 384.0, 678.0]}, {"polygon": [[409.0, 665.0], [501.0, 665.0], [501.0, 681.0], [409.0, 681.0]], "confidence": 0.9272266030311584, "text": "7. Conclusion", "bbox": [409.0, 665.0, 501.0, 681.0]}, {"polygon": [[63.0, 679.0], [382.0, 679.0], [382.0, 693.0], [63.0, 693.0]], "confidence": 0.9746733903884888, "text": "similar performance wrt Movie-BLIP2.  Note that all these", "bbox": [63.0, 679.0, 382.0, 693.0]}, {"polygon": [[409.0, 690.0], [730.0, 690.0], [730.0, 705.0], [409.0, 705.0]], "confidence": 0.982535183429718, "text": "This work advances automatic AD generation for movies by:", "bbox": [409.0, 690.0, 730.0, 705.0]}, {"polygon": [[63.0, 694.0], [295.0, 694.0], [295.0, 708.0], [63.0, 708.0]], "confidence": 0.974983274936676, "text": "models are not pretrained on HowTo-AD yet.", "bbox": [63.0, 694.0, 295.0, 708.0]}, {"polygon": [[409.0, 707.0], [728.0, 707.0], [728.0, 720.0], [409.0, 720.0]], "confidence": 0.9826188087463379, "text": "(i) collecting AD for pixel data through audio-audio alignment", "bbox": [409.0, 707.0, 728.0, 720.0]}, {"polygon": [[63.0, 721.0], [382.0, 721.0], [382.0, 737.0], [63.0, 737.0]], "confidence": 0.9708425402641296, "text": "Effect of HowTo-AD Pretraining. Taking the Movie-BLIP2", "bbox": [63.0, 721.0, 382.0, 737.0]}, {"polygon": [[409.0, 722.0], [729.0, 722.0], [729.0, 736.0], [409.0, 736.0]], "confidence": 0.9841011166572571, "text": "between full movies (without pixels) and public movie snippets,", "bbox": [409.0, 722.0, 729.0, 736.0]}, {"polygon": [[63.0, 737.0], [382.0, 737.0], [382.0, 752.0], [63.0, 752.0]], "confidence": 0.9712030291557312, "text": "and Movie-Llama2 settings from Table 4 , we compare the", "bbox": [63.0, 737.0, 382.0, 752.0]}, {"polygon": [[409.0, 738.0], [728.0, 738.0], [728.0, 751.0], [409.0, 751.0]], "confidence": 0.9831799864768982, "text": "and pseudo-labelling instruction videos; (ii) showing that recent", "bbox": [409.0, 738.0, 728.0, 751.0]}, {"polygon": [[409.0, 753.0], [728.0, 753.0], [728.0, 767.0], [409.0, 767.0]], "confidence": 0.9827783107757568, "text": "video-language architectures provide a significant performance", "bbox": [409.0, 753.0, 728.0, 767.0]}, {"polygon": [[63.0, 754.0], [382.0, 754.0], [382.0, 768.0], [63.0, 768.0]], "confidence": 0.9829557538032532, "text": "effect of HowTo-AD by pretraining the same architecture on the", "bbox": [63.0, 754.0, 382.0, 768.0]}, {"polygon": [[409.0, 768.0], [729.0, 768.0], [729.0, 783.0], [409.0, 783.0]], "confidence": 0.982240617275238, "text": "boost, bringing AD generation systems closer to real-world ap-", "bbox": [409.0, 768.0, 729.0, 783.0]}, {"polygon": [[63.0, 769.0], [382.0, 769.0], [382.0, 784.0], [63.0, 784.0]], "confidence": 0.9808816313743591, "text": "HowTo-AD dataset then finetuning on CMD-AD-Train set. We", "bbox": [63.0, 769.0, 382.0, 784.0]}, {"polygon": [[63.0, 784.0], [382.0, 784.0], [382.0, 798.0], [63.0, 798.0]], "confidence": 0.983345627784729, "text": "also pretrain with the same subset from HowToCaption without", "bbox": [63.0, 784.0, 382.0, 798.0]}, {"polygon": [[409.0, 784.0], [728.0, 784.0], [728.0, 798.0], [409.0, 798.0]], "confidence": 0.9841258525848389, "text": "plications; and (iii) proposing new evaluation methods tailored", "bbox": [409.0, 784.0, 728.0, 798.0]}, {"polygon": [[63.0, 799.0], [383.0, 799.0], [383.0, 814.0], [63.0, 814.0]], "confidence": 0.9770122170448303, "text": "using the character bank or rewriting captions. The results in", "bbox": [63.0, 799.0, 383.0, 814.0]}, {"polygon": [[409.0, 799.0], [728.0, 798.0], [728.0, 812.0], [409.0, 814.0]], "confidence": 0.9774683713912964, "text": "for AD. One of the limitations that necessitates future work", "bbox": [409.0, 799.0, 728.0, 812.0]}, {"polygon": [[409.0, 814.0], [730.0, 814.0], [730.0, 828.0], [409.0, 828.0]], "confidence": 0.9829834699630737, "text": "is the coherence across AD narrations throughout the movie:", "bbox": [409.0, 814.0, 730.0, 828.0]}, {"polygon": [[64.0, 815.0], [383.0, 815.0], [383.0, 829.0], [64.0, 829.0]], "confidence": 0.9774962067604065, "text": "Table 5 show that large-scale pretraining on our HowTo-AD", "bbox": [64.0, 815.0, 383.0, 829.0]}, {"polygon": [[410.0, 830.0], [729.0, 830.0], [729.0, 843.0], [410.0, 843.0]], "confidence": 0.9832909107208252, "text": "AD should not repeat the same information, or provide story-", "bbox": [410.0, 830.0, 729.0, 843.0]}, {"polygon": [[63.0, 831.0], [382.0, 831.0], [382.0, 845.0], [63.0, 845.0]], "confidence": 0.9842168092727661, "text": "dataset substantially boosts the performance on all four metrics", "bbox": [63.0, 831.0, 382.0, 845.0]}, {"polygon": [[409.0, 845.0], [729.0, 845.0], [729.0, 859.0], [409.0, 859.0]], "confidence": 0.9843789935112, "text": "irrelevant details. To this end, external knowledge such as plot", "bbox": [409.0, 845.0, 729.0, 859.0]}, {"polygon": [[63.0, 846.0], [382.0, 846.0], [382.0, 860.0], [63.0, 860.0]], "confidence": 0.9774187207221985, "text": "for both models. e.g. improving CRITIC from 25.2 to 32.7 and", "bbox": [63.0, 846.0, 382.0, 860.0]}, {"polygon": [[409.0, 861.0], [729.0, 861.0], [729.0, 875.0], [409.0, 875.0]], "confidence": 0.9835772514343262, "text": "summaries may be utilized to incorporate story-centric elements.", "bbox": [409.0, 861.0, 729.0, 875.0]}, {"polygon": [[63.0, 862.0], [382.0, 862.0], [382.0, 875.0], [63.0, 875.0]], "confidence": 0.9804436564445496, "text": "CIDEr from 21.7 to 25.0 for Movie-Llama2. But pretraining on", "bbox": [63.0, 862.0, 382.0, 875.0]}, {"polygon": [[409.0, 876.0], [728.0, 876.0], [728.0, 890.0], [409.0, 890.0]], "confidence": 0.9834269285202026, "text": "Future directions could also explore the harmony between the", "bbox": [409.0, 876.0, 728.0, 890.0]}, {"polygon": [[63.0, 877.0], [383.0, 877.0], [383.0, 891.0], [63.0, 891.0]], "confidence": 0.981642484664917, "text": "HowToCaption does not help much on the finetuned movie AD", "bbox": [63.0, 877.0, 383.0, 891.0]}, {"polygon": [[63.0, 892.0], [383.0, 892.0], [383.0, 906.0], [63.0, 906.0]], "confidence": 0.9842620491981506, "text": "task, possibly because of the domain gap from the data and task.", "bbox": [63.0, 892.0, 383.0, 906.0]}, {"polygon": [[409.0, 892.0], [729.0, 892.0], [729.0, 906.0], [409.0, 906.0]], "confidence": 0.9842439889907837, "text": "narration tone and the movie content for an engaging experience.", "bbox": [409.0, 892.0, 729.0, 906.0]}, {"polygon": [[410.0, 918.0], [728.0, 918.0], [728.0, 932.0], [410.0, 932.0]], "confidence": 0.9810646176338196, "text": "Acknowledgements. This research is funded by EPSRC PG", "bbox": [410.0, 918.0, 728.0, 932.0]}, {"polygon": [[63.0, 921.0], [276.0, 921.0], [276.0, 935.0], [63.0, 935.0]], "confidence": 0.9627038836479187, "text": "Comparison with Other Methods.", "bbox": [63.0, 921.0, 276.0, 935.0]}, {"polygon": [[277.0, 921.0], [382.0, 921.0], [382.0, 935.0], [277.0, 935.0]], "confidence": 0.9184784293174744, "text": "We compare with", "bbox": [277.0, 921.0, 382.0, 935.0]}, {"polygon": [[410.0, 933.0], [720.0, 933.0], [720.0, 948.0], [410.0, 948.0]], "confidence": 0.9788464903831482, "text": "VisualAI EP/T028572/1, and ANR-21-CE23-0003-01 CorVis.", "bbox": [410.0, 933.0, 720.0, 948.0]}, {"polygon": [[63.0, 936.0], [382.0, 936.0], [382.0, 951.0], [63.0, 951.0]], "confidence": 0.9790371656417847, "text": "other methods on two datasets: CMD-AD-Eval introduced", "bbox": [63.0, 936.0, 382.0, 951.0]}], "languages": ["en"], "image_bbox": [0.0, 0.0, 816.0, 1056.0], "page": 8}, {"text_lines": [{"polygon": [[63.0, 95.0], [138.0, 95.0], [138.0, 111.0], [63.0, 111.0]], "confidence": 0.9068116545677185, "text": "References", "bbox": [63.0, 95.0, 138.0, 111.0]}, {"polygon": [[434.0, 96.0], [729.0, 96.0], [729.0, 111.0], [434.0, 111.0]], "confidence": 0.9802340865135193, "text": "EVA: Exploring the limits of masked visual representation", "bbox": [434.0, 96.0, 729.0, 111.0]}, {"polygon": [[434.0, 112.0], [599.0, 112.0], [599.0, 125.0], [434.0, 125.0]], "confidence": 0.9656857252120972, "text": "learning at scale. In CVPR , 2023. 2 .", "bbox": [434.0, 112.0, 599.0, 125.0]}, {"polygon": [[68.0, 121.0], [383.0, 121.0], [383.0, 135.0], [68.0, 135.0]], "confidence": 0.9828115701675415, "text": "[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine", "bbox": [68.0, 121.0, 383.0, 135.0]}, {"polygon": [[408.0, 127.0], [729.0, 127.0], [729.0, 141.0], [408.0, 141.0]], "confidence": 0.9816527366638184, "text": "[17] M. A. Fischler and R. C. Bolles. Random sample consensus: A", "bbox": [408.0, 127.0, 729.0, 141.0]}, {"polygon": [[90.0, 136.0], [383.0, 136.0], [383.0, 149.0], [90.0, 149.0]], "confidence": 0.9821423292160034, "text": "Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch,", "bbox": [90.0, 136.0, 383.0, 149.0]}, {"polygon": [[435.0, 142.0], [729.0, 142.0], [729.0, 157.0], [435.0, 157.0]], "confidence": 0.9845963716506958, "text": "paradigm for model fitting with applications to image analysis and", "bbox": [435.0, 142.0, 729.0, 157.0]}, {"polygon": [[91.0, 150.0], [383.0, 152.0], [383.0, 165.0], [91.0, 163.0]], "confidence": 0.9695330858230591, "text": "Katie Millican, Malcolm Reynolds, et al.  Flamingo: a visual", "bbox": [91.0, 150.0, 383.0, 165.0]}, {"polygon": [[435.0, 158.0], [724.0, 158.0], [724.0, 170.0], [435.0, 170.0]], "confidence": 0.974036693572998, "text": "automated cartography.  Comm. ACM , 24(6):381–395, 1981. 4", "bbox": [435.0, 158.0, 724.0, 170.0]}, {"polygon": [[91.0, 164.0], [375.0, 166.0], [375.0, 179.0], [91.0, 177.0]], "confidence": 0.9753744602203369, "text": "language model for few-shot learning. In NeurIPS , 2022. 1, 2", "bbox": [91.0, 164.0, 375.0, 179.0]}, {"polygon": [[408.0, 172.0], [729.0, 171.0], [729.0, 185.0], [408.0, 186.0]], "confidence": 0.9828961491584778, "text": "[18] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh,", "bbox": [408.0, 172.0, 729.0, 185.0]}, {"polygon": [[69.0, 179.0], [384.0, 178.0], [384.0, 192.0], [69.0, 193.0]], "confidence": 0.9826480746269226, "text": "[2] Peter Anderson, Basura Fernando, Mark Johnson, and Stephen", "bbox": [69.0, 179.0, 384.0, 192.0]}, {"polygon": [[435.0, 187.0], [729.0, 187.0], [729.0, 199.0], [435.0, 199.0]], "confidence": 0.9833945631980896, "text": "Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. Image-", "bbox": [435.0, 187.0, 729.0, 199.0]}, {"polygon": [[90.0, 195.0], [383.0, 195.0], [383.0, 206.0], [90.0, 206.0]], "confidence": 0.9825357794761658, "text": "Gould. Spice: Semantic propositional image caption evaluation.", "bbox": [90.0, 195.0, 383.0, 206.0]}, {"polygon": [[434.0, 201.0], [728.0, 201.0], [728.0, 214.0], [434.0, 214.0]], "confidence": 0.9763374328613281, "text": "Bind: One embedding space to bind them all. In CVPR , 2023. 2", "bbox": [434.0, 201.0, 728.0, 214.0]}, {"polygon": [[91.0, 208.0], [320.0, 208.0], [320.0, 221.0], [91.0, 221.0]], "confidence": 0.9689273834228516, "text": "In Proc. ECCV , pages 382–398. Springer, 2016. 3.", "bbox": [91.0, 208.0, 320.0, 221.0]}, {"polygon": [[408.0, 216.0], [679.0, 216.0], [679.0, 230.0], [408.0, 230.0]], "confidence": 0.9787278175354004, "text": "[19] Tengda Han, Weidi Xie, and Andrew Zisserman.", "bbox": [408.0, 216.0, 679.0, 230.0]}, {"polygon": [[677.0, 218.0], [728.0, 218.0], [728.0, 230.0], [677.0, 230.0]], "confidence": 0.8844051957130432, "text": "Temporal", "bbox": [677.0, 218.0, 728.0, 230.0]}, {"polygon": [[69.0, 222.0], [383.0, 222.0], [383.0, 237.0], [69.0, 237.0]], "confidence": 0.9825198650360107, "text": "[3] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun,", "bbox": [69.0, 222.0, 383.0, 237.0]}, {"polygon": [[435.0, 232.0], [728.0, 232.0], [728.0, 244.0], [435.0, 244.0]], "confidence": 0.9805299639701843, "text": "alignment networks for long-term video. In Proc. CVPR , 2022. 4", "bbox": [435.0, 232.0, 728.0, 244.0]}, {"polygon": [[90.0, 238.0], [383.0, 238.0], [383.0, 251.0], [90.0, 251.0]], "confidence": 0.9777359962463379, "text": "Mario Lučić, and Cordelia Schmid.  ViViT: A Video Vision", "bbox": [90.0, 238.0, 383.0, 251.0]}, {"polygon": [[409.0, 246.0], [729.0, 246.0], [729.0, 259.0], [409.0, 259.0]], "confidence": 0.9811856150627136, "text": "[20] Tengda Han, Max Bain, Arsha Nagrani, Gül Varol, Weidi Xie,", "bbox": [409.0, 246.0, 729.0, 259.0]}, {"polygon": [[91.0, 253.0], [234.0, 254.0], [234.0, 266.0], [91.0, 265.0]], "confidence": 0.9653536081314087, "text": "Transformer. In ICCV , 2021. 2", "bbox": [91.0, 253.0, 234.0, 266.0]}, {"polygon": [[435.0, 261.0], [729.0, 261.0], [729.0, 274.0], [435.0, 274.0]], "confidence": 0.982300341129303, "text": "and Andrew Zisserman. AutoAD: Movie description in context.", "bbox": [435.0, 261.0, 729.0, 274.0]}, {"polygon": [[69.0, 267.0], [382.0, 266.0], [382.0, 280.0], [69.0, 281.0]], "confidence": 0.9810452461242676, "text": "[4] Max Bain, Arsha Nagrani, Andrew Brown, and Andrew", "bbox": [69.0, 267.0, 382.0, 280.0]}, {"polygon": [[434.0, 275.0], [582.0, 275.0], [582.0, 289.0], [434.0, 289.0]], "confidence": 0.9556427001953125, "text": "In Proc. CVPR , 2023. 1 , 2 , 7 , 8", "bbox": [434.0, 275.0, 582.0, 289.0]}, {"polygon": [[91.0, 283.0], [151.0, 283.0], [151.0, 294.0], [91.0, 294.0]], "confidence": 0.8553465008735657, "text": "Zisserman.", "bbox": [91.0, 283.0, 151.0, 294.0]}, {"polygon": [[149.0, 283.0], [383.0, 283.0], [383.0, 294.0], [149.0, 294.0]], "confidence": 0.9670311808586121, "text": "Condensed movies: Story based retrieval with", "bbox": [149.0, 283.0, 383.0, 294.0]}, {"polygon": [[408.0, 291.0], [729.0, 291.0], [729.0, 305.0], [408.0, 305.0]], "confidence": 0.9820861220359802, "text": "[21] Tengda Han, Max Bain, Arsha Nagrani, Gül Varol, Weidi Xie,", "bbox": [408.0, 291.0, 729.0, 305.0]}, {"polygon": [[90.0, 296.0], [359.0, 296.0], [359.0, 310.0], [90.0, 310.0]], "confidence": 0.965252697467804, "text": "contextual embeddings. In Proc. ACCV , 2020. 1, 2, 3, 6, 7", "bbox": [90.0, 296.0, 359.0, 310.0]}, {"polygon": [[434.0, 306.0], [729.0, 306.0], [729.0, 319.0], [434.0, 319.0]], "confidence": 0.9656057953834534, "text": "and Andrew Zisserman.  AutoAD II: The sequel – who, when,", "bbox": [434.0, 306.0, 729.0, 319.0]}, {"polygon": [[69.0, 311.0], [384.0, 311.0], [384.0, 325.0], [69.0, 325.0]], "confidence": 0.9825298190116882, "text": "[5] Max Bain, Jaesung Huh, Tengda Han, and Andrew Zisserman.", "bbox": [69.0, 311.0, 384.0, 325.0]}, {"polygon": [[433.0, 320.0], [730.0, 320.0], [730.0, 333.0], [433.0, 333.0]], "confidence": 0.9788836240768433, "text": "and what in movie audio description. In Proc. ICCV , 2023. 1,", "bbox": [433.0, 320.0, 730.0, 333.0]}, {"polygon": [[90.0, 326.0], [383.0, 326.0], [383.0, 339.0], [90.0, 339.0]], "confidence": 0.9799345135688782, "text": "Whisperx: Time-accurate speech transcription of long-form", "bbox": [90.0, 326.0, 383.0, 339.0]}, {"polygon": [[435.0, 336.0], [506.0, 336.0], [506.0, 347.0], [435.0, 347.0]], "confidence": 0.92913818359375, "text": "2, 4, 6, 7, 8, 14", "bbox": [435.0, 336.0, 506.0, 347.0]}, {"polygon": [[91.0, 341.0], [265.0, 341.0], [265.0, 353.0], [91.0, 353.0]], "confidence": 0.9670407772064209, "text": "audio. In INTERSPEECH , 2023. 3 , 8", "bbox": [91.0, 341.0, 265.0, 353.0]}, {"polygon": [[408.0, 349.0], [729.0, 349.0], [729.0, 364.0], [408.0, 364.0]], "confidence": 0.983464241027832, "text": "[22] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras,", "bbox": [408.0, 349.0, 729.0, 364.0]}, {"polygon": [[70.0, 357.0], [383.0, 356.0], [383.0, 368.0], [70.0, 369.0]], "confidence": 0.9790681004524231, "text": "[6] Sébastien Bubeck, Varun Chandrasekaran, Ronen Eldan,", "bbox": [70.0, 357.0, 383.0, 368.0]}, {"polygon": [[434.0, 364.0], [728.0, 364.0], [728.0, 378.0], [434.0, 378.0]], "confidence": 0.9821295738220215, "text": "and Yejin Choi. CLIPScore: A reference-free evaluation metric", "bbox": [434.0, 364.0, 728.0, 378.0]}, {"polygon": [[91.0, 370.0], [383.0, 370.0], [383.0, 382.0], [91.0, 382.0]], "confidence": 0.983240008354187, "text": "Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat", "bbox": [91.0, 370.0, 383.0, 382.0]}, {"polygon": [[434.0, 378.0], [629.0, 378.0], [629.0, 392.0], [434.0, 392.0]], "confidence": 0.9748398661613464, "text": "for image captioning. In EMNLP , 2021. 3", "bbox": [434.0, 378.0, 629.0, 392.0]}, {"polygon": [[91.0, 385.0], [383.0, 385.0], [383.0, 396.0], [91.0, 396.0]], "confidence": 0.969480574131012, "text": "Lee, Yuanzhi Li, Scott Lundberg, et al.  Sparks of artificial", "bbox": [91.0, 385.0, 383.0, 396.0]}, {"polygon": [[408.0, 394.0], [728.0, 394.0], [728.0, 407.0], [408.0, 407.0]], "confidence": 0.9840084910392761, "text": "[23] Gabriel Huang, Bo Pang, Zhenhai Zhu, Clara Rivera, and Radu", "bbox": [408.0, 394.0, 728.0, 407.0]}, {"polygon": [[91.0, 399.0], [383.0, 399.0], [383.0, 411.0], [91.0, 411.0]], "confidence": 0.971293032169342, "text": "general intelligence: Early experiments with gpt-4.  arXiv", "bbox": [91.0, 399.0, 383.0, 411.0]}, {"polygon": [[435.0, 409.0], [729.0, 409.0], [729.0, 422.0], [435.0, 422.0]], "confidence": 0.9750344753265381, "text": "Soricut. Multimodal pretraining for dense video captioning.", "bbox": [435.0, 409.0, 729.0, 422.0]}, {"polygon": [[90.0, 412.0], [256.0, 412.0], [256.0, 425.0], [90.0, 425.0]], "confidence": 0.9479600191116333, "text": "preprint arXiv:2303.12712, 2023. 3", "bbox": [90.0, 412.0, 256.0, 425.0]}, {"polygon": [[433.0, 423.0], [628.0, 423.0], [628.0, 436.0], [433.0, 436.0]], "confidence": 0.9720215797424316, "text": "arXiv preprint arXiv:2011.11760 , 2020. 2", "bbox": [433.0, 423.0, 628.0, 436.0]}, {"polygon": [[69.0, 427.0], [384.0, 427.0], [384.0, 441.0], [69.0, 441.0]], "confidence": 0.9834527969360352, "text": "[7] Aman Chadha, Gurneet Arora, and Navpreet Kaloty. iPerceive:", "bbox": [69.0, 427.0, 384.0, 441.0]}, {"polygon": [[408.0, 438.0], [728.0, 438.0], [728.0, 452.0], [408.0, 452.0]], "confidence": 0.9831109642982483, "text": "[24] Qingqiu Huang, Yu Xiong, Anyi Rao, Jiaze Wang, and Dahua", "bbox": [408.0, 438.0, 728.0, 452.0]}, {"polygon": [[90.0, 442.0], [383.0, 442.0], [383.0, 455.0], [90.0, 455.0]], "confidence": 0.981863260269165, "text": "Applying common-sense reasoning to multi-modal dense video", "bbox": [90.0, 442.0, 383.0, 455.0]}, {"polygon": [[435.0, 454.0], [729.0, 454.0], [729.0, 466.0], [435.0, 466.0]], "confidence": 0.9832964539527893, "text": "Lin. MovieNet: A holistic dataset for movie understanding. In", "bbox": [435.0, 454.0, 729.0, 466.0]}, {"polygon": [[90.0, 457.0], [382.0, 457.0], [382.0, 469.0], [90.0, 469.0]], "confidence": 0.9812569618225098, "text": "captioning and video question answering. In Proc. WACV , 2021. 2", "bbox": [90.0, 457.0, 382.0, 469.0]}, {"polygon": [[435.0, 469.0], [507.0, 469.0], [507.0, 481.0], [435.0, 481.0]], "confidence": 0.9017990827560425, "text": "ECCV , 2020. 2 .", "bbox": [435.0, 469.0, 507.0, 481.0]}, {"polygon": [[69.0, 471.0], [384.0, 470.0], [384.0, 484.0], [69.0, 485.0]], "confidence": 0.9841392636299133, "text": "[8] David Chan, Suzanne Petryk, Joseph E Gonzalez, Trevor Darrell,", "bbox": [69.0, 471.0, 384.0, 484.0]}, {"polygon": [[409.0, 483.0], [729.0, 482.0], [729.0, 495.0], [409.0, 496.0]], "confidence": 0.9820286631584167, "text": "[25] Vladimir Iashin and Esa Rahtu. A better use of audio-visual cues:", "bbox": [409.0, 483.0, 729.0, 495.0]}, {"polygon": [[91.0, 486.0], [382.0, 486.0], [382.0, 498.0], [91.0, 498.0]], "confidence": 0.9825829267501831, "text": "and John Canny. CLAIR: Evaluating image captions with large", "bbox": [91.0, 486.0, 382.0, 498.0]}, {"polygon": [[435.0, 497.0], [729.0, 497.0], [729.0, 510.0], [435.0, 510.0]], "confidence": 0.971239447593689, "text": "Dense video captioning with bi-modal transformer. In BMVC ,", "bbox": [435.0, 497.0, 729.0, 510.0]}, {"polygon": [[91.0, 501.0], [381.0, 501.0], [381.0, 513.0], [91.0, 513.0]], "confidence": 0.9807382822036743, "text": "language models. arXiv preprint arXiv:2310.12971 , 2023. 2 , 3", "bbox": [91.0, 501.0, 381.0, 513.0]}, {"polygon": [[435.0, 512.0], [474.0, 512.0], [474.0, 523.0], [435.0, 523.0]], "confidence": 0.8592315316200256, "text": "2020. 2", "bbox": [435.0, 512.0, 474.0, 523.0]}, {"polygon": [[68.0, 514.0], [383.0, 514.0], [383.0, 529.0], [68.0, 529.0]], "confidence": 0.9737745523452759, "text": "[9] Shaoxiang Chen and Yu-Gang Jiang.  Towards bridging event", "bbox": [68.0, 514.0, 383.0, 529.0]}, {"polygon": [[408.0, 527.0], [728.0, 527.0], [728.0, 541.0], [408.0, 541.0]], "confidence": 0.9834239482879639, "text": "[26] Vladimir Iashin and Esa Rahtu. Multi-modal dense video cap-", "bbox": [408.0, 527.0, 728.0, 541.0]}, {"polygon": [[89.0, 529.0], [382.0, 529.0], [382.0, 543.0], [89.0, 543.0]], "confidence": 0.9829543828964233, "text": "captioner and sentence localizer for weakly supervised dense", "bbox": [89.0, 529.0, 382.0, 543.0]}, {"polygon": [[435.0, 542.0], [726.0, 542.0], [726.0, 555.0], [435.0, 555.0]], "confidence": 0.9775418639183044, "text": "tioning. In CVPR Workshops on Multimodal Learning , 2020. 2", "bbox": [435.0, 542.0, 726.0, 555.0]}, {"polygon": [[89.0, 544.0], [284.0, 544.0], [284.0, 557.0], [89.0, 557.0]], "confidence": 0.9643556475639343, "text": "event captioning. In Proc. CVPR , 2021. 2", "bbox": [89.0, 544.0, 284.0, 557.0]}, {"polygon": [[408.0, 557.0], [728.0, 556.0], [728.0, 570.0], [408.0, 572.0]], "confidence": 0.9830840229988098, "text": "[27] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals,", "bbox": [408.0, 557.0, 728.0, 570.0]}, {"polygon": [[63.0, 559.0], [383.0, 559.0], [383.0, 572.0], [63.0, 572.0]], "confidence": 0.9832450747489929, "text": "[10] Xi Chen, Josip Djolonga, Piotr Padlewski, Basil Mustafa,", "bbox": [63.0, 559.0, 383.0, 572.0]}, {"polygon": [[435.0, 572.0], [729.0, 572.0], [729.0, 586.0], [435.0, 586.0]], "confidence": 0.9705395102500916, "text": "Andrew Zisserman, and Joao Carreira.  Perceiver:  General", "bbox": [435.0, 572.0, 729.0, 586.0]}, {"polygon": [[91.0, 574.0], [383.0, 574.0], [383.0, 586.0], [91.0, 586.0]], "confidence": 0.9837990999221802, "text": "Soravit Changpinyo, Jialin Wu, Carlos Riquelme Ruiz, Sebastian", "bbox": [91.0, 574.0, 383.0, 586.0]}, {"polygon": [[434.0, 586.0], [728.0, 586.0], [728.0, 600.0], [434.0, 600.0]], "confidence": 0.9818330407142639, "text": "perception with iterative attention. In International conference", "bbox": [434.0, 586.0, 728.0, 600.0]}, {"polygon": [[90.0, 588.0], [383.0, 588.0], [383.0, 601.0], [90.0, 601.0]], "confidence": 0.9669204950332642, "text": "Goodman, Xiao Wang, Yi Tay, et al.  Pali-x: On scaling up", "bbox": [90.0, 588.0, 383.0, 601.0]}, {"polygon": [[434.0, 601.0], [694.0, 601.0], [694.0, 614.0], [434.0, 614.0]], "confidence": 0.978066623210907, "text": "on machine learning , pages 4651–4664. PMLR, 2021. 2", "bbox": [434.0, 601.0, 694.0, 614.0]}, {"polygon": [[90.0, 603.0], [383.0, 603.0], [383.0, 616.0], [90.0, 616.0]], "confidence": 0.9795463681221008, "text": "a multilingual vision and language model.  arXiv preprint", "bbox": [90.0, 603.0, 383.0, 616.0]}, {"polygon": [[408.0, 616.0], [728.0, 616.0], [728.0, 629.0], [408.0, 629.0]], "confidence": 0.9831414222717285, "text": "[28] Ming Jiang, Qiuyuan Huang, Lei Zhang, Xin Wang, Pengchuan", "bbox": [408.0, 616.0, 728.0, 629.0]}, {"polygon": [[90.0, 617.0], [216.0, 617.0], [216.0, 630.0], [90.0, 630.0]], "confidence": 0.9563969373703003, "text": "arXiv:2305.18565 , 2023. 3", "bbox": [90.0, 617.0, 216.0, 630.0]}, {"polygon": [[434.0, 630.0], [690.0, 630.0], [690.0, 643.0], [434.0, 643.0]], "confidence": 0.9786162972450256, "text": "Zhang, Zhe Gan, Jana Diesner, and Jianfeng Gao.", "bbox": [434.0, 630.0, 690.0, 643.0]}, {"polygon": [[63.0, 631.0], [383.0, 631.0], [383.0, 645.0], [63.0, 645.0]], "confidence": 0.9703981280326843, "text": "[11] Cheng-Han Chiang and Hung-yi Lee.  Can large language", "bbox": [63.0, 631.0, 383.0, 645.0]}, {"polygon": [[699.0, 631.0], [730.0, 631.0], [730.0, 643.0], [699.0, 643.0]], "confidence": 0.849174976348877, "text": "Tiger:", "bbox": [699.0, 631.0, 730.0, 643.0]}, {"polygon": [[90.0, 645.0], [383.0, 645.0], [383.0, 660.0], [90.0, 660.0]], "confidence": 0.9752224683761597, "text": "models be an alternative to human evaluations?  arXiv preprint", "bbox": [90.0, 645.0, 383.0, 660.0]}, {"polygon": [[435.0, 645.0], [728.0, 645.0], [728.0, 657.0], [435.0, 657.0]], "confidence": 0.98031085729599, "text": "Text-to-image grounding for image caption evaluation. arXiv", "bbox": [435.0, 645.0, 728.0, 657.0]}, {"polygon": [[435.0, 659.0], [600.0, 659.0], [600.0, 672.0], [435.0, 672.0]], "confidence": 0.9591051936149597, "text": "preprint arXiv: 1909.02050 , 2019. 3", "bbox": [435.0, 659.0, 600.0, 672.0]}, {"polygon": [[91.0, 661.0], [240.0, 661.0], [240.0, 674.0], [91.0, 674.0]], "confidence": 0.9541361331939697, "text": "arXiv:2305.01937 , 2023. 2 , 3 , 6", "bbox": [91.0, 661.0, 240.0, 674.0]}, {"polygon": [[408.0, 673.0], [728.0, 675.0], [728.0, 689.0], [408.0, 688.0]], "confidence": 0.9820306897163391, "text": "[29] Hassan Kane, Muhammed Yusuf Kocyigit, Ali Abdalla, Pelkins", "bbox": [408.0, 673.0, 728.0, 689.0]}, {"polygon": [[63.0, 675.0], [383.0, 675.0], [383.0, 689.0], [63.0, 689.0]], "confidence": 0.9830000400543213, "text": "[12] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten", "bbox": [63.0, 675.0, 383.0, 689.0]}, {"polygon": [[435.0, 689.0], [612.0, 689.0], [612.0, 702.0], [435.0, 702.0]], "confidence": 0.9630757570266724, "text": "Ajanoh, and Mohamed Coulibali.", "bbox": [435.0, 689.0, 612.0, 702.0]}, {"polygon": [[91.0, 690.0], [384.0, 690.0], [384.0, 703.0], [91.0, 703.0]], "confidence": 0.9791910648345947, "text": "Bosma, Gaurav Mishra, Adam Roberts, Paul Barham,", "bbox": [91.0, 690.0, 384.0, 703.0]}, {"polygon": [[616.0, 690.0], [728.0, 690.0], [728.0, 702.0], [616.0, 702.0]], "confidence": 0.9381362199783325, "text": "Nubia: Neural based", "bbox": [616.0, 690.0, 728.0, 702.0]}, {"polygon": [[90.0, 704.0], [384.0, 704.0], [384.0, 717.0], [90.0, 717.0]], "confidence": 0.9761573672294617, "text": "Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al.", "bbox": [90.0, 704.0, 384.0, 717.0]}, {"polygon": [[435.0, 704.0], [728.0, 704.0], [728.0, 717.0], [435.0, 717.0]], "confidence": 0.9808045029640198, "text": "interchangeability assessor for text generation. arXiv preprint", "bbox": [435.0, 704.0, 728.0, 717.0]}, {"polygon": [[89.0, 718.0], [383.0, 718.0], [383.0, 733.0], [89.0, 733.0]], "confidence": 0.9787861108779907, "text": "Palm: Scaling language modeling with pathways. arXiv preprint", "bbox": [89.0, 718.0, 383.0, 733.0]}, {"polygon": [[434.0, 718.0], [561.0, 718.0], [561.0, 731.0], [434.0, 731.0]], "confidence": 0.954898476600647, "text": "arXiv:2004.14667 , 2020. 3", "bbox": [434.0, 718.0, 561.0, 731.0]}, {"polygon": [[90.0, 733.0], [215.0, 733.0], [215.0, 747.0], [90.0, 747.0]], "confidence": 0.9301733374595642, "text": "arXiv:2204.02311 , 2022. 2 .", "bbox": [90.0, 733.0, 215.0, 747.0]}, {"polygon": [[408.0, 733.0], [729.0, 733.0], [729.0, 747.0], [408.0, 747.0]], "confidence": 0.9837564826011658, "text": "[30] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan", "bbox": [408.0, 733.0, 729.0, 747.0]}, {"polygon": [[63.0, 748.0], [384.0, 748.0], [384.0, 762.0], [63.0, 762.0]], "confidence": 0.981916069984436, "text": "[13] Yin Cui, Guandao Yang, Andreas Veit, Xun Huang, and", "bbox": [63.0, 748.0, 384.0, 762.0]}, {"polygon": [[433.0, 747.0], [729.0, 747.0], [729.0, 761.0], [433.0, 761.0]], "confidence": 0.966278612613678, "text": "Carlos Niebles.  Dense-captioning events in videos.  In Proc.", "bbox": [433.0, 747.0, 729.0, 761.0]}, {"polygon": [[433.0, 761.0], [578.0, 761.0], [578.0, 775.0], [433.0, 775.0]], "confidence": 0.9459081888198853, "text": "ICCV , pages 706–715, 2017. 2.", "bbox": [433.0, 761.0, 578.0, 775.0]}, {"polygon": [[90.0, 762.0], [384.0, 762.0], [384.0, 777.0], [90.0, 777.0]], "confidence": 0.9785935878753662, "text": "Serge Belongie.  Learning to evaluate image captioning.  In", "bbox": [90.0, 762.0, 384.0, 777.0]}, {"polygon": [[408.0, 776.0], [729.0, 777.0], [729.0, 792.0], [408.0, 790.0]], "confidence": 0.982056736946106, "text": "[31] Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi.", "bbox": [408.0, 776.0, 729.0, 792.0]}, {"polygon": [[91.0, 778.0], [384.0, 778.0], [384.0, 791.0], [91.0, 791.0]], "confidence": 0.9822618961334229, "text": "Proceedings of the IEEE conference on computer vision and", "bbox": [91.0, 778.0, 384.0, 791.0]}, {"polygon": [[89.0, 792.0], [305.0, 792.0], [305.0, 805.0], [89.0, 805.0]], "confidence": 0.9752535820007324, "text": "pattern recognition , pages 5804–5812, 2018. 3", "bbox": [89.0, 792.0, 305.0, 805.0]}, {"polygon": [[434.0, 792.0], [728.0, 792.0], [728.0, 805.0], [434.0, 805.0]], "confidence": 0.9809408783912659, "text": "BLIP: bootstrapping language-image pre-training for unified", "bbox": [434.0, 792.0, 728.0, 805.0]}, {"polygon": [[433.0, 806.0], [729.0, 806.0], [729.0, 819.0], [433.0, 819.0]], "confidence": 0.980134904384613, "text": "vision-language understanding and generation. In ICML , 2022.", "bbox": [433.0, 806.0, 729.0, 819.0]}, {"polygon": [[63.0, 807.0], [384.0, 806.0], [384.0, 819.0], [63.0, 821.0]], "confidence": 0.9827272891998291, "text": "[14] Chaorui Deng, Shizhe Chen, Da Chen, Yuan He, and Qi Wu.", "bbox": [63.0, 807.0, 384.0, 819.0]}, {"polygon": [[90.0, 821.0], [384.0, 820.0], [384.0, 834.0], [90.0, 835.0]], "confidence": 0.9823946952819824, "text": "Sketch, ground, and refine: Top-down dense video captioning.", "bbox": [90.0, 821.0, 384.0, 834.0]}, {"polygon": [[435.0, 821.0], [455.0, 821.0], [455.0, 833.0], [435.0, 833.0]], "confidence": 0.6543123126029968, "text": "1, 21, 21", "bbox": [435.0, 821.0, 455.0, 833.0]}, {"polygon": [[90.0, 835.0], [176.0, 835.0], [176.0, 849.0], [90.0, 849.0]], "confidence": 0.926371693611145, "text": "In CVPR , 2021. 2", "bbox": [90.0, 835.0, 176.0, 849.0]}, {"polygon": [[408.0, 836.0], [729.0, 836.0], [729.0, 850.0], [408.0, 850.0]], "confidence": 0.9822580814361572, "text": "[32] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2:", "bbox": [408.0, 836.0, 729.0, 850.0]}, {"polygon": [[63.0, 850.0], [382.0, 850.0], [382.0, 865.0], [63.0, 865.0]], "confidence": 0.9839884638786316, "text": "[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk", "bbox": [63.0, 850.0, 382.0, 865.0]}, {"polygon": [[435.0, 850.0], [728.0, 850.0], [728.0, 864.0], [435.0, 864.0]], "confidence": 0.979792594909668, "text": "Bootstrapping language-image pre-training with frozen image", "bbox": [435.0, 850.0, 728.0, 864.0]}, {"polygon": [[433.0, 864.0], [729.0, 864.0], [729.0, 878.0], [433.0, 878.0]], "confidence": 0.9794394969940186, "text": "encoders and large language models. arXiv:2301.12597 , 2023.", "bbox": [433.0, 864.0, 729.0, 878.0]}, {"polygon": [[91.0, 866.0], [382.0, 866.0], [382.0, 879.0], [91.0, 879.0]], "confidence": 0.9813808798789978, "text": "Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa", "bbox": [91.0, 866.0, 382.0, 879.0]}, {"polygon": [[91.0, 880.0], [384.0, 880.0], [384.0, 893.0], [91.0, 893.0]], "confidence": 0.9827139973640442, "text": "Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,", "bbox": [91.0, 880.0, 384.0, 893.0]}, {"polygon": [[435.0, 880.0], [468.0, 880.0], [468.0, 892.0], [435.0, 892.0]], "confidence": 0.820203423500061, "text": "2, 3, 5", "bbox": [435.0, 880.0, 468.0, 892.0]}, {"polygon": [[90.0, 894.0], [382.0, 894.0], [382.0, 907.0], [90.0, 907.0]], "confidence": 0.9810623526573181, "text": "et al. An image is worth 16x16 words: Transformers for image", "bbox": [90.0, 894.0, 382.0, 907.0]}, {"polygon": [[408.0, 894.0], [728.0, 894.0], [728.0, 907.0], [408.0, 907.0]], "confidence": 0.982991635799408, "text": "[33] Linjie Li, Zhe Gan, Kevin Lin, Chung-Ching Lin, Zicheng", "bbox": [408.0, 894.0, 728.0, 907.0]}, {"polygon": [[89.0, 908.0], [380.0, 908.0], [380.0, 921.0], [89.0, 921.0]], "confidence": 0.9783033132553101, "text": "recognition at scale. arXiv preprint arXiv:2010.11929 , 2020. 2", "bbox": [89.0, 908.0, 380.0, 921.0]}, {"polygon": [[434.0, 909.0], [728.0, 909.0], [728.0, 922.0], [434.0, 922.0]], "confidence": 0.9719276428222656, "text": "Liu, Ce Liu, and Lijuan Wang.  LAVENDER: Unifying", "bbox": [434.0, 909.0, 728.0, 922.0]}, {"polygon": [[63.0, 923.0], [384.0, 921.0], [384.0, 935.0], [63.0, 937.0]], "confidence": 0.9818286299705505, "text": "[16] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu,", "bbox": [63.0, 923.0, 384.0, 935.0]}, {"polygon": [[433.0, 923.0], [728.0, 923.0], [728.0, 937.0], [433.0, 937.0]], "confidence": 0.9822787642478943, "text": "video-language understanding as masked language modeling. In", "bbox": [433.0, 923.0, 728.0, 937.0]}, {"polygon": [[90.0, 937.0], [384.0, 937.0], [384.0, 951.0], [90.0, 951.0]], "confidence": 0.9814671277999878, "text": "Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao.", "bbox": [90.0, 937.0, 384.0, 951.0]}, {"polygon": [[433.0, 937.0], [507.0, 937.0], [507.0, 951.0], [433.0, 951.0]], "confidence": 0.9023402333259583, "text": "CVPR , 2023. 1.", "bbox": [433.0, 937.0, 507.0, 951.0]}, {"polygon": [[391.0, 976.0], [401.0, 976.0], [401.0, 988.0], [391.0, 988.0]], "confidence": 0.4812418222427368, "text": "9", "bbox": [391.0, 976.0, 401.0, 988.0]}], "languages": ["en"], "image_bbox": [0.0, 0.0, 816.0, 1056.0], "page": 9}, {"text_lines": [{"polygon": [[63.0, 96.0], [384.0, 96.0], [384.0, 111.0], [63.0, 111.0]], "confidence": 0.9830580949783325, "text": "[34] Yehao Li, Ting Yao, Yingwei Pan, Hongyang Chao, and Tao", "bbox": [63.0, 96.0, 384.0, 111.0]}, {"polygon": [[409.0, 96.0], [728.0, 96.0], [728.0, 111.0], [409.0, 111.0]], "confidence": 0.982477068901062, "text": "[51] Botian Shi, Lei Ji, Yaobo Liang, Nan Duan, Peng Chen,", "bbox": [409.0, 96.0, 728.0, 111.0]}, {"polygon": [[90.0, 112.0], [383.0, 112.0], [383.0, 125.0], [90.0, 125.0]], "confidence": 0.9812288284301758, "text": "Mei.  Jointly localizing and describing events for dense video", "bbox": [90.0, 112.0, 383.0, 125.0]}, {"polygon": [[435.0, 112.0], [729.0, 112.0], [729.0, 125.0], [435.0, 125.0]], "confidence": 0.9827176928520203, "text": "Zhendong Niu, and Ming Zhou. Dense procedure captioning in", "bbox": [435.0, 112.0, 729.0, 125.0]}, {"polygon": [[435.0, 126.0], [728.0, 126.0], [728.0, 139.0], [435.0, 139.0]], "confidence": 0.9814057350158691, "text": "narrated instructional videos. In Association for Computational", "bbox": [435.0, 126.0, 728.0, 139.0]}, {"polygon": [[91.0, 127.0], [256.0, 127.0], [256.0, 139.0], [91.0, 139.0]], "confidence": 0.9648643136024475, "text": "captioning. In Proc. CVPR , 2018. 2", "bbox": [91.0, 127.0, 256.0, 139.0]}, {"polygon": [[63.0, 141.0], [382.0, 141.0], [382.0, 155.0], [63.0, 155.0]], "confidence": 0.9827510714530945, "text": "[35] Kevin Lin, Linjie Li, Chung-Ching Lin, Faisal Ahmed, Zhe", "bbox": [63.0, 141.0, 382.0, 155.0]}, {"polygon": [[435.0, 141.0], [527.0, 141.0], [527.0, 154.0], [435.0, 154.0]], "confidence": 0.9523754715919495, "text": "Linguistics , 2019. 2", "bbox": [435.0, 141.0, 527.0, 154.0]}, {"polygon": [[409.0, 154.0], [730.0, 154.0], [730.0, 168.0], [409.0, 168.0]], "confidence": 0.9828206896781921, "text": "[52] Yaya Shi, Xu Yang, Haiyang Xu, Chunfeng Yuan, Bing Li,", "bbox": [409.0, 154.0, 730.0, 168.0]}, {"polygon": [[89.0, 155.0], [384.0, 155.0], [384.0, 169.0], [89.0, 169.0]], "confidence": 0.980925977230072, "text": "Gan, Zicheng Liu, Yumao Lu, and Lijuan Wang.  SwinBERT:", "bbox": [89.0, 155.0, 384.0, 169.0]}, {"polygon": [[90.0, 169.0], [383.0, 169.0], [383.0, 183.0], [90.0, 183.0]], "confidence": 0.9734418988227844, "text": "End to-end transformers with sparse attention for video", "bbox": [90.0, 169.0, 383.0, 183.0]}, {"polygon": [[435.0, 169.0], [728.0, 169.0], [728.0, 182.0], [435.0, 182.0]], "confidence": 0.9794328808784485, "text": "Weiming Hu, and Zheng-Jun Zha. Emscore: Evaluating video", "bbox": [435.0, 169.0, 728.0, 182.0]}, {"polygon": [[433.0, 183.0], [728.0, 183.0], [728.0, 197.0], [433.0, 197.0]], "confidence": 0.9807996153831482, "text": "captioning via coarse-grained and fine-grained embedding", "bbox": [433.0, 183.0, 728.0, 197.0]}, {"polygon": [[90.0, 184.0], [255.0, 184.0], [255.0, 198.0], [90.0, 198.0]], "confidence": 0.9572281241416931, "text": "captioning. In Proc. CVPR , 2022. 2 .", "bbox": [90.0, 184.0, 255.0, 198.0]}, {"polygon": [[434.0, 198.0], [691.0, 198.0], [691.0, 212.0], [434.0, 212.0]], "confidence": 0.9773248434066772, "text": "matching. In Proc. CVPR , pages 17929–17938, 2022. 3", "bbox": [434.0, 198.0, 691.0, 212.0]}, {"polygon": [[63.0, 199.0], [382.0, 199.0], [382.0, 213.0], [63.0, 213.0]], "confidence": 0.9819007515907288, "text": "[36] Kevin Lin, Faisal Ahmed, Linjie Li, Chung-Ching Lin, Ehsan", "bbox": [63.0, 199.0, 382.0, 213.0]}, {"polygon": [[408.0, 212.0], [728.0, 212.0], [728.0, 226.0], [408.0, 226.0]], "confidence": 0.982071042060852, "text": "[53] Nina Shvetsova, Anna Kukleva, Xudong Hong, Christian", "bbox": [408.0, 212.0, 728.0, 226.0]}, {"polygon": [[90.0, 214.0], [382.0, 214.0], [382.0, 227.0], [90.0, 227.0]], "confidence": 0.9808839559555054, "text": "Azarnasab, Zhengyuan Yang, Jianfeng Wang, Lin Liang,", "bbox": [90.0, 214.0, 382.0, 227.0]}, {"polygon": [[434.0, 226.0], [729.0, 226.0], [729.0, 239.0], [434.0, 239.0]], "confidence": 0.9771385192871094, "text": "Rupprecht, Bernt Schiele, and Hilde Kuehne.  HowToCaption:", "bbox": [434.0, 226.0, 729.0, 239.0]}, {"polygon": [[90.0, 228.0], [384.0, 228.0], [384.0, 241.0], [90.0, 241.0]], "confidence": 0.9795701503753662, "text": "Zicheng Liu, Yumao Lu, Ce Liu, and Lijuan Wang.  Mm-vid:", "bbox": [90.0, 228.0, 384.0, 241.0]}, {"polygon": [[434.0, 240.0], [729.0, 240.0], [729.0, 254.0], [434.0, 254.0]], "confidence": 0.9817172288894653, "text": "Prompting LLMs to transform video annotations at scale.", "bbox": [434.0, 240.0, 729.0, 254.0]}, {"polygon": [[90.0, 242.0], [349.0, 242.0], [349.0, 256.0], [90.0, 256.0]], "confidence": 0.978157639503479, "text": "Advancing video understanding with gpt-4v(ision).", "bbox": [90.0, 242.0, 349.0, 256.0]}, {"polygon": [[351.0, 244.0], [382.0, 244.0], [382.0, 255.0], [351.0, 255.0]], "confidence": 0.8217937350273132, "text": "arXiv", "bbox": [351.0, 244.0, 382.0, 255.0]}, {"polygon": [[435.0, 255.0], [583.0, 255.0], [583.0, 268.0], [435.0, 268.0]], "confidence": 0.9567090272903442, "text": "arXiv:2310.04900 , 2023. 1, 2 , 4", "bbox": [435.0, 255.0, 583.0, 268.0]}, {"polygon": [[89.0, 256.0], [255.0, 256.0], [255.0, 270.0], [89.0, 270.0]], "confidence": 0.9577906727790833, "text": "preprint arXiv:2310.19773 , 2023. 4.", "bbox": [89.0, 256.0, 255.0, 270.0]}, {"polygon": [[408.0, 267.0], [728.0, 269.0], [728.0, 284.0], [408.0, 282.0]], "confidence": 0.9830762147903442, "text": "[54] Mattia Soldan, Alejandro Pardo, Juan León Alcázar, Fabian", "bbox": [408.0, 267.0, 728.0, 284.0]}, {"polygon": [[63.0, 271.0], [382.0, 271.0], [382.0, 284.0], [63.0, 284.0]], "confidence": 0.9799733757972717, "text": "[37] Ilya Loshchilov and Frank Hutter.  Decoupled weight decay", "bbox": [63.0, 271.0, 382.0, 284.0]}, {"polygon": [[433.0, 282.0], [729.0, 282.0], [729.0, 297.0], [433.0, 297.0]], "confidence": 0.9821858406066895, "text": "Caba, Chen Zhao, Silvio Giancola, and Bernard Ghanem. MAD:", "bbox": [433.0, 282.0, 729.0, 297.0]}, {"polygon": [[90.0, 286.0], [354.0, 286.0], [354.0, 299.0], [90.0, 299.0]], "confidence": 0.9786871671676636, "text": "regularization. arXiv preprint arXiv:1711.05101 , 2017. 5", "bbox": [90.0, 286.0, 354.0, 299.0]}, {"polygon": [[435.0, 296.0], [728.0, 296.0], [728.0, 311.0], [435.0, 311.0]], "confidence": 0.9825437664985657, "text": "A scalable dataset for language grounding in videos from movie", "bbox": [435.0, 296.0, 728.0, 311.0]}, {"polygon": [[63.0, 300.0], [382.0, 300.0], [382.0, 315.0], [63.0, 315.0]], "confidence": 0.9841724634170532, "text": "[38] Huaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan Duan,", "bbox": [63.0, 300.0, 382.0, 315.0]}, {"polygon": [[435.0, 311.0], [648.0, 311.0], [648.0, 324.0], [435.0, 324.0]], "confidence": 0.9683592319488525, "text": "audio descriptions. In Proc. CVPR , 2022. 1 , 2", "bbox": [435.0, 311.0, 648.0, 324.0]}, {"polygon": [[90.0, 314.0], [383.0, 314.0], [383.0, 329.0], [90.0, 329.0]], "confidence": 0.9797624945640564, "text": "Tianrui Li, Xilin Chen, and Ming Zhou.  UniViLM: A unified", "bbox": [90.0, 314.0, 383.0, 329.0]}, {"polygon": [[408.0, 325.0], [728.0, 325.0], [728.0, 339.0], [408.0, 339.0]], "confidence": 0.9823936820030212, "text": "[55] Enxin Song, Wenhao Chai, Guanhong Wang, Yucheng Zhang,", "bbox": [408.0, 325.0, 728.0, 339.0]}, {"polygon": [[90.0, 329.0], [382.0, 329.0], [382.0, 343.0], [90.0, 343.0]], "confidence": 0.9838384389877319, "text": "video and language pre-training model for multimodal understand-", "bbox": [90.0, 329.0, 382.0, 343.0]}, {"polygon": [[435.0, 339.0], [729.0, 339.0], [729.0, 353.0], [435.0, 353.0]], "confidence": 0.9804074168205261, "text": "Haoyang Zhou, Feiyang Wu, Xun Guo, Tian Ye, Yan Lu,", "bbox": [435.0, 339.0, 729.0, 353.0]}, {"polygon": [[90.0, 343.0], [377.0, 343.0], [377.0, 357.0], [90.0, 357.0]], "confidence": 0.9782249331474304, "text": "ing and generation. arXiv preprint arXiv:2002.06353 , 2020. 2", "bbox": [90.0, 343.0, 377.0, 357.0]}, {"polygon": [[435.0, 354.0], [728.0, 354.0], [728.0, 367.0], [435.0, 367.0]], "confidence": 0.9690780639648438, "text": "Jenq-Neng Hwang, and Gaoang Wang.  MovieChat:  From", "bbox": [435.0, 354.0, 728.0, 367.0]}, {"polygon": [[63.0, 359.0], [383.0, 359.0], [383.0, 372.0], [63.0, 372.0]], "confidence": 0.9810013175010681, "text": "[39] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and", "bbox": [63.0, 359.0, 383.0, 372.0]}, {"polygon": [[433.0, 368.0], [728.0, 368.0], [728.0, 381.0], [433.0, 381.0]], "confidence": 0.9826995134353638, "text": "dense token to sparse memory for long video understanding.", "bbox": [433.0, 368.0, 728.0, 381.0]}, {"polygon": [[90.0, 373.0], [383.0, 373.0], [383.0, 386.0], [90.0, 386.0]], "confidence": 0.975616991519928, "text": "Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video", "bbox": [90.0, 373.0, 383.0, 386.0]}, {"polygon": [[434.0, 383.0], [571.0, 383.0], [571.0, 396.0], [434.0, 396.0]], "confidence": 0.9619288444519043, "text": "arXiv:2307.16449 , 2023. 2 , 3", "bbox": [434.0, 383.0, 571.0, 396.0]}, {"polygon": [[90.0, 387.0], [382.0, 387.0], [382.0, 400.0], [90.0, 400.0]], "confidence": 0.977241575717926, "text": "understanding via large vision and language models. arXiv", "bbox": [90.0, 387.0, 382.0, 400.0]}, {"polygon": [[408.0, 396.0], [729.0, 396.0], [729.0, 410.0], [408.0, 410.0]], "confidence": 0.9840507507324219, "text": "[56] Quan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao.", "bbox": [408.0, 396.0, 729.0, 410.0]}, {"polygon": [[89.0, 401.0], [279.0, 401.0], [279.0, 414.0], [89.0, 414.0]], "confidence": 0.9727079272270203, "text": "preprint arXiv:2306.05424 , 2023. 2 , 3 , 6", "bbox": [89.0, 401.0, 279.0, 414.0]}, {"polygon": [[434.0, 411.0], [728.0, 411.0], [728.0, 425.0], [434.0, 425.0]], "confidence": 0.980362594127655, "text": "Eva-clip: Improved training techniques for clip at scale. arXiv", "bbox": [434.0, 411.0, 728.0, 425.0]}, {"polygon": [[64.0, 417.0], [384.0, 417.0], [384.0, 430.0], [64.0, 430.0]], "confidence": 0.9818451404571533, "text": "[40] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac,", "bbox": [64.0, 417.0, 384.0, 430.0]}, {"polygon": [[434.0, 426.0], [599.0, 426.0], [599.0, 439.0], [434.0, 439.0]], "confidence": 0.9563015103340149, "text": "preprint arXiv:2303.15389 , 2023. 51", "bbox": [434.0, 426.0, 599.0, 439.0]}, {"polygon": [[90.0, 430.0], [384.0, 430.0], [384.0, 444.0], [90.0, 444.0]], "confidence": 0.9825626015663147, "text": "Makarand Tapaswi, Ivan Laptev, and Josef Sivic. Howto100m:", "bbox": [90.0, 430.0, 384.0, 444.0]}, {"polygon": [[408.0, 440.0], [729.0, 440.0], [729.0, 453.0], [408.0, 453.0]], "confidence": 0.9834468364715576, "text": "[57] Makarand Tapaswi, Martin Bauml, and Rainer Stiefelhagen.", "bbox": [408.0, 440.0, 729.0, 453.0]}, {"polygon": [[90.0, 444.0], [383.0, 444.0], [383.0, 459.0], [90.0, 459.0]], "confidence": 0.9818472266197205, "text": "Learning a text-video embedding by watching hundred million", "bbox": [90.0, 444.0, 383.0, 459.0]}, {"polygon": [[435.0, 454.0], [729.0, 454.0], [729.0, 467.0], [435.0, 467.0]], "confidence": 0.973389744758606, "text": "Book2movie: Aligning video scenes with book chapters.  In", "bbox": [435.0, 454.0, 729.0, 467.0]}, {"polygon": [[90.0, 459.0], [384.0, 459.0], [384.0, 473.0], [90.0, 473.0]], "confidence": 0.9734569787979126, "text": "narrated video clips. In Proc. ICCV , pages 2630–2640, 2019.", "bbox": [90.0, 459.0, 384.0, 473.0]}, {"polygon": [[435.0, 468.0], [620.0, 468.0], [620.0, 481.0], [435.0, 481.0]], "confidence": 0.9711179137229919, "text": "Proc. CVPR , pages 1827–1835, 2015. 2", "bbox": [435.0, 468.0, 620.0, 481.0]}, {"polygon": [[90.0, 473.0], [154.0, 473.0], [154.0, 487.0], [90.0, 487.0]], "confidence": 0.9303412437438965, "text": "1, 2, 3, 4, 6, 7", "bbox": [90.0, 473.0, 154.0, 487.0]}, {"polygon": [[409.0, 482.0], [728.0, 482.0], [728.0, 495.0], [409.0, 495.0]], "confidence": 0.9834834337234497, "text": "[58] Makarand Tapaswi, Yukun Zhu, Rainer Stiefelhagen, Antonio", "bbox": [409.0, 482.0, 728.0, 495.0]}, {"polygon": [[63.0, 487.0], [382.0, 487.0], [382.0, 501.0], [63.0, 501.0]], "confidence": 0.9833512306213379, "text": "[41] Ron Mokady, Amir Hertz, and Amit H Bermano. ClipCap: CLIP", "bbox": [63.0, 487.0, 382.0, 501.0]}, {"polygon": [[435.0, 496.0], [729.0, 496.0], [729.0, 509.0], [435.0, 509.0]], "confidence": 0.9801141023635864, "text": "Torralba, Raquel Urtasun, and Sanja Fidler.  Movieqa:  Un-", "bbox": [435.0, 496.0, 729.0, 509.0]}, {"polygon": [[90.0, 502.0], [383.0, 502.0], [383.0, 515.0], [90.0, 515.0]], "confidence": 0.9821595549583435, "text": "prefix for image captioning. arXiv preprint arXiv:2111.09734 ,", "bbox": [90.0, 502.0, 383.0, 515.0]}, {"polygon": [[435.0, 510.0], [729.0, 512.0], [729.0, 525.0], [435.0, 523.0]], "confidence": 0.977029025554657, "text": "derstanding stories in movies through question-answering.  In", "bbox": [435.0, 510.0, 729.0, 525.0]}, {"polygon": [[91.0, 518.0], [129.0, 518.0], [129.0, 529.0], [91.0, 529.0]], "confidence": 0.8602197170257568, "text": "2021. 2", "bbox": [91.0, 518.0, 129.0, 529.0]}, {"polygon": [[435.0, 526.0], [728.0, 526.0], [728.0, 537.0], [435.0, 537.0]], "confidence": 0.9822878837585449, "text": "Proceedings of the IEEE conference on computer vision and", "bbox": [435.0, 526.0, 728.0, 537.0]}, {"polygon": [[63.0, 531.0], [382.0, 530.0], [382.0, 544.0], [63.0, 545.0]], "confidence": 0.983709454536438, "text": "[42] Jonghwan Mun, Linjie Yang, Zhou Ren, Ning Xu, and Bohyung", "bbox": [63.0, 531.0, 382.0, 544.0]}, {"polygon": [[435.0, 540.0], [651.0, 540.0], [651.0, 551.0], [435.0, 551.0]], "confidence": 0.9684535264968872, "text": "pattern recognition , pages 4631–4640, 2016. 2", "bbox": [435.0, 540.0, 651.0, 551.0]}, {"polygon": [[90.0, 547.0], [382.0, 547.0], [382.0, 559.0], [90.0, 559.0]], "confidence": 0.9810831546783447, "text": "Han. Streamlined dense video captioning. In Proc. CVPR , 2019. 2", "bbox": [90.0, 547.0, 382.0, 559.0]}, {"polygon": [[409.0, 553.0], [729.0, 553.0], [729.0, 566.0], [409.0, 566.0]], "confidence": 0.9843981266021729, "text": "[59] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet,", "bbox": [409.0, 553.0, 729.0, 566.0]}, {"polygon": [[63.0, 560.0], [383.0, 560.0], [383.0, 574.0], [63.0, 574.0]], "confidence": 0.9837009906768799, "text": "[43] Alejandro Pardo, Fabian Caba Heilbron, Juan León Alcázar, Ali", "bbox": [63.0, 560.0, 383.0, 574.0]}, {"polygon": [[435.0, 568.0], [729.0, 568.0], [729.0, 580.0], [435.0, 580.0]], "confidence": 0.9794085621833801, "text": "Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière,", "bbox": [435.0, 568.0, 729.0, 580.0]}, {"polygon": [[90.0, 574.0], [383.0, 574.0], [383.0, 588.0], [90.0, 588.0]], "confidence": 0.9806885123252869, "text": "Thabet, and Bernard Ghanem. Moviecuts: A new dataset and", "bbox": [90.0, 574.0, 383.0, 588.0]}, {"polygon": [[435.0, 581.0], [729.0, 581.0], [729.0, 594.0], [435.0, 594.0]], "confidence": 0.9830004572868347, "text": "Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez,", "bbox": [435.0, 581.0, 729.0, 594.0]}, {"polygon": [[90.0, 589.0], [382.0, 589.0], [382.0, 603.0], [90.0, 603.0]], "confidence": 0.972404420375824, "text": "benchmark for cut type recognition.  In European Conference", "bbox": [90.0, 589.0, 382.0, 603.0]}, {"polygon": [[435.0, 595.0], [729.0, 595.0], [729.0, 609.0], [435.0, 609.0]], "confidence": 0.98050856590271, "text": "Armand Joulin, Edouard Grave, and Guillaume Lample.", "bbox": [435.0, 595.0, 729.0, 609.0]}, {"polygon": [[90.0, 604.0], [343.0, 604.0], [343.0, 617.0], [90.0, 617.0]], "confidence": 0.9757893681526184, "text": "on Computer Vision , pages 668–685. Springer, 2022. 2", "bbox": [90.0, 604.0, 343.0, 617.0]}, {"polygon": [[434.0, 609.0], [729.0, 609.0], [729.0, 623.0], [434.0, 623.0]], "confidence": 0.9776129722595215, "text": "LLaMA: Open and efficient foundation language models.", "bbox": [434.0, 609.0, 729.0, 623.0]}, {"polygon": [[64.0, 619.0], [382.0, 619.0], [382.0, 632.0], [64.0, 632.0]], "confidence": 0.9838938117027283, "text": "[44] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,", "bbox": [64.0, 619.0, 382.0, 632.0]}, {"polygon": [[434.0, 624.0], [561.0, 624.0], [561.0, 637.0], [434.0, 637.0]], "confidence": 0.9416741132736206, "text": "arXiv:2302.13971 , 2023. 2.", "bbox": [434.0, 624.0, 561.0, 637.0]}, {"polygon": [[89.0, 633.0], [384.0, 633.0], [384.0, 646.0], [89.0, 646.0]], "confidence": 0.9825613498687744, "text": "Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell,", "bbox": [89.0, 633.0, 384.0, 646.0]}, {"polygon": [[408.0, 638.0], [729.0, 638.0], [729.0, 652.0], [408.0, 652.0]], "confidence": 0.9818523526191711, "text": "[60] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad", "bbox": [408.0, 638.0, 729.0, 652.0]}, {"polygon": [[90.0, 647.0], [383.0, 647.0], [383.0, 660.0], [90.0, 660.0]], "confidence": 0.9813961386680603, "text": "Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya", "bbox": [90.0, 647.0, 383.0, 660.0]}, {"polygon": [[435.0, 652.0], [728.0, 652.0], [728.0, 666.0], [435.0, 666.0]], "confidence": 0.9807929396629333, "text": "Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya", "bbox": [435.0, 652.0, 728.0, 666.0]}, {"polygon": [[90.0, 661.0], [382.0, 661.0], [382.0, 674.0], [90.0, 674.0]], "confidence": 0.9759799838066101, "text": "Sutskever.  Learning transferable visual models from natural", "bbox": [90.0, 661.0, 382.0, 674.0]}, {"polygon": [[435.0, 667.0], [729.0, 667.0], [729.0, 681.0], [435.0, 681.0]], "confidence": 0.9756683111190796, "text": "Batra, Prajjwal Bhargava, Shruti Bhosale, et  al.  Llama 2:", "bbox": [435.0, 667.0, 729.0, 681.0]}, {"polygon": [[91.0, 676.0], [302.0, 676.0], [302.0, 689.0], [91.0, 689.0]], "confidence": 0.9772456884384155, "text": "language supervision. In Proc. ICML , 2021. 2", "bbox": [91.0, 676.0, 302.0, 689.0]}, {"polygon": [[434.0, 682.0], [728.0, 682.0], [728.0, 695.0], [434.0, 695.0]], "confidence": 0.9748743772506714, "text": "Open foundation and fine-tuned chat models. arXiv preprint", "bbox": [434.0, 682.0, 728.0, 695.0]}, {"polygon": [[63.0, 690.0], [383.0, 690.0], [383.0, 704.0], [63.0, 704.0]], "confidence": 0.9768276214599609, "text": "[45] Tanzila Rahman, Bicheng Xu, and Leonid Sigal. Watch, listen", "bbox": [63.0, 690.0, 383.0, 704.0]}, {"polygon": [[433.0, 695.0], [586.0, 695.0], [586.0, 709.0], [433.0, 709.0]], "confidence": 0.9586802124977112, "text": "arXiv:2307.09288 , 2023. 2 , 5, 6", "bbox": [433.0, 695.0, 586.0, 709.0]}, {"polygon": [[90.0, 704.0], [384.0, 704.0], [384.0, 718.0], [90.0, 718.0]], "confidence": 0.9836640357971191, "text": "and tell: Multi-modal weakly supervised dense event captioning.", "bbox": [90.0, 704.0, 384.0, 718.0]}, {"polygon": [[408.0, 710.0], [729.0, 710.0], [729.0, 723.0], [408.0, 723.0]], "confidence": 0.9837556481361389, "text": "[61] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi Parikh.", "bbox": [408.0, 710.0, 729.0, 723.0]}, {"polygon": [[90.0, 718.0], [199.0, 718.0], [199.0, 733.0], [90.0, 733.0]], "confidence": 0.9262107014656067, "text": "In Proc. ICCV , 2019. 2 .", "bbox": [90.0, 718.0, 199.0, 733.0]}, {"polygon": [[434.0, 725.0], [728.0, 725.0], [728.0, 737.0], [434.0, 737.0]], "confidence": 0.9789838194847107, "text": "Cider: Consensus-based image description evaluation. In Proc.", "bbox": [434.0, 725.0, 728.0, 737.0]}, {"polygon": [[63.0, 733.0], [382.0, 733.0], [382.0, 748.0], [63.0, 748.0]], "confidence": 0.9828423857688904, "text": "[46] Anna Rohrbach, Marcus Rohrbach, Niket Tandon, and Bernt", "bbox": [63.0, 733.0, 382.0, 748.0]}, {"polygon": [[434.0, 739.0], [615.0, 739.0], [615.0, 751.0], [434.0, 751.0]], "confidence": 0.9599801898002625, "text": "CVPR , pages 4566–4575, 2015. 2 , 3 , 7", "bbox": [434.0, 739.0, 615.0, 751.0]}, {"polygon": [[90.0, 748.0], [382.0, 748.0], [382.0, 762.0], [90.0, 762.0]], "confidence": 0.9799642562866211, "text": "Schiele. A dataset for movie description. In Proc. CVPR , 2015. 2", "bbox": [90.0, 748.0, 382.0, 762.0]}, {"polygon": [[409.0, 751.0], [729.0, 751.0], [729.0, 765.0], [409.0, 765.0]], "confidence": 0.9832702875137329, "text": "[62] Jingwen Wang, Wenhao Jiang, Lin Ma, Wei Liu, and Yong Xu.", "bbox": [409.0, 751.0, 729.0, 765.0]}, {"polygon": [[63.0, 763.0], [383.0, 763.0], [383.0, 777.0], [63.0, 777.0]], "confidence": 0.984272837638855, "text": "[47] Anna Rohrbach, Atousa Torabi, Marcus Rohrbach, Niket Tandon,", "bbox": [63.0, 763.0, 383.0, 777.0]}, {"polygon": [[435.0, 767.0], [728.0, 767.0], [728.0, 779.0], [435.0, 779.0]], "confidence": 0.9848377704620361, "text": "Bidirectional attentive fusion with context gating for dense video", "bbox": [435.0, 767.0, 728.0, 779.0]}, {"polygon": [[89.0, 777.0], [382.0, 777.0], [382.0, 791.0], [89.0, 791.0]], "confidence": 0.9817519187927246, "text": "Christopher Pal, Hugo Larochelle, Aaron Courville, and Bernt", "bbox": [89.0, 777.0, 382.0, 791.0]}, {"polygon": [[435.0, 782.0], [602.0, 782.0], [602.0, 794.0], [435.0, 794.0]], "confidence": 0.9634004235267639, "text": "captioning. In Proc. CVPR , 2018. 2", "bbox": [435.0, 782.0, 602.0, 794.0]}, {"polygon": [[89.0, 792.0], [362.0, 792.0], [362.0, 806.0], [89.0, 806.0]], "confidence": 0.9797972440719604, "text": "Schiele. Movie description. IJCV , 123(1):94–120, 2017. 2", "bbox": [89.0, 792.0, 362.0, 806.0]}, {"polygon": [[409.0, 794.0], [728.0, 794.0], [728.0, 808.0], [409.0, 808.0]], "confidence": 0.98405921459198, "text": "[63] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin", "bbox": [409.0, 794.0, 728.0, 808.0]}, {"polygon": [[63.0, 806.0], [383.0, 806.0], [383.0, 821.0], [63.0, 821.0]], "confidence": 0.9835765957832336, "text": "[48] Paul Hongsuck Seo, Arsha Nagrani, Anurag Arnab, and Cordelia", "bbox": [63.0, 806.0, 383.0, 821.0]}, {"polygon": [[434.0, 809.0], [728.0, 809.0], [728.0, 822.0], [434.0, 822.0]], "confidence": 0.9744597673416138, "text": "Lin, Zhe Gan, Zicheng Liu, Ce Liu, and Lijuan Wang. Git: A", "bbox": [434.0, 809.0, 728.0, 822.0]}, {"polygon": [[90.0, 821.0], [383.0, 821.0], [383.0, 836.0], [90.0, 836.0]], "confidence": 0.9825254678726196, "text": "Schmid. End-to-end generative pretraining for multimodal video", "bbox": [90.0, 821.0, 383.0, 836.0]}, {"polygon": [[436.0, 824.0], [728.0, 824.0], [728.0, 836.0], [436.0, 836.0]], "confidence": 0.983299970626831, "text": "generative image-to-text transformer for vision and language.", "bbox": [436.0, 824.0, 728.0, 836.0]}, {"polygon": [[90.0, 836.0], [353.0, 836.0], [353.0, 850.0], [90.0, 850.0]], "confidence": 0.9771407842636108, "text": "captioning. In Proc. CVPR , pages 17959–17968, 2022. 2", "bbox": [90.0, 836.0, 353.0, 850.0]}, {"polygon": [[434.0, 838.0], [629.0, 838.0], [629.0, 850.0], [434.0, 850.0]], "confidence": 0.9712642431259155, "text": "arXiv preprint arXiv:2205.14100 , 2022. 3", "bbox": [434.0, 838.0, 629.0, 850.0]}, {"polygon": [[63.0, 850.0], [382.0, 850.0], [382.0, 865.0], [63.0, 865.0]], "confidence": 0.9819979071617126, "text": "[49] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu", "bbox": [63.0, 850.0, 382.0, 865.0]}, {"polygon": [[432.0, 851.0], [728.0, 851.0], [728.0, 865.0], [432.0, 865.0]], "confidence": 0.9813174605369568, "text": "Teng Wang, Huicheng Zheng, Mingjing Yu, Qian Tian, and", "bbox": [432.0, 851.0, 728.0, 865.0]}, {"polygon": [[410.0, 853.0], [434.0, 853.0], [434.0, 865.0], [410.0, 865.0]], "confidence": 0.7869150638580322, "text": "[64]", "bbox": [410.0, 853.0, 434.0, 865.0]}, {"polygon": [[90.0, 865.0], [382.0, 865.0], [382.0, 879.0], [90.0, 879.0]], "confidence": 0.9806044101715088, "text": "Soricut. Conceptual captions: A cleaned, hypernymed, image", "bbox": [90.0, 865.0, 382.0, 879.0]}, {"polygon": [[435.0, 865.0], [728.0, 865.0], [728.0, 879.0], [435.0, 879.0]], "confidence": 0.9826709628105164, "text": "Haifeng Hu. Event-centric hierarchical representation for dense", "bbox": [435.0, 865.0, 728.0, 879.0]}, {"polygon": [[90.0, 880.0], [382.0, 880.0], [382.0, 893.0], [90.0, 893.0]], "confidence": 0.9733290672302246, "text": "alt-text dataset for automatic image captioning.  In Association", "bbox": [90.0, 880.0, 382.0, 893.0]}, {"polygon": [[435.0, 880.0], [728.0, 880.0], [728.0, 893.0], [435.0, 893.0]], "confidence": 0.979125440120697, "text": "video captioning.  IEEE Transactions on Circuits and Systems", "bbox": [435.0, 880.0, 728.0, 893.0]}, {"polygon": [[89.0, 894.0], [269.0, 894.0], [269.0, 907.0], [89.0, 907.0]], "confidence": 0.9738783836364746, "text": "for Computational Linguistics , 2018. 2", "bbox": [89.0, 894.0, 269.0, 907.0]}, {"polygon": [[435.0, 894.0], [575.0, 894.0], [575.0, 907.0], [435.0, 907.0]], "confidence": 0.9637130498886108, "text": "for Video Technology , 2020. 2", "bbox": [435.0, 894.0, 575.0, 907.0]}, {"polygon": [[64.0, 909.0], [384.0, 909.0], [384.0, 922.0], [64.0, 922.0]], "confidence": 0.9833844900131226, "text": "[50] Zhiqiang Shen, Jianguo Li, Zhou Su, Minjun Li, Yurong Chen,", "bbox": [64.0, 909.0, 384.0, 922.0]}, {"polygon": [[409.0, 909.0], [729.0, 909.0], [729.0, 923.0], [409.0, 923.0]], "confidence": 0.9821097254753113, "text": "[65] Teng Wang, Ruimao Zhang, Zhichao Lu, Feng Zheng, Ran", "bbox": [409.0, 909.0, 729.0, 923.0]}, {"polygon": [[90.0, 923.0], [382.0, 923.0], [382.0, 936.0], [90.0, 936.0]], "confidence": 0.9801644086837769, "text": "Yu-Gang Jiang, and Xiangyang Xue. Weakly supervised dense", "bbox": [90.0, 923.0, 382.0, 936.0]}, {"polygon": [[433.0, 923.0], [728.0, 923.0], [728.0, 936.0], [433.0, 936.0]], "confidence": 0.9801542162895203, "text": "Cheng, and Ping Luo. End-to-end dense video captioning with", "bbox": [433.0, 923.0, 728.0, 936.0]}, {"polygon": [[90.0, 937.0], [284.0, 937.0], [284.0, 951.0], [90.0, 951.0]], "confidence": 0.9695387482643127, "text": "video captioning. In Proc. CVPR , 2017. 2", "bbox": [90.0, 937.0, 284.0, 951.0]}, {"polygon": [[435.0, 937.0], [641.0, 937.0], [641.0, 951.0], [435.0, 951.0]], "confidence": 0.973107099533081, "text": "parallel decoding. In Proc. ICCV , 2021. 2 , 3", "bbox": [435.0, 937.0, 641.0, 951.0]}, {"polygon": [[389.0, 975.0], [405.0, 975.0], [405.0, 990.0], [389.0, 990.0]], "confidence": 0.5832688808441162, "text": "10", "bbox": [389.0, 975.0, 405.0, 990.0]}], "languages": ["en"], "image_bbox": [0.0, 0.0, 816.0, 1056.0], "page": 10}, {"text_lines": [{"polygon": [[409.0, 96.0], [728.0, 96.0], [728.0, 111.0], [409.0, 111.0]], "confidence": 0.9761642217636108, "text": "[82] Luowei Zhou, Chenliang Xu, and Jason J Corso.  Towards", "bbox": [409.0, 96.0, 728.0, 111.0]}, {"polygon": [[63.0, 97.0], [383.0, 97.0], [383.0, 111.0], [63.0, 111.0]], "confidence": 0.9815961718559265, "text": "[66] Yujia Wang, Wei Liang, Haikun Huang, Yongqi Zhang,", "bbox": [63.0, 97.0, 383.0, 111.0]}, {"polygon": [[90.0, 112.0], [252.0, 112.0], [252.0, 125.0], [90.0, 125.0]], "confidence": 0.9636300802230835, "text": "Dingzeyu Li, and Lap-Fai Yu.", "bbox": [90.0, 112.0, 252.0, 125.0]}, {"polygon": [[257.0, 112.0], [384.0, 112.0], [384.0, 125.0], [257.0, 125.0]], "confidence": 0.9554181098937988, "text": "Toward automatic audio", "bbox": [257.0, 112.0, 384.0, 125.0]}, {"polygon": [[435.0, 112.0], [728.0, 112.0], [728.0, 125.0], [435.0, 125.0]], "confidence": 0.9840729832649231, "text": "automatic learning of procedures from web instructional videos.", "bbox": [435.0, 112.0, 728.0, 125.0]}, {"polygon": [[89.0, 126.0], [383.0, 126.0], [383.0, 139.0], [89.0, 139.0]], "confidence": 0.9786953926086426, "text": "description generation for accessible videos.  In Proceedings", "bbox": [89.0, 126.0, 383.0, 139.0]}, {"polygon": [[435.0, 128.0], [516.0, 128.0], [516.0, 139.0], [435.0, 139.0]], "confidence": 0.9290839433670044, "text": "In AAAI , 2018. 2", "bbox": [435.0, 128.0, 516.0, 139.0]}, {"polygon": [[89.0, 140.0], [383.0, 140.0], [383.0, 154.0], [89.0, 154.0]], "confidence": 0.9815184473991394, "text": "of the 2021 CHI Conference on Human Factors in Computing", "bbox": [89.0, 140.0, 383.0, 154.0]}, {"polygon": [[408.0, 141.0], [728.0, 141.0], [728.0, 155.0], [408.0, 155.0]], "confidence": 0.9836547374725342, "text": "[83] Luowei Zhou, Yingbo Zhou, Jason J Corso, Richard Socher, and", "bbox": [408.0, 141.0, 728.0, 155.0]}, {"polygon": [[433.0, 155.0], [728.0, 155.0], [728.0, 169.0], [433.0, 169.0]], "confidence": 0.9819717407226562, "text": "Caiming Xiong. End-to-end dense video captioning with masked", "bbox": [433.0, 155.0, 728.0, 169.0]}, {"polygon": [[90.0, 156.0], [225.0, 156.0], [225.0, 168.0], [90.0, 168.0]], "confidence": 0.9583914875984192, "text": "Systems , pages 1–12, 2021. 1.", "bbox": [90.0, 156.0, 225.0, 168.0]}, {"polygon": [[63.0, 170.0], [383.0, 170.0], [383.0, 184.0], [63.0, 184.0]], "confidence": 0.9823786616325378, "text": "[67] Yu Xiong, Qingqiu Huang, Lingfeng Guo, Hang Zhou, Bolei", "bbox": [63.0, 170.0, 383.0, 184.0]}, {"polygon": [[435.0, 171.0], [616.0, 171.0], [616.0, 183.0], [435.0, 183.0]], "confidence": 0.9716371893882751, "text": "transformer. In Proc. CVPR , 2018. 2 , 3", "bbox": [435.0, 171.0, 616.0, 183.0]}, {"polygon": [[90.0, 185.0], [383.0, 185.0], [383.0, 199.0], [90.0, 199.0]], "confidence": 0.9760352373123169, "text": "Zhou, and Dahua Lin.  A graph-based framework to bridge", "bbox": [90.0, 185.0, 383.0, 199.0]}, {"polygon": [[408.0, 185.0], [728.0, 185.0], [728.0, 199.0], [408.0, 199.0]], "confidence": 0.9823483824729919, "text": "[84] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed", "bbox": [408.0, 185.0, 728.0, 199.0]}, {"polygon": [[90.0, 200.0], [382.0, 200.0], [382.0, 213.0], [90.0, 213.0]], "confidence": 0.9796295762062073, "text": "movies and synopses. In Proc. ICCV , pages 4592–4601, 2019. 2", "bbox": [90.0, 200.0, 382.0, 213.0]}, {"polygon": [[495.0, 200.0], [729.0, 200.0], [729.0, 213.0], [495.0, 213.0]], "confidence": 0.9738847613334656, "text": "MiniGPT-4: Enhancing vision-language under-", "bbox": [495.0, 200.0, 729.0, 213.0]}, {"polygon": [[435.0, 201.0], [488.0, 201.0], [488.0, 212.0], [435.0, 212.0]], "confidence": 0.9045946002006531, "text": "Elhoseiny.", "bbox": [435.0, 201.0, 488.0, 212.0]}, {"polygon": [[435.0, 214.0], [728.0, 214.0], [728.0, 227.0], [435.0, 227.0]], "confidence": 0.9820939302444458, "text": "standing with advanced large language models. arXiv preprint", "bbox": [435.0, 214.0, 728.0, 227.0]}, {"polygon": [[63.0, 215.0], [383.0, 215.0], [383.0, 229.0], [63.0, 229.0]], "confidence": 0.9832161068916321, "text": "[68] Antoine Yang, Arsha Nagrani, Paul Hongsuck Seo, Antoine", "bbox": [63.0, 215.0, 383.0, 229.0]}, {"polygon": [[433.0, 228.0], [584.0, 228.0], [584.0, 241.0], [433.0, 241.0]], "confidence": 0.9648849368095398, "text": "arXiv:2304.10592 , 2023. 2 , 3 , 5", "bbox": [433.0, 228.0, 584.0, 241.0]}, {"polygon": [[89.0, 229.0], [383.0, 229.0], [383.0, 243.0], [89.0, 243.0]], "confidence": 0.9827352166175842, "text": "Miech, Jordi Pont-Tuset, Ivan Laptev, Josef Sivic, and Cordelia", "bbox": [89.0, 229.0, 383.0, 243.0]}, {"polygon": [[91.0, 244.0], [384.0, 244.0], [384.0, 257.0], [91.0, 257.0]], "confidence": 0.9756196737289429, "text": "Schmid. Vid2seq: Large-scale pretraining of a visual lan-", "bbox": [91.0, 244.0, 384.0, 257.0]}, {"polygon": [[91.0, 258.0], [383.0, 258.0], [383.0, 271.0], [91.0, 271.0]], "confidence": 0.9753092527389526, "text": "guage model for dense video captioning.  arXiv preprint", "bbox": [91.0, 258.0, 383.0, 271.0]}, {"polygon": [[89.0, 271.0], [229.0, 271.0], [229.0, 286.0], [89.0, 286.0]], "confidence": 0.9552658200263977, "text": "arXiv:2302.14115 , 2023. 2 , 3", "bbox": [89.0, 271.0, 229.0, 286.0]}, {"polygon": [[64.0, 289.0], [383.0, 289.0], [383.0, 302.0], [64.0, 302.0]], "confidence": 0.982411801815033, "text": "[69] Zhengyuan Yang, Linjie Li, Kevin Lin, Jianfeng Wang,", "bbox": [64.0, 289.0, 383.0, 302.0]}, {"polygon": [[90.0, 303.0], [383.0, 303.0], [383.0, 316.0], [90.0, 316.0]], "confidence": 0.9803958535194397, "text": "Chung-Ching Lin, Zicheng Liu, and Lijuan Wang.  The dawn", "bbox": [90.0, 303.0, 383.0, 316.0]}, {"polygon": [[90.0, 318.0], [382.0, 318.0], [382.0, 330.0], [90.0, 330.0]], "confidence": 0.9796614646911621, "text": "of LMMs: Preliminary explorations with gpt-4v (ision). arXiv", "bbox": [90.0, 318.0, 382.0, 330.0]}, {"polygon": [[90.0, 332.0], [267.0, 332.0], [267.0, 344.0], [90.0, 344.0]], "confidence": 0.9629480242729187, "text": "preprint arXiv:2309.17421 , 9, 2023. 4", "bbox": [90.0, 332.0, 267.0, 344.0]}, {"polygon": [[63.0, 346.0], [382.0, 346.0], [382.0, 360.0], [63.0, 360.0]], "confidence": 0.9762521386146545, "text": "[70] Yanzhi Yi, Hangyu Deng, and Jinglu Hu. Improving image", "bbox": [63.0, 346.0, 382.0, 360.0]}, {"polygon": [[90.0, 361.0], [383.0, 361.0], [383.0, 375.0], [90.0, 375.0]], "confidence": 0.9834173321723938, "text": "captioning evaluation by considering inter references variance.", "bbox": [90.0, 361.0, 383.0, 375.0]}, {"polygon": [[90.0, 376.0], [382.0, 376.0], [382.0, 389.0], [90.0, 389.0]], "confidence": 0.981680154800415, "text": "In Proceedings of the 58th Annual Meeting of the Association", "bbox": [90.0, 376.0, 382.0, 389.0]}, {"polygon": [[89.0, 390.0], [344.0, 390.0], [344.0, 403.0], [89.0, 403.0]], "confidence": 0.978681206703186, "text": "for Computational Linguistics , pages 985–994, 2020. 3", "bbox": [89.0, 390.0, 344.0, 403.0]}, {"polygon": [[63.0, 405.0], [383.0, 405.0], [383.0, 419.0], [63.0, 419.0]], "confidence": 0.983967661857605, "text": "[71] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba", "bbox": [63.0, 405.0, 383.0, 419.0]}, {"polygon": [[90.0, 418.0], [382.0, 420.0], [382.0, 435.0], [90.0, 433.0]], "confidence": 0.9821891188621521, "text": "Seyedhosseini, and Yonghui Wu. CoCa: Contrastive captioners", "bbox": [90.0, 418.0, 382.0, 435.0]}, {"polygon": [[90.0, 434.0], [382.0, 434.0], [382.0, 448.0], [90.0, 448.0]], "confidence": 0.9722062945365906, "text": "are image-text foundation models. Transactions on Machine", "bbox": [90.0, 434.0, 382.0, 448.0]}, {"polygon": [[90.0, 450.0], [231.0, 450.0], [231.0, 462.0], [90.0, 462.0]], "confidence": 0.9611691236495972, "text": "Learning Research , 2022. 1, 2", "bbox": [90.0, 450.0, 231.0, 462.0]}, {"polygon": [[63.0, 464.0], [329.0, 464.0], [329.0, 478.0], [63.0, 478.0]], "confidence": 0.9589676856994629, "text": "[72] Keunwoo Peter Yu. VideoBLIP, 2023. 2 , 3 , 5, 8 , 13", "bbox": [63.0, 464.0, 329.0, 478.0]}, {"polygon": [[63.0, 480.0], [383.0, 480.0], [383.0, 494.0], [63.0, 494.0]], "confidence": 0.9835363626480103, "text": "[73] Shoubin Yu, Jaemin Cho, Prateek Yadav, and Mohit Bansal.", "bbox": [63.0, 480.0, 383.0, 494.0]}, {"polygon": [[90.0, 495.0], [383.0, 495.0], [383.0, 509.0], [90.0, 509.0]], "confidence": 0.980643093585968, "text": "Self-chained image-language model for video localization and", "bbox": [90.0, 495.0, 383.0, 509.0]}, {"polygon": [[90.0, 510.0], [279.0, 510.0], [279.0, 523.0], [90.0, 523.0]], "confidence": 0.9672503471374512, "text": "question answering. In NeurIPS , 2023. 3", "bbox": [90.0, 510.0, 279.0, 523.0]}, {"polygon": [[63.0, 525.0], [382.0, 525.0], [382.0, 539.0], [63.0, 539.0]], "confidence": 0.9822203516960144, "text": "[74] Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu,", "bbox": [63.0, 525.0, 382.0, 539.0]}, {"polygon": [[90.0, 539.0], [384.0, 539.0], [384.0, 554.0], [90.0, 554.0]], "confidence": 0.9833613634109497, "text": "Jae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi. MERLOT:", "bbox": [90.0, 539.0, 384.0, 554.0]}, {"polygon": [[89.0, 554.0], [382.0, 554.0], [382.0, 568.0], [89.0, 568.0]], "confidence": 0.9823048710823059, "text": "Multimodal neural script knowledge models. In NeurIPS , 2021. 1", "bbox": [89.0, 554.0, 382.0, 568.0]}, {"polygon": [[63.0, 570.0], [382.0, 570.0], [382.0, 583.0], [63.0, 583.0]], "confidence": 0.9836924076080322, "text": "[75] Rowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yanpeng", "bbox": [63.0, 570.0, 382.0, 583.0]}, {"polygon": [[89.0, 584.0], [382.0, 584.0], [382.0, 597.0], [89.0, 597.0]], "confidence": 0.9831280708312988, "text": "Zhao, Mohammadreza Salehi, Aditya Kusupati, Jack Hessel, Ali", "bbox": [89.0, 584.0, 382.0, 597.0]}, {"polygon": [[90.0, 600.0], [382.0, 600.0], [382.0, 611.0], [90.0, 611.0]], "confidence": 0.9825971722602844, "text": "Farhadi, and Yejin Choi. MERLOT reserve: Multimodal neural", "bbox": [90.0, 600.0, 382.0, 611.0]}, {"polygon": [[90.0, 613.0], [383.0, 613.0], [383.0, 626.0], [90.0, 626.0]], "confidence": 0.9814613461494446, "text": "script knowledge through vision and language and sound.  In", "bbox": [90.0, 613.0, 383.0, 626.0]}, {"polygon": [[90.0, 628.0], [163.0, 628.0], [163.0, 640.0], [90.0, 640.0]], "confidence": 0.9134103655815125, "text": "CVPR , 2022. 2", "bbox": [90.0, 628.0, 163.0, 640.0]}, {"polygon": [[63.0, 642.0], [382.0, 643.0], [382.0, 657.0], [63.0, 656.0]], "confidence": 0.9833235144615173, "text": "[76] Chaoyi Zhang, Kevin Lin, Zhengyuan Yang, Jianfeng Wang,", "bbox": [63.0, 642.0, 382.0, 657.0]}, {"polygon": [[89.0, 658.0], [382.0, 658.0], [382.0, 671.0], [89.0, 671.0]], "confidence": 0.9815846681594849, "text": "Linjie Li, Chung-Ching Lin, Zicheng Liu, and Lijuan Wang.", "bbox": [89.0, 658.0, 382.0, 671.0]}, {"polygon": [[90.0, 672.0], [382.0, 672.0], [382.0, 685.0], [90.0, 685.0]], "confidence": 0.9791306853294373, "text": "Mm-narrator: Narrating long-form videos with multimodal", "bbox": [90.0, 672.0, 382.0, 685.0]}, {"polygon": [[90.0, 686.0], [378.0, 686.0], [378.0, 699.0], [90.0, 699.0]], "confidence": 0.9795414209365845, "text": "in-context learning. arXiv preprint arXiv:2311.17435 , 2023. 8", "bbox": [90.0, 686.0, 378.0, 699.0]}, {"polygon": [[63.0, 701.0], [299.0, 701.0], [299.0, 715.0], [63.0, 715.0]], "confidence": 0.9748650789260864, "text": "[77] Hang Zhang, Xin Li, and Lidong Bing.", "bbox": [63.0, 701.0, 299.0, 715.0]}, {"polygon": [[308.0, 703.0], [384.0, 703.0], [384.0, 714.0], [308.0, 714.0]], "confidence": 0.9109699726104736, "text": "Video-LLaMA:", "bbox": [308.0, 703.0, 384.0, 714.0]}, {"polygon": [[90.0, 716.0], [384.0, 716.0], [384.0, 730.0], [90.0, 730.0]], "confidence": 0.9820716977119446, "text": "An instruction-tuned audio-visual language model for video", "bbox": [90.0, 716.0, 384.0, 730.0]}, {"polygon": [[90.0, 729.0], [363.0, 729.0], [363.0, 744.0], [90.0, 744.0]], "confidence": 0.9728041887283325, "text": "understanding. In EMNLP 2023 Demo , 2023. 2 , 3 , 5 , 8 , 13", "bbox": [90.0, 729.0, 363.0, 744.0]}, {"polygon": [[63.0, 746.0], [383.0, 745.0], [383.0, 760.0], [63.0, 761.0]], "confidence": 0.9834794402122498, "text": "[78] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe,", "bbox": [63.0, 746.0, 383.0, 760.0]}, {"polygon": [[91.0, 762.0], [384.0, 762.0], [384.0, 774.0], [91.0, 774.0]], "confidence": 0.9813410043716431, "text": "Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab,", "bbox": [91.0, 762.0, 384.0, 774.0]}, {"polygon": [[90.0, 775.0], [383.0, 775.0], [383.0, 789.0], [90.0, 789.0]], "confidence": 0.9763059616088867, "text": "Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer", "bbox": [90.0, 775.0, 383.0, 789.0]}, {"polygon": [[89.0, 789.0], [380.0, 789.0], [380.0, 803.0], [89.0, 803.0]], "confidence": 0.9805151224136353, "text": "language models. arXiv preprint arXiv:2205.01068 , 2022. 2 , 5", "bbox": [89.0, 789.0, 380.0, 803.0]}, {"polygon": [[63.0, 805.0], [383.0, 805.0], [383.0, 819.0], [63.0, 819.0]], "confidence": 0.984337329864502, "text": "[79] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger,", "bbox": [63.0, 805.0, 383.0, 819.0]}, {"polygon": [[90.0, 820.0], [383.0, 820.0], [383.0, 833.0], [90.0, 833.0]], "confidence": 0.975044310092926, "text": "and Yoav Artzi. BERTScore: Evaluating text generation with", "bbox": [90.0, 820.0, 383.0, 833.0]}, {"polygon": [[89.0, 834.0], [233.0, 834.0], [233.0, 847.0], [89.0, 847.0]], "confidence": 0.9498063325881958, "text": "bert. In Proc. ICLR , 2020. 2 , 31", "bbox": [89.0, 834.0, 233.0, 847.0]}, {"polygon": [[63.0, 849.0], [384.0, 849.0], [384.0, 864.0], [63.0, 864.0]], "confidence": 0.9842899441719055, "text": "[80] Yue Zhao, Ishan Misra, Philipp Krähenbühl, and Rohit Girdhar.", "bbox": [63.0, 849.0, 384.0, 864.0]}, {"polygon": [[89.0, 864.0], [383.0, 864.0], [383.0, 878.0], [89.0, 878.0]], "confidence": 0.9835607409477234, "text": "Learning video representations from large language models. In", "bbox": [89.0, 864.0, 383.0, 878.0]}, {"polygon": [[89.0, 878.0], [163.0, 878.0], [163.0, 892.0], [89.0, 892.0]], "confidence": 0.9217715859413147, "text": "CVPR , 2023. 2", "bbox": [89.0, 878.0, 163.0, 892.0]}, {"polygon": [[63.0, 894.0], [383.0, 894.0], [383.0, 908.0], [63.0, 908.0]], "confidence": 0.9828906059265137, "text": "[81] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang,", "bbox": [63.0, 894.0, 383.0, 908.0]}, {"polygon": [[89.0, 907.0], [382.0, 909.0], [382.0, 923.0], [89.0, 922.0]], "confidence": 0.982037365436554, "text": "Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng", "bbox": [89.0, 907.0, 382.0, 923.0]}, {"polygon": [[89.0, 923.0], [382.0, 923.0], [382.0, 937.0], [89.0, 937.0]], "confidence": 0.975926399230957, "text": "Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and", "bbox": [89.0, 923.0, 382.0, 937.0]}, {"polygon": [[89.0, 937.0], [376.0, 937.0], [376.0, 951.0], [89.0, 951.0]], "confidence": 0.9759005308151245, "text": "chatbot arena. arXiv preprint arXiv:2306.05685 , 2023. 2 , 3 , 6", "bbox": [89.0, 937.0, 376.0, 951.0]}, {"polygon": [[389.0, 975.0], [403.0, 975.0], [403.0, 990.0], [389.0, 990.0]], "confidence": 0.5435163378715515, "text": "11", "bbox": [389.0, 975.0, 403.0, 990.0]}], "languages": ["en"], "image_bbox": [0.0, 0.0, 816.0, 1056.0], "page": 11}, {"text_lines": [{"polygon": [[218.0, 91.0], [575.0, 91.0], [575.0, 113.0], [218.0, 113.0]], "confidence": 0.9762691259384155, "text": "AutoAD III: The Prequel – Back to the Pixels", "bbox": [218.0, 91.0, 575.0, 113.0]}, {"polygon": [[359.0, 127.0], [433.0, 126.0], [433.0, 145.0], [359.0, 146.0]], "confidence": 0.8861219882965088, "text": "Appendix", "bbox": [359.0, 127.0, 433.0, 145.0]}, {"polygon": [[389.0, 334.0], [411.0, 334.0], [411.0, 347.0], [389.0, 347.0]], "confidence": 0.5955007672309875, "text": "(a)", "bbox": [389.0, 334.0, 411.0, 347.0]}, {"polygon": [[89.0, 356.0], [108.0, 356.0], [108.0, 364.0], [89.0, 364.0]], "confidence": 0.7447526454925537, "text": "#10", "bbox": [89.0, 356.0, 108.0, 364.0]}, {"polygon": [[74.0, 383.0], [89.0, 385.0], [86.0, 450.0], [71.0, 449.0]], "confidence": 0.9149404168128967, "text": "MD clip index", "bbox": [74.0, 383.0, 89.0, 450.0]}, {"polygon": [[93.0, 396.0], [106.0, 396.0], [106.0, 404.0], [93.0, 404.0]], "confidence": 0.655113697052002, "text": "#7", "bbox": [93.0, 396.0, 106.0, 404.0]}, {"polygon": [[93.0, 408.0], [106.0, 408.0], [106.0, 417.0], [93.0, 417.0]], "confidence": 0.6640987396240234, "text": "#6", "bbox": [93.0, 408.0, 106.0, 417.0]}, {"polygon": [[93.0, 432.0], [106.0, 432.0], [106.0, 441.0], [93.0, 441.0]], "confidence": 0.6653891205787659, "text": "#4", "bbox": [93.0, 432.0, 106.0, 441.0]}, {"polygon": [[140.0, 490.0], [175.0, 490.0], [175.0, 501.0], [140.0, 501.0]], "confidence": 0.8565519452095032, "text": "0:45:00", "bbox": [140.0, 490.0, 175.0, 501.0]}, {"polygon": [[235.0, 490.0], [273.0, 490.0], [273.0, 501.0], [235.0, 501.0]], "confidence": 0.8568987846374512, "text": "01:00:00", "bbox": [235.0, 490.0, 273.0, 501.0]}, {"polygon": [[330.0, 490.0], [370.0, 490.0], [370.0, 501.0], [330.0, 501.0]], "confidence": 0.8683497905731201, "text": "01:15:00", "bbox": [330.0, 490.0, 370.0, 501.0]}, {"polygon": [[428.0, 490.0], [466.0, 490.0], [466.0, 501.0], [428.0, 501.0]], "confidence": 0.8773103952407837, "text": "01:30:00", "bbox": [428.0, 490.0, 466.0, 501.0]}, {"polygon": [[523.0, 490.0], [563.0, 490.0], [563.0, 500.0], [523.0, 500.0]], "confidence": 0.8832765221595764, "text": "01:45:00", "bbox": [523.0, 490.0, 563.0, 500.0]}, {"polygon": [[620.0, 490.0], [659.0, 490.0], [659.0, 501.0], [620.0, 501.0]], "confidence": 0.8470784425735474, "text": "02:00:00", "bbox": [620.0, 490.0, 659.0, 501.0]}, {"polygon": [[264.0, 501.0], [571.0, 501.0], [571.0, 515.0], [264.0, 515.0]], "confidence": 0.9838835597038269, "text": "AudioVault timestamps of the full movie \"The Philadelphia Story\"", "bbox": [264.0, 501.0, 571.0, 515.0]}, {"polygon": [[388.0, 531.0], [413.0, 531.0], [413.0, 547.0], [388.0, 547.0]], "confidence": 0.7433584332466125, "text": "(b)", "bbox": [388.0, 531.0, 413.0, 547.0]}, {"polygon": [[63.0, 555.0], [728.0, 555.0], [728.0, 569.0], [63.0, 569.0]], "confidence": 0.9858585596084595, "text": "Figure A.1. (a) More examples of RANSAC for audio-audio alignment. Five samples are chosen from five different movies. The four samples", "bbox": [63.0, 555.0, 728.0, 569.0]}, {"polygon": [[63.0, 570.0], [728.0, 570.0], [728.0, 584.0], [63.0, 584.0]], "confidence": 0.9917228817939758, "text": "on the left are successful RANSAC results that have low MSE and reasonable slopes. The sample on the right is filtered out since the fitted result", "bbox": [63.0, 570.0, 728.0, 584.0]}, {"polygon": [[64.0, 586.0], [728.0, 586.0], [728.0, 598.0], [64.0, 598.0]], "confidence": 0.9906472563743591, "text": "from RANSAC has a high MSE and the slope does match our criteria. Note that the inliers ( blue dots) are not continuous because those time", "bbox": [64.0, 586.0, 728.0, 598.0]}, {"polygon": [[63.0, 600.0], [729.0, 600.0], [729.0, 612.0], [63.0, 612.0]], "confidence": 0.9878608584403992, "text": "segments correspond to AD audio and have been masked out before the alignment process. (b) Audio-audio alignment results of a full movie.", "bbox": [63.0, 600.0, 729.0, 612.0]}, {"polygon": [[64.0, 614.0], [728.0, 614.0], [728.0, 626.0], [64.0, 626.0]], "confidence": 0.9875791072845459, "text": "We localize ten CMD clips from the corresponding full-movie AudioVault audio file by visualizing their aligned timestamps from the RANSAC", "bbox": [64.0, 614.0, 728.0, 626.0]}, {"polygon": [[63.0, 627.0], [728.0, 627.0], [728.0, 639.0], [63.0, 639.0]], "confidence": 0.990580677986145, "text": "algorithm. We can clearly see that CMD dataset contains multiple discontinuous movie clips, but the precise timestamps of each clip are obtained", "bbox": [63.0, 627.0, 728.0, 639.0]}, {"polygon": [[63.0, 640.0], [684.0, 640.0], [684.0, 654.0], [63.0, 654.0]], "confidence": 0.9893937110900879, "text": "from our alignment algorithm, which are not provided by CMD. The example movie is 'The Philadelphia Story' (1940, tt0032904 ).", "bbox": [63.0, 640.0, 684.0, 654.0]}, {"polygon": [[63.0, 668.0], [184.0, 668.0], [184.0, 686.0], [63.0, 686.0]], "confidence": 0.9449358582496643, "text": "A. Dataset Details", "bbox": [63.0, 668.0, 184.0, 686.0]}, {"polygon": [[409.0, 671.0], [728.0, 671.0], [728.0, 685.0], [409.0, 685.0]], "confidence": 0.9797112941741943, "text": "use the spacy package in Python to detect unique names in", "bbox": [409.0, 671.0, 728.0, 685.0]}, {"polygon": [[408.0, 687.0], [728.0, 687.0], [728.0, 701.0], [408.0, 701.0]], "confidence": 0.9768913388252258, "text": "the YouTube ASR. We discard videos with more than 5 unique", "bbox": [408.0, 687.0, 728.0, 701.0]}, {"polygon": [[63.0, 696.0], [333.0, 696.0], [333.0, 712.0], [63.0, 712.0]], "confidence": 0.9687034487724304, "text": "A.1. CMD-AD – Pixels from Aligned CMD", "bbox": [63.0, 696.0, 333.0, 712.0]}, {"polygon": [[408.0, 703.0], [729.0, 703.0], [729.0, 716.0], [408.0, 716.0]], "confidence": 0.981397271156311, "text": "names, because we find those videos typically correspond to", "bbox": [408.0, 703.0, 729.0, 716.0]}, {"polygon": [[409.0, 718.0], [729.0, 718.0], [729.0, 732.0], [409.0, 732.0]], "confidence": 0.9828699827194214, "text": "News or Sports programmes and are not suitable to be trans-", "bbox": [409.0, 718.0, 729.0, 732.0]}, {"polygon": [[63.0, 722.0], [383.0, 722.0], [383.0, 737.0], [63.0, 737.0]], "confidence": 0.977818489074707, "text": "Following the main paper Figure 2, a few more examples", "bbox": [63.0, 722.0, 383.0, 737.0]}, {"polygon": [[409.0, 733.0], [728.0, 733.0], [728.0, 748.0], [409.0, 748.0]], "confidence": 0.9792232513427734, "text": "formed to movie AD style. (ii) we use the spacy package", "bbox": [409.0, 733.0, 728.0, 748.0]}, {"polygon": [[63.0, 739.0], [382.0, 739.0], [382.0, 752.0], [63.0, 752.0]], "confidence": 0.9700674414634705, "text": "of RANSAC output are shown in Figure A.1 -(a).  Note that", "bbox": [63.0, 739.0, 382.0, 752.0]}, {"polygon": [[409.0, 749.0], [728.0, 749.0], [728.0, 762.0], [409.0, 762.0]], "confidence": 0.982498288154602, "text": "to detect the subject of each sentence in the HowToCaption", "bbox": [409.0, 749.0, 728.0, 762.0]}, {"polygon": [[63.0, 754.0], [382.0, 754.0], [382.0, 769.0], [63.0, 769.0]], "confidence": 0.9720514416694641, "text": "we filter valid RANSAC output by two thresholds:  (1) the", "bbox": [63.0, 754.0, 382.0, 769.0]}, {"polygon": [[409.0, 764.0], [728.0, 764.0], [728.0, 778.0], [409.0, 778.0]], "confidence": 0.9840434193611145, "text": "dataset. We discard those videos which do not contain a subject", "bbox": [409.0, 764.0, 728.0, 778.0]}, {"polygon": [[63.0, 769.0], [382.0, 769.0], [382.0, 784.0], [63.0, 784.0]], "confidence": 0.9648587703704834, "text": "slope 0.8 < W' < 1.25, and (2) MSE< 100. Additionally, we", "bbox": [63.0, 769.0, 382.0, 784.0]}, {"polygon": [[408.0, 780.0], [728.0, 780.0], [728.0, 794.0], [408.0, 794.0]], "confidence": 0.982967734336853, "text": "in any of the captions, e.g. the captions like 'cut carrots', 'chunk", "bbox": [408.0, 780.0, 728.0, 794.0]}, {"polygon": [[63.0, 784.0], [382.0, 784.0], [382.0, 799.0], [63.0, 799.0]], "confidence": 0.9823753833770752, "text": "visualize the audio-audio alignment results of a full movie", "bbox": [63.0, 784.0, 382.0, 799.0]}, {"polygon": [[409.0, 795.0], [728.0, 795.0], [728.0, 808.0], [409.0, 808.0]], "confidence": 0.9744985699653625, "text": "on', etc. Because our caption transforming stage in the main", "bbox": [409.0, 795.0, 728.0, 808.0]}, {"polygon": [[63.0, 800.0], [201.0, 800.0], [201.0, 814.0], [63.0, 814.0]], "confidence": 0.9550034403800964, "text": "example in Figure A.1 -(b).", "bbox": [63.0, 800.0, 201.0, 814.0]}, {"polygon": [[409.0, 810.0], [728.0, 810.0], [728.0, 825.0], [409.0, 825.0]], "confidence": 0.9842842221260071, "text": "paper Section 3.2 requires the detected subjects. (iii) we extract", "bbox": [409.0, 810.0, 728.0, 825.0]}, {"polygon": [[80.0, 818.0], [383.0, 818.0], [383.0, 832.0], [80.0, 832.0]], "confidence": 0.981075644493103, "text": "The script for conducting audio-audio alignment between", "bbox": [80.0, 818.0, 383.0, 832.0]}, {"polygon": [[408.0, 826.0], [728.0, 826.0], [728.0, 840.0], [408.0, 840.0]], "confidence": 0.9747944474220276, "text": "a few video frames at the introduction of the video – which is", "bbox": [408.0, 826.0, 728.0, 840.0]}, {"polygon": [[63.0, 833.0], [383.0, 833.0], [383.0, 847.0], [63.0, 847.0]], "confidence": 0.9806538820266724, "text": "CMD movie clip and AudioVault audio chunk is shown in", "bbox": [63.0, 833.0, 383.0, 847.0]}, {"polygon": [[409.0, 841.0], [729.0, 841.0], [729.0, 855.0], [409.0, 855.0]], "confidence": 0.9736197590827942, "text": "inferred from the first few ASR timestamps – then we use a", "bbox": [409.0, 841.0, 729.0, 855.0]}, {"polygon": [[63.0, 848.0], [132.0, 848.0], [132.0, 863.0], [63.0, 863.0]], "confidence": 0.9066203832626343, "text": "Algorithm 1.", "bbox": [63.0, 848.0, 132.0, 863.0]}, {"polygon": [[409.0, 857.0], [729.0, 857.0], [729.0, 871.0], [409.0, 871.0]], "confidence": 0.9839842915534973, "text": "face detector 5 to verify whether a face exists in these frames.", "bbox": [409.0, 857.0, 729.0, 871.0]}, {"polygon": [[409.0, 872.0], [728.0, 872.0], [728.0, 886.0], [409.0, 886.0]], "confidence": 0.9830350279808044, "text": "We discard videos that do not have frames with faces because", "bbox": [409.0, 872.0, 728.0, 886.0]}, {"polygon": [[63.0, 878.0], [337.0, 878.0], [337.0, 894.0], [63.0, 894.0]], "confidence": 0.9705198407173157, "text": "A.2. HowTo-AD – Pixels from HowTo100M", "bbox": [63.0, 878.0, 337.0, 894.0]}, {"polygon": [[409.0, 888.0], [630.0, 888.0], [630.0, 902.0], [409.0, 902.0]], "confidence": 0.9769536852836609, "text": "our character bank requires face exemplars.", "bbox": [409.0, 888.0, 630.0, 902.0]}, {"polygon": [[63.0, 904.0], [382.0, 904.0], [382.0, 920.0], [63.0, 920.0]], "confidence": 0.9806137681007385, "text": "The original HowTo100M and HowToCaption datasets have", "bbox": [63.0, 904.0, 382.0, 920.0]}, {"polygon": [[427.0, 905.0], [728.0, 905.0], [728.0, 920.0], [427.0, 920.0]], "confidence": 0.9826242327690125, "text": "A combination of these filters reduces the number of videos", "bbox": [427.0, 905.0, 728.0, 920.0]}, {"polygon": [[63.0, 921.0], [382.0, 921.0], [382.0, 935.0], [63.0, 935.0]], "confidence": 0.9769666790962219, "text": "more than 1.2M videos.  We apply a sequence of filtering", "bbox": [63.0, 921.0, 382.0, 935.0]}, {"polygon": [[64.0, 936.0], [382.0, 936.0], [382.0, 950.0], [64.0, 950.0]], "confidence": 0.9814871549606323, "text": "processes to get high-quality videos for our purpose. (i) we", "bbox": [64.0, 936.0, 382.0, 950.0]}, {"polygon": [[428.0, 939.0], [700.0, 938.0], [700.0, 951.0], [428.0, 952.0]], "confidence": 0.9665262699127197, "text": "https://github.com/ageitgey/face_recognition", "bbox": [428.0, 939.0, 700.0, 951.0]}, {"polygon": [[390.0, 976.0], [403.0, 976.0], [403.0, 990.0], [390.0, 990.0]], "confidence": 0.6558201909065247, "text": "12", "bbox": [390.0, 976.0, 403.0, 990.0]}], "languages": ["en"], "image_bbox": [0.0, 0.0, 816.0, 1056.0], "page": 12}, {"text_lines": [{"polygon": [[80.0, 95.0], [121.0, 95.0], [121.0, 103.0], [80.0, 103.0]], "confidence": 0.8137974143028259, "text": "VfvFZ-Kd6Q", "bbox": [80.0, 95.0, 121.0, 103.0]}, {"polygon": [[78.0, 175.0], [112.0, 175.0], [112.0, 183.0], [78.0, 183.0]], "confidence": 0.7805663347244263, "text": "TPJYaKMu0", "bbox": [78.0, 175.0, 112.0, 183.0]}, {"polygon": [[409.0, 292.0], [728.0, 292.0], [728.0, 305.0], [409.0, 305.0]], "confidence": 0.9734550714492798, "text": "Figure A.3. Category distribution of HowTo-AD dataset.  Similar", "bbox": [409.0, 292.0, 728.0, 305.0]}, {"polygon": [[409.0, 306.0], [729.0, 308.0], [729.0, 321.0], [409.0, 319.0]], "confidence": 0.9827664494514465, "text": "to the original HowTo100M dataset, the HowTo-AD dataset has a", "bbox": [409.0, 306.0, 729.0, 321.0]}, {"polygon": [[410.0, 322.0], [728.0, 322.0], [728.0, 334.0], [410.0, 334.0]], "confidence": 0.9843301177024841, "text": "long-tailed distribution over action categories, dominated by 'Food", "bbox": [410.0, 322.0, 728.0, 334.0]}, {"polygon": [[195.0, 335.0], [376.0, 333.0], [376.0, 342.0], [195.0, 343.0]], "confidence": 0.9565975666046143, "text": "us to water the plant until it starts draining through", "bbox": [195.0, 335.0, 376.0, 342.0]}, {"polygon": [[409.0, 335.0], [707.0, 335.0], [707.0, 349.0], [409.0, 349.0]], "confidence": 0.983429491519928, "text": "and Entertaining', 'Hobbies and Crafts' and 'Home and Garden'.", "bbox": [409.0, 335.0, 707.0, 349.0]}, {"polygon": [[82.0, 353.0], [115.0, 353.0], [115.0, 359.0], [82.0, 359.0]], "confidence": 0.7191088199615479, "text": "FISaR3ode8", "bbox": [82.0, 353.0, 115.0, 359.0]}, {"polygon": [[409.0, 381.0], [728.0, 381.0], [728.0, 396.0], [409.0, 396.0]], "confidence": 0.9834020137786865, "text": "architecture contains 39 transformer blocks with a consistent", "bbox": [409.0, 381.0, 728.0, 396.0]}, {"polygon": [[408.0, 396.0], [728.0, 396.0], [728.0, 410.0], [408.0, 410.0]], "confidence": 0.975393533706665, "text": "hidden dimension of 1408 channels and 16 heads.  (ii) The", "bbox": [408.0, 396.0, 728.0, 410.0]}, {"polygon": [[408.0, 412.0], [728.0, 412.0], [728.0, 426.0], [408.0, 426.0]], "confidence": 0.978874921798706, "text": "Q-former is from BLIP-2, which contains 12 transformer blocks", "bbox": [408.0, 412.0, 728.0, 426.0]}, {"polygon": [[148.0, 419.0], [274.0, 419.0], [274.0, 427.0], [148.0, 427.0]], "confidence": 0.9716131687164307, "text": "Kay stirs the soup and adds spinach", "bbox": [148.0, 419.0, 274.0, 427.0]}, {"polygon": [[409.0, 428.0], [729.0, 428.0], [729.0, 442.0], [409.0, 442.0]], "confidence": 0.9828174710273743, "text": "with 32 learnable queries and cross-attends visual features with a", "bbox": [409.0, 428.0, 729.0, 442.0]}, {"polygon": [[409.0, 443.0], [728.0, 443.0], [728.0, 457.0], [409.0, 457.0]], "confidence": 0.9733068346977234, "text": "dimension of (1+16 2 ) × 768. For each image input, it produces", "bbox": [409.0, 443.0, 728.0, 457.0]}, {"polygon": [[63.0, 445.0], [382.0, 445.0], [382.0, 459.0], [63.0, 459.0]], "confidence": 0.9782018065452576, "text": "Figure A.2. More samples from HowTo-AD dataset. Four examples", "bbox": [63.0, 445.0, 382.0, 459.0]}, {"polygon": [[409.0, 458.0], [728.0, 458.0], [728.0, 473.0], [409.0, 473.0]], "confidence": 0.9829779267311096, "text": "features with a dimension of 32 × 768. (iii) The video Q-Former", "bbox": [409.0, 458.0, 728.0, 473.0]}, {"polygon": [[63.0, 460.0], [382.0, 460.0], [382.0, 473.0], [63.0, 473.0]], "confidence": 0.9851349592208862, "text": "are shown in this figure with the 'character bank' shown on the left", "bbox": [63.0, 460.0, 382.0, 473.0]}, {"polygon": [[408.0, 474.0], [728.0, 474.0], [728.0, 488.0], [408.0, 488.0]], "confidence": 0.9822320342063904, "text": "is from Video-Llama, which also contains 12 transformer blocks", "bbox": [408.0, 474.0, 728.0, 488.0]}, {"polygon": [[64.0, 476.0], [382.0, 476.0], [382.0, 488.0], [64.0, 488.0]], "confidence": 0.9813395142555237, "text": "and the re-written video caption with randomly sampled character", "bbox": [64.0, 476.0, 382.0, 488.0]}, {"polygon": [[63.0, 489.0], [383.0, 489.0], [383.0, 502.0], [63.0, 502.0]], "confidence": 0.9839810729026794, "text": "names shown on the right. The YouTube IDs are displayed on top of", "bbox": [63.0, 489.0, 383.0, 502.0]}, {"polygon": [[409.0, 489.0], [728.0, 489.0], [728.0, 503.0], [409.0, 503.0]], "confidence": 0.9827661514282227, "text": "with 32 learnable queries. It cross-attends video features with a", "bbox": [409.0, 489.0, 728.0, 503.0]}, {"polygon": [[63.0, 503.0], [155.0, 503.0], [155.0, 516.0], [63.0, 516.0]], "confidence": 0.949282705783844, "text": "the character banks.", "bbox": [63.0, 503.0, 155.0, 516.0]}, {"polygon": [[409.0, 505.0], [729.0, 505.0], [729.0, 519.0], [409.0, 519.0]], "confidence": 0.9821490049362183, "text": "dimension of 8 × 32 × 768 and produces 32 × 768 video features.", "bbox": [409.0, 505.0, 729.0, 519.0]}, {"polygon": [[408.0, 520.0], [728.0, 520.0], [728.0, 534.0], [408.0, 534.0]], "confidence": 0.9841328859329224, "text": "(iv) The projector is a linear layer that projects 32 × 768 visual", "bbox": [408.0, 520.0, 728.0, 534.0]}, {"polygon": [[409.0, 535.0], [729.0, 535.0], [729.0, 550.0], [409.0, 550.0]], "confidence": 0.981910228729248, "text": "features into 32 × 4096 dimension to fit the language model.", "bbox": [409.0, 535.0, 729.0, 550.0]}, {"polygon": [[63.0, 546.0], [384.0, 546.0], [384.0, 561.0], [63.0, 561.0]], "confidence": 0.9800745844841003, "text": "from 1.2M to 180k, from which we build the HowTo-AD", "bbox": [63.0, 546.0, 384.0, 561.0]}, {"polygon": [[409.0, 551.0], [729.0, 551.0], [729.0, 565.0], [409.0, 565.0]], "confidence": 0.9836533665657043, "text": "We have two projectors for video and image inputs respectively.", "bbox": [409.0, 551.0, 729.0, 565.0]}, {"polygon": [[63.0, 563.0], [320.0, 563.0], [320.0, 577.0], [63.0, 577.0]], "confidence": 0.9794787764549255, "text": "dataset as described in the main paper Section 3.2.", "bbox": [63.0, 563.0, 320.0, 577.0]}, {"polygon": [[408.0, 566.0], [728.0, 566.0], [728.0, 580.0], [408.0, 580.0]], "confidence": 0.9808493256568909, "text": "(v) The language model is Llama2-7B, which contains 32", "bbox": [408.0, 566.0, 728.0, 580.0]}, {"polygon": [[80.0, 579.0], [383.0, 579.0], [383.0, 593.0], [80.0, 593.0]], "confidence": 0.9703092575073242, "text": "In Figure A.2 , we show a few more examples from", "bbox": [80.0, 579.0, 383.0, 593.0]}, {"polygon": [[409.0, 582.0], [660.0, 582.0], [660.0, 596.0], [409.0, 596.0]], "confidence": 0.9781370162963867, "text": "transformer blocks with 4096 hidden dimensions.", "bbox": [409.0, 582.0, 660.0, 596.0]}, {"polygon": [[63.0, 594.0], [382.0, 594.0], [382.0, 608.0], [63.0, 608.0]], "confidence": 0.9751575589179993, "text": "HowTo-AD dataset.  We notice a few limitations of the", "bbox": [63.0, 594.0, 382.0, 608.0]}, {"polygon": [[425.0, 599.0], [729.0, 599.0], [729.0, 614.0], [425.0, 614.0]], "confidence": 0.9810692071914673, "text": "By default, we only train the parameters in the video", "bbox": [425.0, 599.0, 729.0, 614.0]}, {"polygon": [[63.0, 609.0], [383.0, 609.0], [383.0, 624.0], [63.0, 624.0]], "confidence": 0.981964111328125, "text": "HowTo-AD: (1) inevitably, the video categories are dominated", "bbox": [63.0, 609.0, 383.0, 624.0]}, {"polygon": [[408.0, 615.0], [730.0, 615.0], [730.0, 629.0], [408.0, 629.0]], "confidence": 0.9805381894111633, "text": "Q-Former described in (iii) and the projector described in (iv).", "bbox": [408.0, 615.0, 730.0, 629.0]}, {"polygon": [[63.0, 624.0], [384.0, 624.0], [384.0, 639.0], [63.0, 639.0]], "confidence": 0.9837616682052612, "text": "by 'Food and Entertaining', in other words, cooking activities.", "bbox": [63.0, 624.0, 384.0, 639.0]}, {"polygon": [[410.0, 630.0], [728.0, 630.0], [728.0, 645.0], [410.0, 645.0]], "confidence": 0.9818267226219177, "text": "The rest of the components are initialized from Video-Llama", "bbox": [410.0, 630.0, 728.0, 645.0]}, {"polygon": [[63.0, 641.0], [383.0, 641.0], [383.0, 655.0], [63.0, 655.0]], "confidence": 0.9817814230918884, "text": "Since the dataset originates from HowTo100M where 50% of", "bbox": [63.0, 641.0, 383.0, 655.0]}, {"polygon": [[409.0, 645.0], [728.0, 645.0], [728.0, 660.0], [409.0, 660.0]], "confidence": 0.9783490300178528, "text": "pretrained weights.  We also experimented with a setting", "bbox": [409.0, 645.0, 728.0, 660.0]}, {"polygon": [[63.0, 656.0], [384.0, 656.0], [384.0, 670.0], [63.0, 670.0]], "confidence": 0.9813094735145569, "text": "the categories belongs to cooking activities. A breakdown of cat-", "bbox": [63.0, 656.0, 384.0, 670.0]}, {"polygon": [[409.0, 660.0], [728.0, 660.0], [728.0, 674.0], [409.0, 674.0]], "confidence": 0.9840197563171387, "text": "that only trains the linear projector (iv), it produces a lower", "bbox": [409.0, 660.0, 728.0, 674.0]}, {"polygon": [[63.0, 672.0], [382.0, 672.0], [382.0, 686.0], [63.0, 686.0]], "confidence": 0.9754153490066528, "text": "egory statistics is shown in Figure A.3 . (2) our current pipeline", "bbox": [63.0, 672.0, 382.0, 686.0]}, {"polygon": [[409.0, 675.0], [589.0, 675.0], [589.0, 690.0], [409.0, 690.0]], "confidence": 0.9688870906829834, "text": "performance (CIDEr drops by 1.5).", "bbox": [409.0, 675.0, 589.0, 690.0]}, {"polygon": [[63.0, 687.0], [382.0, 687.0], [382.0, 702.0], [63.0, 702.0]], "confidence": 0.9807382225990295, "text": "that transforms HowToCaption into HowTo-AD assumes the", "bbox": [63.0, 687.0, 382.0, 702.0]}, {"polygon": [[63.0, 704.0], [383.0, 704.0], [383.0, 717.0], [63.0, 717.0]], "confidence": 0.984042227268219, "text": "video only has a single character. However, it's common that in", "bbox": [63.0, 704.0, 383.0, 717.0]}, {"polygon": [[63.0, 718.0], [384.0, 718.0], [384.0, 733.0], [63.0, 733.0]], "confidence": 0.9849461913108826, "text": "instructional videos, two or three instructors appear in the scene.", "bbox": [63.0, 718.0, 384.0, 733.0]}, {"polygon": [[408.0, 718.0], [729.0, 718.0], [729.0, 733.0], [408.0, 733.0]], "confidence": 0.9739483594894409, "text": "Movie-BLIP2 is based on the Video-BLIP [ 72 ] repository 7 .", "bbox": [408.0, 718.0, 729.0, 733.0]}, {"polygon": [[63.0, 733.0], [383.0, 733.0], [383.0, 748.0], [63.0, 748.0]], "confidence": 0.9806759357452393, "text": "The single-character assumption may introduce noisy AD in the", "bbox": [63.0, 733.0, 383.0, 748.0]}, {"polygon": [[409.0, 735.0], [728.0, 735.0], [728.0, 748.0], [409.0, 748.0]], "confidence": 0.9796501994132996, "text": "With an input of 8 movie frames, the architecture is mostly", "bbox": [409.0, 735.0, 728.0, 748.0]}, {"polygon": [[63.0, 749.0], [384.0, 749.0], [384.0, 763.0], [63.0, 763.0]], "confidence": 0.9831674098968506, "text": "HowTo-AD dataset, and we consider fixing this in future work.", "bbox": [63.0, 749.0, 384.0, 763.0]}, {"polygon": [[408.0, 750.0], [729.0, 750.0], [729.0, 764.0], [408.0, 764.0]], "confidence": 0.9773670434951782, "text": "the same as Video-Llama, except for two differences.  First,", "bbox": [408.0, 750.0, 729.0, 764.0]}, {"polygon": [[409.0, 766.0], [729.0, 766.0], [729.0, 780.0], [409.0, 780.0]], "confidence": 0.9823580980300903, "text": "the functions of the Q-former and video Q-Former are merged.", "bbox": [409.0, 766.0, 729.0, 780.0]}, {"polygon": [[63.0, 780.0], [237.0, 780.0], [237.0, 797.0], [63.0, 797.0]], "confidence": 0.959743857383728, "text": "B. Implementation Details", "bbox": [63.0, 780.0, 237.0, 797.0]}, {"polygon": [[409.0, 781.0], [728.0, 781.0], [728.0, 795.0], [409.0, 795.0]], "confidence": 0.9792110323905945, "text": "In Movie-BLIP2, the Q-former cross-attends to all the visual", "bbox": [409.0, 781.0, 728.0, 795.0]}, {"polygon": [[409.0, 796.0], [729.0, 796.0], [729.0, 811.0], [409.0, 811.0]], "confidence": 0.9719604253768921, "text": "features at once with a dimension of 8 × (1+16 2 ) × 1408, then", "bbox": [409.0, 796.0, 729.0, 811.0]}, {"polygon": [[63.0, 806.0], [180.0, 806.0], [180.0, 822.0], [63.0, 822.0]], "confidence": 0.9435866475105286, "text": "B.1. Architectures", "bbox": [63.0, 806.0, 180.0, 822.0]}, {"polygon": [[409.0, 813.0], [728.0, 813.0], [728.0, 826.0], [409.0, 826.0]], "confidence": 0.9792130589485168, "text": "produces video features 32 × 768. There is no video Q-Former in", "bbox": [409.0, 813.0, 728.0, 826.0]}, {"polygon": [[409.0, 828.0], [728.0, 828.0], [728.0, 842.0], [409.0, 842.0]], "confidence": 0.9805806875228882, "text": "Movie-BLIP2. Second, the language model is OPT-2.7B, which", "bbox": [409.0, 828.0, 728.0, 842.0]}, {"polygon": [[63.0, 831.0], [384.0, 831.0], [384.0, 846.0], [63.0, 846.0]], "confidence": 0.975548505783081, "text": "Movie-Llama2 is based on the visual branch of the Video-", "bbox": [63.0, 831.0, 384.0, 846.0]}, {"polygon": [[409.0, 843.0], [722.0, 843.0], [722.0, 858.0], [409.0, 858.0]], "confidence": 0.9826908707618713, "text": "contains 32 transformer blocks with 2560 hidden dimensions.", "bbox": [409.0, 843.0, 722.0, 858.0]}, {"polygon": [[63.0, 848.0], [383.0, 848.0], [383.0, 861.0], [63.0, 861.0]], "confidence": 0.9741688966751099, "text": "Llama [ 77 ] repository 6 .  The input to the model is 8 movie", "bbox": [63.0, 848.0, 383.0, 861.0]}, {"polygon": [[424.0, 859.0], [729.0, 859.0], [729.0, 874.0], [424.0, 874.0]], "confidence": 0.9811517596244812, "text": "Similarly, we train the parameters in the Q-former and", "bbox": [424.0, 859.0, 729.0, 874.0]}, {"polygon": [[63.0, 863.0], [384.0, 863.0], [384.0, 878.0], [63.0, 878.0]], "confidence": 0.9815959334373474, "text": "frames uniformly sampled from a given AD time segment.", "bbox": [63.0, 863.0, 384.0, 878.0]}, {"polygon": [[409.0, 875.0], [728.0, 875.0], [728.0, 889.0], [409.0, 889.0]], "confidence": 0.9834104776382446, "text": "the projector. The rest of the components are initialized from", "bbox": [409.0, 875.0, 728.0, 889.0]}, {"polygon": [[63.0, 879.0], [384.0, 879.0], [384.0, 893.0], [63.0, 893.0]], "confidence": 0.974521815776825, "text": "The model contains five components, as detailed below.  (i)", "bbox": [63.0, 879.0, 384.0, 893.0]}, {"polygon": [[409.0, 891.0], [728.0, 891.0], [728.0, 905.0], [409.0, 905.0]], "confidence": 0.9834914207458496, "text": "Movie-BLIP pretrained weights. But only training the projector", "bbox": [409.0, 891.0, 728.0, 905.0]}, {"polygon": [[63.0, 894.0], [383.0, 894.0], [383.0, 909.0], [63.0, 909.0]], "confidence": 0.9806411266326904, "text": "The visual encoder is from EVA-CLIP which takes input", "bbox": [63.0, 894.0, 383.0, 909.0]}, {"polygon": [[409.0, 906.0], [573.0, 906.0], [573.0, 920.0], [409.0, 920.0]], "confidence": 0.9680579900741577, "text": "produces a similar performance.", "bbox": [409.0, 906.0, 573.0, 920.0]}, {"polygon": [[63.0, 909.0], [382.0, 909.0], [382.0, 924.0], [63.0, 924.0]], "confidence": 0.980207085609436, "text": "from 224 × 224 × 3 pixels and uses 14 × 14 patch size.  The", "bbox": [63.0, 909.0, 382.0, 924.0]}, {"polygon": [[81.0, 938.0], [345.0, 938.0], [345.0, 951.0], [81.0, 951.0]], "confidence": 0.9695398211479187, "text": "6https://github.com/DAMO-NLP-SG/Video-LLaMA", "bbox": [81.0, 938.0, 345.0, 951.0]}, {"polygon": [[427.0, 938.0], [650.0, 938.0], [650.0, 951.0], [427.0, 951.0]], "confidence": 0.9547824859619141, "text": "7https://github.com/yukw777/VideoBLIP", "bbox": [427.0, 938.0, 650.0, 951.0]}, {"polygon": [[389.0, 975.0], [404.0, 975.0], [404.0, 990.0], [389.0, 990.0]], "confidence": 0.45233258605003357, "text": "1313", "bbox": [389.0, 975.0, 404.0, 990.0]}], "languages": ["en"], "image_bbox": [0.0, 0.0, 816.0, 1056.0], "page": 13}, {"text_lines": [{"polygon": [[99.0, 95.0], [197.0, 95.0], [197.0, 108.0], [99.0, 108.0]], "confidence": 0.9505009651184082, "text": "AudioVault version 1", "bbox": [99.0, 95.0, 197.0, 108.0]}, {"polygon": [[258.0, 95.0], [355.0, 95.0], [355.0, 108.0], [258.0, 108.0]], "confidence": 0.9502841234207153, "text": "AudioVault version 2", "bbox": [258.0, 95.0, 355.0, 108.0]}, {"polygon": [[71.0, 113.0], [225.0, 113.0], [225.0, 124.0], [71.0, 124.0]], "confidence": 0.9578768014907837, "text": "● Donny turns to glare at the back of", "bbox": [71.0, 113.0, 225.0, 124.0]}, {"polygon": [[232.0, 113.0], [367.0, 113.0], [367.0, 124.0], [232.0, 124.0]], "confidence": 0.9334966540336609, "text": "• ", "bbox": [232.0, 113.0, 367.0, 124.0]}, {"polygon": [[78.0, 126.0], [155.0, 126.0], [155.0, 137.0], [78.0, 137.0]], "confidence": 0.947117030620575, "text": "his mother's chair.", "bbox": [78.0, 126.0, 155.0, 137.0]}, {"polygon": [[241.0, 126.0], [365.0, 126.0], [365.0, 137.0], [241.0, 137.0]], "confidence": 0.964833676815033, "text": "the back of his mother's chair.", "bbox": [241.0, 126.0, 365.0, 137.0]}, {"polygon": [[233.0, 138.0], [320.0, 138.0], [320.0, 149.0], [233.0, 149.0]], "confidence": 0.9177041053771973, "text": ". He paces the room,", "bbox": [233.0, 138.0, 320.0, 149.0]}, {"polygon": [[71.0, 139.0], [158.0, 139.0], [158.0, 149.0], [71.0, 149.0]], "confidence": 0.9203768968582153, "text": "● He paces the room,", "bbox": [71.0, 139.0, 158.0, 149.0]}, {"polygon": [[71.0, 150.0], [218.0, 150.0], [218.0, 161.0], [71.0, 161.0]], "confidence": 0.9519554376602173, "text": "● Donnie marches purposefully out", "bbox": [71.0, 150.0, 218.0, 161.0]}, {"polygon": [[233.0, 150.0], [363.0, 150.0], [363.0, 161.0], [233.0, 161.0]], "confidence": 0.9467712044715881, "text": "● Donnie marches purposefully", "bbox": [233.0, 150.0, 363.0, 161.0]}, {"polygon": [[79.0, 162.0], [131.0, 162.0], [131.0, 173.0], [79.0, 173.0]], "confidence": 0.9173545241355896, "text": "of the room.", "bbox": [79.0, 162.0, 131.0, 173.0]}, {"polygon": [[242.0, 162.0], [308.0, 162.0], [308.0, 173.0], [242.0, 173.0]], "confidence": 0.9382345676422119, "text": "out of the room.", "bbox": [242.0, 162.0, 308.0, 173.0]}, {"polygon": [[72.0, 192.0], [220.0, 192.0], [220.0, 202.0], [72.0, 202.0]], "confidence": 0.9535335302352905, "text": "● He picks up the receiver and looks at", "bbox": [72.0, 192.0, 220.0, 202.0]}, {"polygon": [[233.0, 193.0], [354.0, 193.0], [354.0, 202.0], [233.0, 202.0]], "confidence": 0.9469460844993591, "text": "● He picks up thea receiver and", "bbox": [233.0, 193.0, 354.0, 202.0]}, {"polygon": [[79.0, 205.0], [180.0, 205.0], [180.0, 215.0], [79.0, 215.0]], "confidence": 0.9597927927970886, "text": "his New York calling card.", "bbox": [79.0, 205.0, 180.0, 215.0]}, {"polygon": [[244.0, 205.0], [371.0, 205.0], [371.0, 213.0], [244.0, 213.0]], "confidence": 0.9707595705986023, "text": "looks at his New York calling card", "bbox": [244.0, 205.0, 371.0, 213.0]}, {"polygon": [[240.0, 216.0], [357.0, 216.0], [357.0, 227.0], [240.0, 227.0]], "confidence": 0.9635405540466309, "text": "He sits up and pulls the grouping", "bbox": [240.0, 216.0, 357.0, 227.0]}, {"polygon": [[72.0, 222.0], [214.0, 223.0], [214.0, 234.0], [72.0, 233.0]], "confidence": 0.9562776684761047, "text": ". He pulls the grouping of four chairs.", "bbox": [72.0, 222.0, 214.0, 234.0]}, {"polygon": [[80.0, 235.0], [204.0, 235.0], [204.0, 244.0], [80.0, 244.0]], "confidence": 0.9705290794372559, "text": "closer and props his feet on them.", "bbox": [80.0, 235.0, 204.0, 244.0]}, {"polygon": [[244.0, 249.0], [282.0, 249.0], [282.0, 257.0], [244.0, 257.0]], "confidence": 0.9088495373725891, "text": "to himself.", "bbox": [244.0, 249.0, 282.0, 257.0]}, {"polygon": [[72.0, 252.0], [217.0, 252.0], [217.0, 263.0], [72.0, 263.0]], "confidence": 0.9521687626838684, "text": ". The officer checks the form and sets", "bbox": [72.0, 252.0, 217.0, 263.0]}, {"polygon": [[243.0, 261.0], [363.0, 261.0], [363.0, 269.0], [243.0, 269.0]], "confidence": 0.9492719769477844, "text": "The officer eheeksglances at the", "bbox": [243.0, 261.0, 363.0, 269.0]}, {"polygon": [[80.0, 265.0], [109.0, 265.0], [109.0, 276.0], [80.0, 276.0]], "confidence": 0.8924862742424011, "text": "it aside.", "bbox": [80.0, 265.0, 109.0, 276.0]}, {"polygon": [[242.0, 271.0], [322.0, 271.0], [322.0, 279.0], [242.0, 279.0]], "confidence": 0.9374613165855408, "text": "form and sets it aside.", "bbox": [242.0, 271.0, 322.0, 279.0]}, {"polygon": [[64.0, 303.0], [382.0, 303.0], [382.0, 315.0], [64.0, 315.0]], "confidence": 0.9791516065597534, "text": "Figure A.4. Special cases of inter-rater analysis: Two versions of", "bbox": [64.0, 303.0, 382.0, 315.0]}, {"polygon": [[64.0, 317.0], [382.0, 317.0], [382.0, 329.0], [64.0, 329.0]], "confidence": 0.9837598204612732, "text": "AD from AudioVault might not be produced independently. In these", "bbox": [64.0, 317.0, 382.0, 329.0]}, {"polygon": [[63.0, 332.0], [384.0, 332.0], [384.0, 344.0], [63.0, 344.0]], "confidence": 0.9849864840507507, "text": "two examples, annotators edit a few words from the earlier version,", "bbox": [63.0, 332.0, 384.0, 344.0]}, {"polygon": [[63.0, 345.0], [383.0, 345.0], [383.0, 358.0], [63.0, 358.0]], "confidence": 0.9774138927459717, "text": "to produce a new version of AD. We discard these samples when", "bbox": [63.0, 345.0, 383.0, 358.0]}, {"polygon": [[63.0, 359.0], [382.0, 359.0], [382.0, 374.0], [63.0, 374.0]], "confidence": 0.9807795882225037, "text": "conducting inter-rater analysis since these examples do not reflect real", "bbox": [63.0, 359.0, 382.0, 374.0]}, {"polygon": [[64.0, 374.0], [220.0, 374.0], [220.0, 388.0], [64.0, 388.0]], "confidence": 0.964432418346405, "text": "human-human agreement on AD.", "bbox": [64.0, 374.0, 220.0, 388.0]}, {"polygon": [[228.0, 405.0], [291.0, 405.0], [291.0, 412.0], [228.0, 412.0]], "confidence": 0.9169222116470337, "text": "Movie-Llama2 predic", "bbox": [228.0, 405.0, 291.0, 412.0]}, {"polygon": [[114.0, 406.0], [162.0, 406.0], [162.0, 412.0], [114.0, 412.0]], "confidence": 0.8968884348869324, "text": "AndioVault anno", "bbox": [114.0, 406.0, 162.0, 412.0]}, {"polygon": [[102.0, 418.0], [192.0, 418.0], [192.0, 426.0], [102.0, 426.0]], "confidence": 0.9150535464286804, "text": "nes follows her around a come", "bbox": [102.0, 418.0, 192.0, 426.0]}, {"polygon": [[227.0, 420.0], [282.0, 420.0], [282.0, 426.0], [227.0, 426.0]], "confidence": 0.9441993832588196, "text": "Holmes follows her", "bbox": [227.0, 420.0, 282.0, 426.0]}, {"polygon": [[465.0, 421.0], [606.0, 421.0], [606.0, 430.0], [465.0, 430.0]], "confidence": 0.9329227805137634, "text": "edge. Prediction : Garfield lands on the role", "bbox": [465.0, 421.0, 606.0, 430.0]}, {"polygon": [[106.0, 439.0], [185.0, 439.0], [185.0, 444.0], [106.0, 444.0]], "confidence": 0.9551929831504822, "text": "red hair under her black ver", "bbox": [106.0, 439.0, 185.0, 444.0]}, {"polygon": [[409.0, 447.0], [728.0, 447.0], [728.0, 460.0], [409.0, 460.0]], "confidence": 0.9816546440124512, "text": "Figure A.6. Additional qualitative results. For each movie clip, three", "bbox": [409.0, 447.0, 728.0, 460.0]}, {"polygon": [[112.0, 456.0], [202.0, 456.0], [202.0, 462.0], [112.0, 462.0]], "confidence": 0.9172749519348145, "text": "on is blurred, but he sees a mar", "bbox": [112.0, 456.0, 202.0, 462.0]}, {"polygon": [[235.0, 456.0], [297.0, 456.0], [297.0, 462.0], [235.0, 462.0]], "confidence": 0.8470170497894287, "text": "and twisted at an odd a", "bbox": [235.0, 456.0, 297.0, 462.0]}, {"polygon": [[409.0, 461.0], [728.0, 461.0], [728.0, 474.0], [409.0, 474.0]], "confidence": 0.9847283363342285, "text": "frames are uniformly extracted and visualized. The movie clips are", "bbox": [409.0, 461.0, 728.0, 474.0]}, {"polygon": [[114.0, 466.0], [180.0, 466.0], [180.0, 472.0], [114.0, 472.0]], "confidence": 0.954474687576294, "text": "ary up off the road and", "bbox": [114.0, 466.0, 180.0, 472.0]}, {"polygon": [[410.0, 476.0], [729.0, 476.0], [729.0, 489.0], [410.0, 489.0]], "confidence": 0.9787504076957703, "text": "from Inferno (2016, tt3062096 ), Ice Age (2002, tt0268380 ),", "bbox": [410.0, 476.0, 729.0, 489.0]}, {"polygon": [[409.0, 490.0], [729.0, 490.0], [729.0, 503.0], [409.0, 503.0]], "confidence": 0.9804361462593079, "text": "Sex and the City 2 (2010, tt1261945 ) and Garfield (2004,", "bbox": [409.0, 490.0, 729.0, 503.0]}, {"polygon": [[86.0, 497.0], [206.0, 497.0], [206.0, 504.0], [86.0, 504.0]], "confidence": 0.9736208915710449, "text": "Marty's speedometer tops at a stop sign,", "bbox": [86.0, 497.0, 206.0, 504.0]}, {"polygon": [[409.0, 504.0], [729.0, 504.0], [729.0, 517.0], [409.0, 517.0]], "confidence": 0.9810696840286255, "text": "t0356634 ). For qualitative results in video format, please refer to", "bbox": [409.0, 504.0, 729.0, 517.0]}, {"polygon": [[90.0, 516.0], [150.0, 516.0], [150.0, 521.0], [90.0, 521.0]], "confidence": 0.9301158785820007, "text": "Marty's radiation help", "bbox": [90.0, 516.0, 150.0, 521.0]}, {"polygon": [[409.0, 518.0], [616.0, 518.0], [616.0, 531.0], [409.0, 531.0]], "confidence": 0.9688345193862915, "text": "the mp4 file in our supplementary material.", "bbox": [409.0, 518.0, 616.0, 531.0]}, {"polygon": [[87.0, 534.0], [196.0, 534.0], [196.0, 539.0], [87.0, 539.0]], "confidence": 0.9692367315292358, "text": "George rides through the square on a", "bbox": [87.0, 534.0, 196.0, 539.0]}, {"polygon": [[225.0, 534.0], [303.0, 534.0], [303.0, 539.0], [225.0, 539.0]], "confidence": 0.9594270586967468, "text": "Marty rides his bike out of", "bbox": [225.0, 534.0, 303.0, 539.0]}, {"polygon": [[89.0, 551.0], [114.0, 551.0], [114.0, 557.0], [89.0, 557.0]], "confidence": 0.7756078243255615, "text": "ater him", "bbox": [89.0, 551.0, 114.0, 557.0]}, {"polygon": [[409.0, 559.0], [728.0, 559.0], [728.0, 574.0], [409.0, 574.0]], "confidence": 0.9836933612823486, "text": "annotations for each movie and discard those movies with more", "bbox": [409.0, 559.0, 728.0, 574.0]}, {"polygon": [[408.0, 575.0], [728.0, 575.0], [728.0, 589.0], [408.0, 589.0]], "confidence": 0.9774027466773987, "text": "than one identical annotations.  It ends up filtering out 18%", "bbox": [408.0, 575.0, 728.0, 589.0]}, {"polygon": [[63.0, 577.0], [383.0, 577.0], [383.0, 591.0], [63.0, 591.0]], "confidence": 0.9763901233673096, "text": "Figure A.5. Examples of CRITIC and LLM-AD-Eval metrics. We", "bbox": [63.0, 577.0, 383.0, 591.0]}, {"polygon": [[409.0, 591.0], [729.0, 591.0], [729.0, 605.0], [409.0, 605.0]], "confidence": 0.9816171526908875, "text": "from 402 movies, and the remaining trustworthy independent", "bbox": [409.0, 591.0, 729.0, 605.0]}, {"polygon": [[63.0, 593.0], [383.0, 593.0], [383.0, 605.0], [63.0, 605.0]], "confidence": 0.983231246471405, "text": "show seven AD annotation-prediction pairs from two movies. For both", "bbox": [63.0, 593.0, 383.0, 605.0]}, {"polygon": [[409.0, 606.0], [652.0, 606.0], [652.0, 621.0], [409.0, 621.0]], "confidence": 0.9785734415054321, "text": "AD annotations are used for inter-rater analysis.", "bbox": [409.0, 606.0, 652.0, 621.0]}, {"polygon": [[63.0, 607.0], [382.0, 607.0], [382.0, 619.0], [63.0, 619.0]], "confidence": 0.9829623699188232, "text": "CRITIC and LLM-AD-Eval metrics, higher values indicate higher", "bbox": [63.0, 607.0, 382.0, 619.0]}, {"polygon": [[63.0, 620.0], [382.0, 620.0], [382.0, 633.0], [63.0, 633.0]], "confidence": 0.9854387640953064, "text": "quality. For details of metrics please refer to the main paper Section 5.", "bbox": [63.0, 620.0, 382.0, 633.0]}, {"polygon": [[409.0, 630.0], [657.0, 630.0], [657.0, 646.0], [409.0, 646.0]], "confidence": 0.9730038642883301, "text": "C.2. Analysis of the Evaluation Metrics", "bbox": [409.0, 630.0, 657.0, 646.0]}, {"polygon": [[409.0, 655.0], [730.0, 655.0], [730.0, 669.0], [409.0, 669.0]], "confidence": 0.9817814230918884, "text": "A few examples of AD annotations and (both good and bad)", "bbox": [409.0, 655.0, 730.0, 669.0]}, {"polygon": [[63.0, 660.0], [190.0, 660.0], [190.0, 678.0], [63.0, 678.0]], "confidence": 0.9323227405548096, "text": "B.2. LLM-AD-Eval", "bbox": [63.0, 660.0, 190.0, 678.0]}, {"polygon": [[409.0, 671.0], [729.0, 671.0], [729.0, 685.0], [409.0, 685.0]], "confidence": 0.9725876450538635, "text": "model predictions are shown in Figure A.5 , we show both", "bbox": [409.0, 671.0, 729.0, 685.0]}, {"polygon": [[63.0, 686.0], [384.0, 686.0], [384.0, 701.0], [63.0, 701.0]], "confidence": 0.9736636877059937, "text": "We use the gpt-3.5-turbo model from OpenAI to", "bbox": [63.0, 686.0, 384.0, 701.0]}, {"polygon": [[408.0, 686.0], [729.0, 686.0], [729.0, 700.0], [408.0, 700.0]], "confidence": 0.9819857478141785, "text": "CRITIC and LLM-AD-Eval metrics for each pair. Qualitatively,", "bbox": [408.0, 686.0, 729.0, 700.0]}, {"polygon": [[63.0, 702.0], [383.0, 702.0], [383.0, 716.0], [63.0, 716.0]], "confidence": 0.9811223149299622, "text": "construct the LLM-AD-Eval metric. Our customized prompt", "bbox": [63.0, 702.0, 383.0, 716.0]}, {"polygon": [[409.0, 702.0], [728.0, 702.0], [728.0, 716.0], [409.0, 716.0]], "confidence": 0.9814798831939697, "text": "we can feel that LLM-AD-Eval correlates to human judgement", "bbox": [409.0, 702.0, 728.0, 716.0]}, {"polygon": [[63.0, 718.0], [382.0, 718.0], [382.0, 732.0], [63.0, 732.0]], "confidence": 0.9738540053367615, "text": "is shown in Algorithm 2 .  The LLM is instructed to evaluate", "bbox": [63.0, 718.0, 382.0, 732.0]}, {"polygon": [[408.0, 718.0], [727.0, 718.0], [727.0, 731.0], [408.0, 731.0]], "confidence": 0.9747555255889893, "text": "of semantic matching between two sentences. The CRITIC", "bbox": [408.0, 718.0, 727.0, 731.0]}, {"polygon": [[63.0, 733.0], [383.0, 733.0], [383.0, 748.0], [63.0, 748.0]], "confidence": 0.9822421669960022, "text": "the level of match between two sentences and return a score", "bbox": [63.0, 733.0, 383.0, 748.0]}, {"polygon": [[408.0, 733.0], [728.0, 733.0], [728.0, 748.0], [408.0, 748.0]], "confidence": 0.982586681842804, "text": "metric gives zeros when the predictions do not contain the", "bbox": [408.0, 733.0, 728.0, 748.0]}, {"polygon": [[63.0, 748.0], [144.0, 748.0], [144.0, 762.0], [63.0, 762.0]], "confidence": 0.9327396154403687, "text": "between 0 to 5.", "bbox": [63.0, 748.0, 144.0, 762.0]}, {"polygon": [[409.0, 748.0], [554.0, 748.0], [554.0, 762.0], [409.0, 762.0]], "confidence": 0.9662018418312073, "text": "characters in the annotation.", "bbox": [409.0, 748.0, 554.0, 762.0]}, {"polygon": [[409.0, 771.0], [707.0, 771.0], [707.0, 787.0], [409.0, 787.0]], "confidence": 0.9755908846855164, "text": "C.3. Better Character Exemplars for CMD-AD", "bbox": [409.0, 771.0, 707.0, 787.0]}, {"polygon": [[63.0, 777.0], [208.0, 777.0], [208.0, 795.0], [63.0, 795.0]], "confidence": 0.9522631168365479, "text": "C. Additional Results", "bbox": [63.0, 777.0, 208.0, 795.0]}, {"polygon": [[409.0, 797.0], [729.0, 797.0], [729.0, 812.0], [409.0, 812.0]], "confidence": 0.9821006059646606, "text": "In AutoAD-II [ 21 ], the character exemplars are constructed", "bbox": [409.0, 797.0, 729.0, 812.0]}, {"polygon": [[63.0, 802.0], [378.0, 802.0], [378.0, 819.0], [63.0, 819.0]], "confidence": 0.9788961410522461, "text": "C.1. Details of Inter-rater Analysis on AudioVault", "bbox": [63.0, 802.0, 378.0, 819.0]}, {"polygon": [[409.0, 813.0], [728.0, 813.0], [728.0, 826.0], [409.0, 826.0]], "confidence": 0.9818031787872314, "text": "from the in-context nearest neighbours of actors' IMDb profile", "bbox": [409.0, 813.0, 728.0, 826.0]}, {"polygon": [[63.0, 828.0], [382.0, 828.0], [382.0, 843.0], [63.0, 843.0]], "confidence": 0.9820435643196106, "text": "As described in the main paper Section 6.3, there are 402", "bbox": [63.0, 828.0, 382.0, 843.0]}, {"polygon": [[408.0, 828.0], [728.0, 828.0], [728.0, 843.0], [408.0, 843.0]], "confidence": 0.9832229614257812, "text": "pictures. However, we qualitatively find this method does not", "bbox": [408.0, 828.0, 728.0, 843.0]}, {"polygon": [[63.0, 843.0], [384.0, 843.0], [384.0, 858.0], [63.0, 858.0]], "confidence": 0.9769517779350281, "text": "movies with multiple versions of AD annotations. However,", "bbox": [63.0, 843.0, 384.0, 858.0]}, {"polygon": [[409.0, 843.0], [728.0, 843.0], [728.0, 858.0], [409.0, 858.0]], "confidence": 0.9816468358039856, "text": "work well for old movies where the IMDb profile pictures", "bbox": [409.0, 843.0, 728.0, 858.0]}, {"polygon": [[63.0, 858.0], [384.0, 858.0], [384.0, 872.0], [63.0, 872.0]], "confidence": 0.9822118282318115, "text": "we found in some cases two versions of AD annotations are not", "bbox": [63.0, 858.0, 384.0, 872.0]}, {"polygon": [[409.0, 859.0], [728.0, 859.0], [728.0, 873.0], [409.0, 873.0]], "confidence": 0.9816906452178955, "text": "are of low quality, or cartoon movies where the IMDb profile", "bbox": [409.0, 859.0, 728.0, 873.0]}, {"polygon": [[63.0, 874.0], [383.0, 874.0], [383.0, 888.0], [63.0, 888.0]], "confidence": 0.9754059314727783, "text": "produced independently , as shown in Figure A.4 – annotators", "bbox": [63.0, 874.0, 383.0, 888.0]}, {"polygon": [[409.0, 874.0], [678.0, 874.0], [678.0, 888.0], [409.0, 888.0]], "confidence": 0.9822204113006592, "text": "pictures are the voice actors instead of the characters.", "bbox": [409.0, 874.0, 678.0, 888.0]}, {"polygon": [[63.0, 890.0], [384.0, 890.0], [384.0, 903.0], [63.0, 903.0]], "confidence": 0.9842654466629028, "text": "slightly modify an older version of AD to produce a new version,", "bbox": [63.0, 890.0, 384.0, 903.0]}, {"polygon": [[425.0, 890.0], [729.0, 890.0], [729.0, 904.0], [425.0, 904.0]], "confidence": 0.9829639792442322, "text": "We investigated an additional source of character exemplars,", "bbox": [425.0, 890.0, 729.0, 904.0]}, {"polygon": [[63.0, 905.0], [383.0, 905.0], [383.0, 920.0], [63.0, 920.0]], "confidence": 0.9799110293388367, "text": "therefore these samples do not reflect the real human-human", "bbox": [63.0, 905.0, 383.0, 920.0]}, {"polygon": [[409.0, 905.0], [727.0, 905.0], [727.0, 920.0], [409.0, 920.0]], "confidence": 0.9808052182197571, "text": "which are from the 'photo' pages on IMDb, like https://", "bbox": [409.0, 905.0, 727.0, 920.0]}, {"polygon": [[63.0, 921.0], [382.0, 921.0], [382.0, 935.0], [63.0, 935.0]], "confidence": 0.9832656979560852, "text": "agreement on AD annotations. To discard these samples in our", "bbox": [63.0, 921.0, 382.0, 935.0]}, {"polygon": [[409.0, 922.0], [727.0, 922.0], [727.0, 935.0], [409.0, 935.0]], "confidence": 0.957335889339447, "text": "wwwwwwww . c", "bbox": [409.0, 922.0, 727.0, 935.0]}, {"polygon": [[63.0, 937.0], [382.0, 935.0], [382.0, 950.0], [63.0, 952.0]], "confidence": 0.9831283092498779, "text": "inter-rater analysis, we apply a filter to detect the identical AD", "bbox": [63.0, 937.0, 382.0, 950.0]}, {"polygon": [[410.0, 937.0], [727.0, 937.0], [727.0, 951.0], [410.0, 951.0]], "confidence": 0.9630383849143982, "text": "nm0068338 or https://www.imdb.com/title/", "bbox": [410.0, 937.0, 727.0, 951.0]}, {"polygon": [[389.0, 975.0], [404.0, 975.0], [404.0, 990.0], [389.0, 990.0]], "confidence": 0.6371278166770935, "text": "14", "bbox": [389.0, 975.0, 404.0, 990.0]}], "languages": ["en"], "image_bbox": [0.0, 0.0, 816.0, 1056.0], "page": 14}, {"text_lines": [{"polygon": [[94.0, 99.0], [138.0, 99.0], [138.0, 114.0], [94.0, 114.0]], "confidence": 0.8555694818496704, "text": "Method", "bbox": [94.0, 99.0, 138.0, 114.0]}, {"polygon": [[258.0, 99.0], [349.0, 99.0], [349.0, 114.0], [258.0, 114.0]], "confidence": 0.850490927696228, "text": "| CIDEr CRITIC", "bbox": [258.0, 99.0, 349.0, 114.0]}, {"polygon": [[178.0, 100.0], [218.0, 100.0], [218.0, 114.0], [178.0, 114.0]], "confidence": 0.8718283176422119, "text": "Setting", "bbox": [178.0, 100.0, 218.0, 114.0]}, {"polygon": [[94.0, 121.0], [262.0, 121.0], [262.0, 136.0], [94.0, 136.0]], "confidence": 0.9506888389587402, "text": "Movie-Llama2  new exemplars", "bbox": [94.0, 121.0, 262.0, 136.0]}, {"polygon": [[266.0, 121.0], [294.0, 121.0], [294.0, 136.0], [266.0, 136.0]], "confidence": 0.722006618976593, "text": "21.7", "bbox": [266.0, 121.0, 294.0, 136.0]}, {"polygon": [[313.0, 121.0], [339.0, 121.0], [339.0, 136.0], [313.0, 136.0]], "confidence": 0.7472525835037231, "text": "25.2", "bbox": [313.0, 121.0, 339.0, 136.0]}, {"polygon": [[95.0, 137.0], [255.0, 137.0], [255.0, 151.0], [95.0, 151.0]], "confidence": 0.9349215626716614, "text": "Movie-Llama2 old exemplars", "bbox": [95.0, 137.0, 255.0, 151.0]}, {"polygon": [[267.0, 137.0], [293.0, 137.0], [293.0, 150.0], [267.0, 150.0]], "confidence": 0.7686155438423157, "text": "20.1", "bbox": [267.0, 137.0, 293.0, 150.0]}, {"polygon": [[312.0, 137.0], [339.0, 137.0], [339.0, 150.0], [312.0, 150.0]], "confidence": 0.7739521265029907, "text": "23.3", "bbox": [312.0, 137.0, 339.0, 150.0]}, {"polygon": [[64.0, 161.0], [384.0, 161.0], [384.0, 175.0], [64.0, 175.0]], "confidence": 0.9754595756530762, "text": "Table A.1. Effect of new character exemplars on CMD-AD-Eval.", "bbox": [64.0, 161.0, 384.0, 175.0]}, {"polygon": [[63.0, 176.0], [362.0, 176.0], [362.0, 189.0], [63.0, 189.0]], "confidence": 0.9826854467391968, "text": "Both experiments are conducted without HowTo-AD pretraining.", "bbox": [63.0, 176.0, 362.0, 189.0]}, {"polygon": [[63.0, 212.0], [382.0, 212.0], [382.0, 227.0], [63.0, 227.0]], "confidence": 0.9759485721588135, "text": "tt0046912/characters/nm0001537 . These photos", "bbox": [63.0, 212.0, 382.0, 227.0]}, {"polygon": [[63.0, 226.0], [383.0, 226.0], [383.0, 242.0], [63.0, 242.0]], "confidence": 0.9835982918739319, "text": "are the movie frames uploaded by users, corresponding to spe-", "bbox": [63.0, 226.0, 383.0, 242.0]}, {"polygon": [[63.0, 242.0], [384.0, 242.0], [384.0, 256.0], [63.0, 256.0]], "confidence": 0.9838213920593262, "text": "cific characters. However, this source has two major issues: first,", "bbox": [63.0, 242.0, 384.0, 256.0]}, {"polygon": [[63.0, 257.0], [383.0, 257.0], [383.0, 272.0], [63.0, 272.0]], "confidence": 0.9826036691665649, "text": "the photos are biased towards famous characters, and less-known", "bbox": [63.0, 257.0, 383.0, 272.0]}, {"polygon": [[63.0, 273.0], [382.0, 273.0], [382.0, 287.0], [63.0, 287.0]], "confidence": 0.9838946461677551, "text": "characters usually do not have photos uploaded; second, photos", "bbox": [63.0, 273.0, 382.0, 287.0]}, {"polygon": [[63.0, 288.0], [383.0, 288.0], [383.0, 303.0], [63.0, 303.0]], "confidence": 0.9804229140281677, "text": "under one character may contain other characters. We find only", "bbox": [63.0, 288.0, 383.0, 303.0]}, {"polygon": [[63.0, 303.0], [383.0, 303.0], [383.0, 318.0], [63.0, 318.0]], "confidence": 0.9834717512130737, "text": "50% characters have valid photo pages, and we use face detec-", "bbox": [63.0, 303.0, 383.0, 318.0]}, {"polygon": [[63.0, 320.0], [383.0, 320.0], [383.0, 334.0], [63.0, 334.0]], "confidence": 0.9839754104614258, "text": "tor 8 to select photos with only one face; For those without photo", "bbox": [63.0, 320.0, 383.0, 334.0]}, {"polygon": [[63.0, 335.0], [383.0, 335.0], [383.0, 350.0], [63.0, 350.0]], "confidence": 0.9824392795562744, "text": "pages, we fall back to the original AutoAD-II strategy to find", "bbox": [63.0, 335.0, 383.0, 350.0]}, {"polygon": [[63.0, 351.0], [382.0, 351.0], [382.0, 365.0], [63.0, 365.0]], "confidence": 0.982291042804718, "text": "in-context exemplars. The main paper uses this new strategy", "bbox": [63.0, 351.0, 382.0, 365.0]}, {"polygon": [[63.0, 366.0], [382.0, 366.0], [382.0, 381.0], [63.0, 381.0]], "confidence": 0.983765184879303, "text": "for character exemplars for all the experiments. We ablate the", "bbox": [63.0, 366.0, 382.0, 381.0]}, {"polygon": [[63.0, 381.0], [369.0, 381.0], [369.0, 396.0], [63.0, 396.0]], "confidence": 0.976375162601471, "text": "effect of this update on the character exemplar in Table A.1 .", "bbox": [63.0, 381.0, 369.0, 396.0]}, {"polygon": [[63.0, 410.0], [282.0, 410.0], [282.0, 429.0], [63.0, 429.0]], "confidence": 0.9686989784240723, "text": "D. Additional Qualitative Results", "bbox": [63.0, 410.0, 282.0, 429.0]}, {"polygon": [[63.0, 437.0], [384.0, 437.0], [384.0, 453.0], [63.0, 453.0]], "confidence": 0.9720631241798401, "text": "More qualitative results are shown in Figure A.6. Additionally,", "bbox": [63.0, 437.0, 384.0, 453.0]}, {"polygon": [[63.0, 454.0], [382.0, 454.0], [382.0, 468.0], [63.0, 468.0]], "confidence": 0.9766504764556885, "text": "on the project page https://www.robots.ox.ac.uk/", "bbox": [63.0, 454.0, 382.0, 468.0]}, {"polygon": [[63.0, 469.0], [383.0, 469.0], [383.0, 483.0], [63.0, 483.0]], "confidence": 0.9803463816642761, "text": "vgg/research/autoad/ , we display a few visualization", "bbox": [63.0, 469.0, 383.0, 483.0]}, {"polygon": [[63.0, 484.0], [382.0, 484.0], [382.0, 498.0], [63.0, 498.0]], "confidence": 0.9799392819404602, "text": "of our model predictions as MP4 files. The AD ground-truth", "bbox": [63.0, 484.0, 382.0, 498.0]}, {"polygon": [[63.0, 499.0], [382.0, 499.0], [382.0, 514.0], [63.0, 514.0]], "confidence": 0.9834190607070923, "text": "and our model predictions are provided as subtitles with the", "bbox": [63.0, 499.0, 382.0, 514.0]}, {"polygon": [[63.0, 514.0], [382.0, 514.0], [382.0, 529.0], [63.0, 529.0]], "confidence": 0.9792711138725281, "text": "format '[title] ground-truth || prediction'. The voice of AD is", "bbox": [63.0, 514.0, 382.0, 529.0]}, {"polygon": [[63.0, 530.0], [382.0, 530.0], [382.0, 544.0], [63.0, 544.0]], "confidence": 0.9812609553337097, "text": "generated from OpenAI text-to-speech API 9 and fused with", "bbox": [63.0, 530.0, 382.0, 544.0]}, {"polygon": [[63.0, 545.0], [220.0, 545.0], [220.0, 560.0], [63.0, 560.0]], "confidence": 0.9669159650802612, "text": "the original movie soundtrack.", "bbox": [63.0, 545.0, 220.0, 560.0]}, {"polygon": [[79.0, 912.0], [357.0, 912.0], [357.0, 925.0], [79.0, 925.0]], "confidence": 0.9722958207130432, "text": "8https://github.com/ageitgey/face_recognition", "bbox": [79.0, 912.0, 357.0, 925.0]}, {"polygon": [[78.0, 923.0], [382.0, 926.0], [382.0, 942.0], [78.0, 938.0]], "confidence": 0.9748783707618713, "text": "9https://platform.openai.com/docs/guides/text-to-", "bbox": [78.0, 923.0, 382.0, 942.0]}, {"polygon": [[65.0, 938.0], [106.0, 938.0], [106.0, 951.0], [65.0, 951.0]], "confidence": 0.8384712934494019, "text": "speech", "bbox": [65.0, 938.0, 106.0, 951.0]}, {"polygon": [[388.0, 975.0], [404.0, 975.0], [404.0, 990.0], [388.0, 990.0]], "confidence": 0.4905546009540558, "text": "15", "bbox": [388.0, 975.0, 404.0, 990.0]}], "languages": ["en"], "image_bbox": [0.0, 0.0, 816.0, 1056.0], "page": 15}, {"text_lines": [{"polygon": [[69.0, 136.0], [352.0, 138.0], [352.0, 153.0], [69.0, 151.0]], "confidence": 0.9792437553405762, "text": "Algorithm 1: Python script for audio-audio alignment.", "bbox": [69.0, 136.0, 352.0, 153.0]}, {"polygon": [[81.0, 166.0], [159.0, 166.0], [159.0, 176.0], [81.0, 176.0]], "confidence": 0.9208896160125732, "text": "import torch", "bbox": [81.0, 166.0, 159.0, 176.0]}, {"polygon": [[82.0, 176.0], [191.0, 176.0], [191.0, 189.0], [82.0, 189.0]], "confidence": 0.9421731233596802, "text": "import torchaudio", "bbox": [82.0, 176.0, 191.0, 189.0]}, {"polygon": [[82.0, 187.0], [197.0, 187.0], [197.0, 198.0], [82.0, 198.0]], "confidence": 0.9453980326652527, "text": "import numpy as np", "bbox": [82.0, 187.0, 197.0, 198.0]}, {"polygon": [[81.0, 196.0], [384.0, 196.0], [384.0, 209.0], [81.0, 209.0]], "confidence": 0.9714112281799316, "text": "from sklearn.linear_model import RANSACRegressor", "bbox": [81.0, 196.0, 384.0, 209.0]}, {"polygon": [[81.0, 208.0], [371.0, 208.0], [371.0, 220.0], [81.0, 220.0]], "confidence": 0.9771142601966858, "text": "from sklearn.metrics import mean_squared_error", "bbox": [81.0, 208.0, 371.0, 220.0]}, {"polygon": [[80.0, 228.0], [428.0, 226.0], [428.0, 240.0], [80.0, 241.0]], "confidence": 0.9775564074516296, "text": "def aa_match(cmd_wav, audiovault_wav, audiovault_anno):", "bbox": [80.0, 228.0, 428.0, 240.0]}, {"polygon": [[99.0, 250.0], [268.0, 250.0], [268.0, 262.0], [99.0, 262.0]], "confidence": 0.952564001083374, "text": "cmd_wav: audio path of CMD", "bbox": [99.0, 250.0, 268.0, 262.0]}, {"polygon": [[100.0, 260.0], [397.0, 258.0], [397.0, 271.0], [100.0, 273.0]], "confidence": 0.9689866900444031, "text": "audiovault_wav: audio path of AudioVault chunk,", "bbox": [100.0, 260.0, 397.0, 271.0]}, {"polygon": [[101.0, 271.0], [561.0, 271.0], [561.0, 283.0], [101.0, 283.0]], "confidence": 0.9825252890586853, "text": "audiovault_anno: AD annotations (with timestamps) of the AudioVault chunk", "bbox": [101.0, 271.0, 561.0, 283.0]}, {"polygon": [[100.0, 293.0], [368.0, 292.0], [368.0, 304.0], [100.0, 305.0]], "confidence": 0.9758109450340271, "text": "Return: mse, slope and intercept of RANSAC", "bbox": [100.0, 293.0, 368.0, 304.0]}, {"polygon": [[101.0, 313.0], [291.0, 313.0], [291.0, 325.0], [101.0, 325.0]], "confidence": 0.9671379327774048, "text": "resolution_per_mel = 512/16000", "bbox": [101.0, 313.0, 291.0, 325.0]}, {"polygon": [[99.0, 334.0], [397.0, 334.0], [397.0, 346.0], [99.0, 346.0]], "confidence": 0.9784277081489563, "text": "cmd_waveform, cmd_sr = torchaudio.load(cmd_wav)", "bbox": [99.0, 334.0, 397.0, 346.0]}, {"polygon": [[99.0, 345.0], [372.0, 345.0], [372.0, 358.0], [99.0, 358.0]], "confidence": 0.9746293425559998, "text": "cmd_melspec = mel_spectrogram(cmd_waveform)", "bbox": [99.0, 345.0, 372.0, 358.0]}, {"polygon": [[99.0, 355.0], [516.0, 354.0], [516.0, 367.0], [99.0, 368.0]], "confidence": 0.9802690148353577, "text": "cmd_melspec = cmd_melspec / cmd_melspec.norm(dim=-2, keepdim=True)", "bbox": [99.0, 355.0, 516.0, 367.0]}, {"polygon": [[100.0, 376.0], [428.0, 376.0], [428.0, 388.0], [100.0, 388.0]], "confidence": 0.9788430333137512, "text": "av_waveform, av_sr = torchaudio.load(audiovault_wav)", "bbox": [100.0, 376.0, 428.0, 388.0]}, {"polygon": [[100.0, 388.0], [359.0, 388.0], [359.0, 399.0], [100.0, 399.0]], "confidence": 0.9741582274436951, "text": "av_melspec = mel_spectrogram(av_waveform)", "bbox": [100.0, 388.0, 359.0, 399.0]}, {"polygon": [[100.0, 398.0], [497.0, 398.0], [497.0, 410.0], [100.0, 410.0]], "confidence": 0.9777400493621826, "text": "av_melspec = av_melspec / av_melspec.norm(dim=-2, keepdim=True)", "bbox": [100.0, 398.0, 497.0, 410.0]}, {"polygon": [[101.0, 419.0], [499.0, 419.0], [499.0, 432.0], [101.0, 432.0]], "confidence": 0.9818276166915894, "text": "# mask melspec as zeros if AD appears. Only compute non-AD part", "bbox": [101.0, 419.0, 499.0, 432.0]}, {"polygon": [[100.0, 429.0], [372.0, 429.0], [372.0, 441.0], [100.0, 441.0]], "confidence": 0.9752309918403625, "text": "av_melspec_masked = torch.clone(av_melspec)", "bbox": [100.0, 429.0, 372.0, 441.0]}, {"polygon": [[100.0, 440.0], [237.0, 440.0], [237.0, 453.0], [100.0, 453.0]], "confidence": 0.9477170705795288, "text": "start_end_mel_idx = []", "bbox": [100.0, 440.0, 237.0, 453.0]}, {"polygon": [[101.0, 449.0], [362.0, 449.0], [362.0, 462.0], [101.0, 462.0]], "confidence": 0.9537559151649475, "text": "for _ , row in audiovault_anno.iterrows() :", "bbox": [101.0, 449.0, 362.0, 462.0]}, {"polygon": [[119.0, 459.0], [588.0, 459.0], [588.0, 484.0], [119.0, 484.0]], "confidence": 0.9842550158500671, "text": "start_mel_idx = int(row['start'] / resolution_per_mel) # second to mel_idx\nend_mel_idx = int(row['end'] / resolution_per_mel)", "bbox": [119.0, 459.0, 588.0, 484.0]}, {"polygon": [[120.0, 482.0], [448.0, 480.0], [448.0, 494.0], [120.0, 496.0]], "confidence": 0.977310836315155, "text": "av_melspec_masked[...,start_mel_idx:end_mel_idx] = 0", "bbox": [120.0, 482.0, 448.0, 494.0]}, {"polygon": [[121.0, 493.0], [459.0, 493.0], [459.0, 506.0], [121.0, 506.0]], "confidence": 0.9773650169372559, "text": "start_end_mel_idx.append([start_mel_idx, end_mel_idx])", "bbox": [121.0, 493.0, 459.0, 506.0]}, {"polygon": [[99.0, 513.0], [343.0, 513.0], [343.0, 526.0], [99.0, 526.0]], "confidence": 0.9685076475143433, "text": "window_size = 50 # equal to 3.2 second", "bbox": [99.0, 513.0, 343.0, 526.0]}, {"polygon": [[101.0, 535.0], [404.0, 535.0], [404.0, 547.0], [101.0, 547.0]], "confidence": 0.9718760251998901, "text": "# use 1D conv to compute correlation in parallel", "bbox": [101.0, 535.0, 404.0, 547.0]}, {"polygon": [[100.0, 545.0], [388.0, 546.0], [388.0, 558.0], [100.0, 557.0]], "confidence": 0.9769580960273743, "text": "# use unfold() to get chunks by moving windows", "bbox": [100.0, 545.0, 388.0, 558.0]}, {"polygon": [[99.0, 555.0], [704.0, 555.0], [704.0, 568.0], [99.0, 568.0]], "confidence": 0.986444354057312, "text": "melspec_av_all_cuts = av_melspec_masked.unfold(dimension=-1, size=window_size, step=window_size)", "bbox": [99.0, 555.0, 704.0, 568.0]}, {"polygon": [[99.0, 566.0], [570.0, 566.0], [570.0, 579.0], [99.0, 579.0]], "confidence": 0.9780167937278748, "text": "melspec_unfold = cmd_melspec.unfold(dimension=-1, size=window_size, step=1)", "bbox": [99.0, 566.0, 570.0, 579.0]}, {"polygon": [[99.0, 578.0], [609.0, 578.0], [609.0, 590.0], [99.0, 590.0]], "confidence": 0.983605146408081, "text": "conv_output = torch.einsum('bctw,bcaw->bta', melspec_unfold, melspec_av_all_cuts)", "bbox": [99.0, 578.0, 609.0, 590.0]}, {"polygon": [[99.0, 586.0], [586.0, 586.0], [586.0, 610.0], [99.0, 610.0]], "confidence": 0.9791790246963501, "text": "ad_mask = torch.any(melspec_av_all_cuts.mean(1)==0, dim==1)\nconv_output = conv_output[0, :, ~ad_mask[0].bool()] # only keep non-AD chunks", "bbox": [99.0, 586.0, 586.0, 610.0]}, {"polygon": [[99.0, 598.0], [301.0, 599.0], [301.0, 611.0], [99.0, 610.0]], "confidence": 0.9647576808929443, "text": "conv_output = conv_output[0, :,", "bbox": [99.0, 598.0, 301.0, 611.0]}, {"polygon": [[99.0, 609.0], [335.0, 609.0], [335.0, 621.0], [99.0, 621.0]], "confidence": 0.9718865752220154, "text": "max_cor, max_pos = conv_output.max(0)", "bbox": [99.0, 609.0, 335.0, 621.0]}, {"polygon": [[99.0, 620.0], [272.0, 620.0], [272.0, 631.0], [99.0, 631.0]], "confidence": 0.9627019166946411, "text": "plot_outputs = torch.stack(", "bbox": [99.0, 620.0, 272.0, 631.0]}, {"polygon": [[121.0, 629.0], [704.0, 628.0], [704.0, 642.0], [121.0, 643.0]], "confidence": 0.9767293930053711, "text": "[torch.arange(0, av_melspec_masked.shape[-1]-window_size+1, window_size)[\"ad_mask[0].bool()],", "bbox": [121.0, 629.0, 704.0, 642.0]}, {"polygon": [[124.0, 641.0], [390.0, 641.0], [390.0, 653.0], [124.0, 653.0]], "confidence": 0.9282668232917786, "text": "mx_cos, max_cor/window_size], -1).numpy()", "bbox": [124.0, 641.0, 390.0, 653.0]}, {"polygon": [[99.0, 661.0], [405.0, 661.0], [405.0, 686.0], [99.0, 686.0]], "confidence": 0.9790173172950745, "text": "# prepare scatters for RANSAC\npeak_cor = np.max([i[-1] for i in plot_outputs])", "bbox": [99.0, 661.0, 405.0, 686.0]}, {"polygon": [[99.0, 682.0], [635.0, 679.0], [635.0, 706.0], [99.0, 709.0]], "confidence": 0.9900093674659729, "text": "samples_X = (plot_outputs[:, 1:2] + np.linspace(0, window_size, 5)[None,:]).flatten()\nsamples_y = (plot_outputs[:, 0:1] + np.linspace(0, window_size, 5)[None,:]].flatten()", "bbox": [99.0, 682.0, 635.0, 706.0]}, {"polygon": [[100.0, 704.0], [511.0, 704.0], [511.0, 717.0], [100.0, 717.0]], "confidence": 0.5590455532073975, "text": "ssssse", "bbox": [100.0, 704.0, 511.0, 717.0]}, {"polygon": [[100.0, 725.0], [529.0, 725.0], [529.0, 737.0], [100.0, 737.0]], "confidence": 0.9839520454406738, "text": "ransac = RANSACRegressor(random_state=0, residual_threshold=50).fit(", "bbox": [100.0, 725.0, 529.0, 737.0]}, {"polygon": [[121.0, 736.0], [453.0, 736.0], [453.0, 748.0], [121.0, 748.0]], "confidence": 0.4939012825489044, "text": "ssssse", "bbox": [121.0, 736.0, 453.0, 748.0]}, {"polygon": [[101.0, 757.0], [302.0, 757.0], [302.0, 769.0], [101.0, 769.0]], "confidence": 0.9657648801803589, "text": "inlier_mask = ransac.inlier_mask", "bbox": [101.0, 757.0, 302.0, 769.0]}, {"polygon": [[100.0, 768.0], [365.0, 765.0], [365.0, 779.0], [100.0, 782.0]], "confidence": 0.9738779067993164, "text": "outlier_mask = np.logical_not(inlier_mask)", "bbox": [100.0, 768.0, 365.0, 779.0]}, {"polygon": [[101.0, 778.0], [327.0, 778.0], [327.0, 791.0], [101.0, 791.0]], "confidence": 0.9028339982032776, "text": "ssseeeerrrrvvville", "bbox": [101.0, 778.0, 327.0, 791.0]}, {"polygon": [[101.0, 789.0], [371.0, 789.0], [371.0, 800.0], [101.0, 800.0]], "confidence": 0.9722251296043396, "text": "intercept = ransac.estimator_.intercept_[0]", "bbox": [101.0, 789.0, 371.0, 800.0]}, {"polygon": [[101.0, 811.0], [159.0, 811.0], [159.0, 820.0], [101.0, 820.0]], "confidence": 0.8783096075057983, "text": "# get mse", "bbox": [101.0, 811.0, 159.0, 820.0]}, {"polygon": [[99.0, 820.0], [446.0, 820.0], [446.0, 832.0], [99.0, 832.0]], "confidence": 0.9791073203086853, "text": "y_pred = ransac.predict(samples_X[inlier_mask][:,None])", "bbox": [99.0, 820.0, 446.0, 832.0]}, {"polygon": [[99.0, 831.0], [503.0, 831.0], [503.0, 843.0], [99.0, 843.0]], "confidence": 0.9823010563850403, "text": "mse = mean_squared_error(samples_y[inlier_mask][:,None], y_pred)", "bbox": [99.0, 831.0, 503.0, 843.0]}, {"polygon": [[99.0, 851.0], [240.0, 852.0], [240.0, 877.0], [99.0, 875.0]], "confidence": 0.9058430194854736, "text": "print('MSE=', mse)", "bbox": [99.0, 851.0, 240.0, 877.0]}, {"polygon": [[99.0, 872.0], [289.0, 873.0], [289.0, 886.0], [99.0, 885.0]], "confidence": 0.9387932419776917, "text": "print('intercept=', intercept)", "bbox": [99.0, 872.0, 289.0, 886.0]}, {"polygon": [[102.0, 884.0], [277.0, 884.0], [277.0, 895.0], [102.0, 895.0]], "confidence": 0.9644544124603271, "text": "return mse, slope, intercept", "bbox": [102.0, 884.0, 277.0, 895.0]}, {"polygon": [[388.0, 976.0], [405.0, 976.0], [405.0, 990.0], [388.0, 990.0]], "confidence": 0.5912330150604248, "text": "16", "bbox": [388.0, 976.0, 405.0, 990.0]}], "languages": ["en"], "image_bbox": [0.0, 0.0, 816.0, 1056.0], "page": 16}, {"text_lines": [{"polygon": [[70.0, 226.0], [479.0, 226.0], [479.0, 242.0], [70.0, 242.0]], "confidence": 0.9852199554443359, "text": "Algorithm 2: Python script for LLM-AD-Eval, with OpenAI gpt-3.5-turbo API.", "bbox": [70.0, 226.0, 479.0, 242.0]}, {"polygon": [[80.0, 254.0], [165.0, 254.0], [165.0, 278.0], [80.0, 278.0]], "confidence": 0.9537847638130188, "text": "import ast\nimport openai", "bbox": [80.0, 254.0, 165.0, 278.0]}, {"polygon": [[79.0, 286.0], [348.0, 286.0], [348.0, 309.0], [79.0, 309.0]], "confidence": 0.9754801988601685, "text": "def eval_each(text_gt, text_pred, client):\n    # Compute the LLM-AD-Eval score", "bbox": [79.0, 286.0, 348.0, 309.0]}, {"polygon": [[99.0, 307.0], [380.0, 307.0], [380.0, 319.0], [99.0, 319.0]], "confidence": 0.9758661985397339, "text": "completion = client.chat.completions.create(", "bbox": [99.0, 307.0, 380.0, 319.0]}, {"polygon": [[118.0, 317.0], [262.0, 317.0], [262.0, 330.0], [118.0, 330.0]], "confidence": 0.9491412043571472, "text": "model=\"gpt-3.5-turbo\",", "bbox": [118.0, 317.0, 262.0, 330.0]}, {"polygon": [[119.0, 328.0], [187.0, 328.0], [187.0, 340.0], [119.0, 340.0]], "confidence": 0.8698766827583313, "text": "messages=[", "bbox": [119.0, 328.0, 187.0, 340.0]}, {"polygon": [[140.0, 341.0], [150.0, 341.0], [150.0, 350.0], [140.0, 350.0]], "confidence": 0.09553108364343643, "text": "11", "bbox": [140.0, 341.0, 150.0, 350.0]}, {"polygon": [[158.0, 349.0], [267.0, 349.0], [267.0, 361.0], [158.0, 361.0]], "confidence": 0.9251654148101807, "text": "\"role\": \"system\",", "bbox": [158.0, 349.0, 267.0, 361.0]}, {"polygon": [[159.0, 359.0], [228.0, 359.0], [228.0, 372.0], [159.0, 372.0]], "confidence": 0.894862174987793, "text": "\"content\":", "bbox": [159.0, 359.0, 228.0, 372.0]}, {"polygon": [[178.0, 370.0], [435.0, 370.0], [435.0, 382.0], [178.0, 382.0]], "confidence": 0.9748975038528442, "text": "\"You are an intelligent chatbot designed", "bbox": [178.0, 370.0, 435.0, 382.0]}, {"polygon": [[210.0, 381.0], [718.0, 381.0], [718.0, 393.0], [210.0, 393.0]], "confidence": 0.9874641299247742, "text": "for evaluating the quality of generative outputs for movie audio descriptions. \"", "bbox": [210.0, 381.0, 718.0, 393.0]}, {"polygon": [[178.0, 391.0], [689.0, 391.0], [689.0, 403.0], [178.0, 403.0]], "confidence": 0.9869798421859741, "text": "\"Your task is to compare the predicted audio descriptions with the correct audio", "bbox": [178.0, 391.0, 689.0, 403.0]}, {"polygon": [[203.0, 400.0], [743.0, 400.0], [743.0, 425.0], [203.0, 425.0]], "confidence": 0.989922046661377, "text": "descriptions and determine its level of match, considering mainly the visual elements\n like actions, objects and interactions. Here's how you can accomplish the task:\"", "bbox": [203.0, 400.0, 743.0, 425.0]}, {"polygon": [[178.0, 422.0], [231.0, 424.0], [231.0, 433.0], [178.0, 432.0]], "confidence": 0.8550660610198975, "text": "\"\"", "bbox": [178.0, 422.0, 231.0, 433.0]}, {"polygon": [[178.0, 433.0], [296.0, 433.0], [296.0, 446.0], [178.0, 446.0]], "confidence": 0.932018518447876, "text": "\"#INSTRUCTIONS: \"", "bbox": [178.0, 433.0, 296.0, 446.0]}, {"polygon": [[178.0, 444.0], [496.0, 444.0], [496.0, 456.0], [178.0, 456.0]], "confidence": 0.9789648056030273, "text": "\"- Check if the predicted audio description covers", "bbox": [178.0, 444.0, 496.0, 456.0]}, {"polygon": [[204.0, 454.0], [742.0, 454.0], [742.0, 468.0], [204.0, 468.0]], "confidence": 0.9849331974983215, "text": "the main visual events from the movie, especially focusing on the verbs and nouns.\\n\"", "bbox": [204.0, 454.0, 742.0, 468.0]}, {"polygon": [[178.0, 465.0], [430.0, 465.0], [430.0, 477.0], [178.0, 477.0]], "confidence": 0.9733077883720398, "text": "\"- Evaluate whether the predicted audio", "bbox": [178.0, 465.0, 430.0, 477.0]}, {"polygon": [[208.0, 474.0], [742.0, 474.0], [742.0, 499.0], [208.0, 499.0]], "confidence": 0.9911156296730042, "text": "description includes specific details rather than just generic points. It should \nprovide comprehensive information that is tied to specific elements of the video.\\n\"", "bbox": [208.0, 474.0, 742.0, 499.0]}, {"polygon": [[178.0, 496.0], [326.0, 498.0], [326.0, 510.0], [178.0, 508.0]], "confidence": 0.9532433152198792, "text": "\"- Consider synonyms or", "bbox": [178.0, 496.0, 326.0, 510.0]}, {"polygon": [[210.0, 506.0], [736.0, 506.0], [736.0, 520.0], [210.0, 520.0]], "confidence": 0.9857553839683533, "text": "paraphrases as valid matches. Consider pronouns like 'he' or 'she' as valid matches", "bbox": [210.0, 506.0, 736.0, 520.0]}, {"polygon": [[210.0, 518.0], [702.0, 518.0], [702.0, 530.0], [210.0, 530.0]], "confidence": 0.9864084720611572, "text": "with character names. Consider different character names as valid matches. \\n\"", "bbox": [210.0, 518.0, 702.0, 530.0]}, {"polygon": [[178.0, 527.0], [743.0, 527.0], [743.0, 552.0], [178.0, 552.0]], "confidence": 0.9781087636947632, "text": "\"- Provide a single evaluation score that reflects the level of match of the\n    prediction, considering the visual elements like actions, objects and interactions.\"", "bbox": [178.0, 527.0, 743.0, 552.0]}, {"polygon": [[140.0, 551.0], [156.0, 551.0], [156.0, 573.0], [140.0, 573.0]], "confidence": 0.29900097846984863, "text": "),", "bbox": [140.0, 551.0, 156.0, 573.0]}, {"polygon": [[159.0, 571.0], [255.0, 571.0], [255.0, 583.0], [159.0, 583.0]], "confidence": 0.9172656536102295, "text": "\"role\": \"user\",", "bbox": [159.0, 571.0, 255.0, 583.0]}, {"polygon": [[159.0, 579.0], [229.0, 581.0], [229.0, 594.0], [159.0, 592.0]], "confidence": 0.8930522799491882, "text": "\"content\":", "bbox": [159.0, 579.0, 229.0, 594.0]}, {"polygon": [[178.0, 592.0], [589.0, 592.0], [589.0, 604.0], [178.0, 604.0]], "confidence": 0.9839980602264404, "text": "\"Please evaluate the following movie audio description pair:\\n\\n\"", "bbox": [178.0, 592.0, 589.0, 604.0]}, {"polygon": [[178.0, 602.0], [439.0, 603.0], [439.0, 616.0], [178.0, 615.0]], "confidence": 0.9746441841125488, "text": "f\"Correct Audio Description: {text_gt}\\n\"", "bbox": [178.0, 602.0, 439.0, 616.0]}, {"polygon": [[178.0, 612.0], [648.0, 612.0], [648.0, 637.0], [178.0, 637.0]], "confidence": 0.9868236780166626, "text": "f\"Predicted Audio Description: {text_pred}\\n\\n\"\n\"Provide your evaluation only as a matching score where the matching score", "bbox": [178.0, 612.0, 648.0, 637.0]}, {"polygon": [[177.0, 633.0], [731.0, 633.0], [731.0, 658.0], [177.0, 658.0]], "confidence": 0.9839781522750854, "text": "\"Please an integer value between 0 and 5, with 5 indicating the highest level of match.\n\"Please generate the response in the form of a Python dictionary string", "bbox": [177.0, 633.0, 731.0, 658.0]}, {"polygon": [[210.0, 655.0], [724.0, 655.0], [724.0, 667.0], [210.0, 667.0]], "confidence": 0.9848542809486389, "text": "with keys 'score', where its value is the matching score in INTEGER, not STRING.\"", "bbox": [210.0, 655.0, 724.0, 667.0]}, {"polygon": [[178.0, 665.0], [280.0, 667.0], [280.0, 679.0], [178.0, 677.0]], "confidence": 0.9239688515663147, "text": "\"DO NOT PROVIDE", "bbox": [178.0, 665.0, 280.0, 679.0]}, {"polygon": [[178.0, 676.0], [720.0, 676.0], [720.0, 700.0], [178.0, 700.0]], "confidence": 0.9869997501373291, "text": "ANY OTHER OUTPUT TEXT OR EXPLANATION. Only provide the Python dictionary string.\n\"For example, your response should look like this: {'score': }.\"", "bbox": [178.0, 676.0, 720.0, 700.0]}, {"polygon": [[719.0, 676.0], [729.0, 676.0], [729.0, 685.0], [719.0, 685.0]], "confidence": 0.8733255863189697, "text": "\"\"", "bbox": [719.0, 676.0, 729.0, 685.0]}, {"polygon": [[140.0, 699.0], [148.0, 699.0], [148.0, 709.0], [140.0, 709.0]], "confidence": 0.237879678606987, "text": "l", "bbox": [140.0, 699.0, 148.0, 709.0]}, {"polygon": [[121.0, 709.0], [129.0, 709.0], [129.0, 722.0], [121.0, 722.0]], "confidence": 0.2768170237541199, "text": "]", "bbox": [121.0, 709.0, 129.0, 722.0]}, {"polygon": [[100.0, 729.0], [367.0, 729.0], [367.0, 741.0], [100.0, 741.0]], "confidence": 0.9725468754768372, "text": "# Convert response to a Python dictionary.", "bbox": [100.0, 729.0, 367.0, 741.0]}, {"polygon": [[100.0, 740.0], [454.0, 740.0], [454.0, 752.0], [100.0, 752.0]], "confidence": 0.9803110957145691, "text": "response_message = completion.choices[0].message.content", "bbox": [100.0, 740.0, 454.0, 752.0]}, {"polygon": [[101.0, 750.0], [416.0, 750.0], [416.0, 762.0], [101.0, 762.0]], "confidence": 0.9722921252250671, "text": "response_dict = ast.literal_eval(response_message)", "bbox": [101.0, 750.0, 416.0, 762.0]}, {"polygon": [[101.0, 762.0], [229.0, 762.0], [229.0, 773.0], [101.0, 773.0]], "confidence": 0.9512955546379089, "text": "return response_dict", "bbox": [101.0, 762.0, 229.0, 773.0]}, {"polygon": [[80.0, 783.0], [189.0, 783.0], [189.0, 794.0], [80.0, 794.0]], "confidence": 0.9423844814300537, "text": "client = OpenAI()", "bbox": [80.0, 783.0, 189.0, 794.0]}, {"polygon": [[80.0, 793.0], [314.0, 793.0], [314.0, 806.0], [80.0, 806.0]], "confidence": 0.9702126383781433, "text": "eval_each(text_gt, text_pred, client)", "bbox": [80.0, 793.0, 314.0, 806.0]}, {"polygon": [[388.0, 975.0], [405.0, 975.0], [405.0, 990.0], [388.0, 990.0]], "confidence": 0.6280514597892761, "text": "17", "bbox": [388.0, 975.0, 405.0, 990.0]}], "languages": ["en"], "image_bbox": [0.0, 0.0, 816.0, 1056.0], "page": 17}]}