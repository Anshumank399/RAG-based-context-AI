{"sample3": [{"text_lines": [{"polygon": [[65.0, 138.0], [728.0, 138.0], [728.0, 160.0], [65.0, 160.0]], "confidence": 0.9864480495452881, "text": "Guess The Unseen: Dynamic 3D Scene Reconstruction from Partial 2D Glimpses", "bbox": [65.0, 138.0, 728.0, 160.0]}, {"polygon": [[238.0, 192.0], [306.0, 192.0], [306.0, 208.0], [238.0, 208.0]], "confidence": 0.8947537541389465, "text": "Inhee Lee", "bbox": [238.0, 192.0, 306.0, 208.0]}, {"polygon": [[337.0, 192.0], [439.0, 192.0], [439.0, 210.0], [337.0, 210.0]], "confidence": 0.920609176158905, "text": "Byungjun Kim", "bbox": [337.0, 192.0, 439.0, 210.0]}, {"polygon": [[468.0, 192.0], [554.0, 192.0], [554.0, 208.0], [468.0, 208.0]], "confidence": 0.9113284945487976, "text": "Hanbyul Joo", "bbox": [468.0, 192.0, 554.0, 208.0]}, {"polygon": [[310.0, 212.0], [480.0, 212.0], [480.0, 227.0], [310.0, 227.0]], "confidence": 0.9601209163665771, "text": "Seoul National University", "bbox": [310.0, 212.0, 480.0, 227.0]}, {"polygon": [[241.0, 233.0], [550.0, 233.0], [550.0, 248.0], [241.0, 248.0]], "confidence": 0.9739946126937866, "text": "{ininin0516, byungjun.kim, hbjoo}@snu.ac.kr", "bbox": [241.0, 233.0, 550.0, 248.0]}, {"polygon": [[280.0, 250.0], [504.0, 250.0], [504.0, 264.0], [280.0, 264.0]], "confidence": 0.9673985242843628, "text": "https://snuvclab.github.io/gtu/", "bbox": [280.0, 250.0, 504.0, 264.0]}, {"polygon": [[11.0, 293.0], [55.0, 294.0], [53.0, 740.0], [9.0, 739.0]], "confidence": 0.9516170620918274, "text": "urXiv:2404.14410v1 [cs.CV] 22 Apr 202", "bbox": [11.0, 293.0, 55.0, 740.0]}, {"polygon": [[123.0, 324.0], [256.0, 324.0], [256.0, 333.0], [123.0, 333.0]], "confidence": 0.9662563800811768, "text": "Occluded partial observations", "bbox": [123.0, 324.0, 256.0, 333.0]}, {"polygon": [[367.0, 507.0], [468.0, 507.0], [468.0, 519.0], [367.0, 519.0]], "confidence": 0.9518035054206848, "text": "Novel View Synthesis", "bbox": [367.0, 507.0, 468.0, 519.0]}, {"polygon": [[568.0, 507.0], [666.0, 507.0], [666.0, 519.0], [568.0, 519.0]], "confidence": 0.9517277479171753, "text": "Novel Pose Synthesis", "bbox": [568.0, 507.0, 666.0, 519.0]}, {"polygon": [[131.0, 528.0], [263.0, 528.0], [263.0, 542.0], [131.0, 542.0]], "confidence": 0.9548680782318115, "text": "Input: Monocular Video", "bbox": [131.0, 528.0, 263.0, 542.0]}, {"polygon": [[407.0, 528.0], [597.0, 528.0], [597.0, 542.0], [407.0, 542.0]], "confidence": 0.9678215384483337, "text": "Output: Animatable Person 3D-GS", "bbox": [407.0, 528.0, 597.0, 542.0]}, {"polygon": [[63.0, 559.0], [728.0, 559.0], [728.0, 574.0], [63.0, 574.0]], "confidence": 0.9912550449371338, "text": "Figure 1. We present a method to reconstruct dynamic scenes from a monocular video capturing partial 2D observations. As a key advantage,", "bbox": [63.0, 559.0, 728.0, 574.0]}, {"polygon": [[63.0, 574.0], [728.0, 574.0], [728.0, 589.0], [63.0, 589.0]], "confidence": 0.990595817565918, "text": "our method can estimate the unseen body parts by leveraging a pre-trained diffusion model [ 42 ] via SDS method [ 39 ]. The reconstructed", "bbox": [63.0, 574.0, 728.0, 589.0]}, {"polygon": [[63.0, 590.0], [728.0, 590.0], [728.0, 604.0], [63.0, 604.0]], "confidence": 0.9907637238502502, "text": "scenes can be rendered to any viewpoint and each human body can be transformed into any body posture controlled by SMPL [ 29 ]", "bbox": [63.0, 590.0, 728.0, 604.0]}, {"polygon": [[64.0, 606.0], [121.0, 606.0], [121.0, 617.0], [64.0, 617.0]], "confidence": 0.915627658367157, "text": "parameters.", "bbox": [64.0, 606.0, 121.0, 617.0]}, {"polygon": [[192.0, 641.0], [254.0, 641.0], [254.0, 657.0], [192.0, 657.0]], "confidence": 0.8846327066421509, "text": "Abstract", "bbox": [192.0, 641.0, 254.0, 657.0]}, {"polygon": [[409.0, 643.0], [728.0, 643.0], [728.0, 657.0], [409.0, 657.0]], "confidence": 0.9821436405181885, "text": "observations. After reconstruction, our method is capable of", "bbox": [409.0, 643.0, 728.0, 657.0]}, {"polygon": [[408.0, 659.0], [728.0, 659.0], [728.0, 673.0], [408.0, 673.0]], "confidence": 0.9832658171653748, "text": "not only rendering the scene in any novel views at arbitrary", "bbox": [408.0, 659.0, 728.0, 673.0]}, {"polygon": [[408.0, 674.0], [728.0, 674.0], [728.0, 689.0], [408.0, 689.0]], "confidence": 0.9821730256080627, "text": "time instances, but also editing the 3D scene by removing", "bbox": [408.0, 674.0, 728.0, 689.0]}, {"polygon": [[80.0, 682.0], [383.0, 682.0], [383.0, 696.0], [80.0, 696.0]], "confidence": 0.9807605147361755, "text": "In this paper, we present a method to reconstruct the", "bbox": [80.0, 682.0, 383.0, 696.0]}, {"polygon": [[409.0, 690.0], [728.0, 690.0], [728.0, 705.0], [409.0, 705.0]], "confidence": 0.9815866351127625, "text": "individual humans or applying different motions for each", "bbox": [409.0, 690.0, 728.0, 705.0]}, {"polygon": [[63.0, 698.0], [384.0, 698.0], [384.0, 712.0], [63.0, 712.0]], "confidence": 0.9778555631637573, "text": "world and multiple dynamic humans in 3D from a monocu-", "bbox": [63.0, 698.0, 384.0, 712.0]}, {"polygon": [[409.0, 707.0], [728.0, 707.0], [728.0, 720.0], [409.0, 720.0]], "confidence": 0.9810062050819397, "text": "human. Through various experiments, we demonstrate the", "bbox": [409.0, 707.0, 728.0, 720.0]}, {"polygon": [[63.0, 715.0], [384.0, 715.0], [384.0, 728.0], [63.0, 728.0]], "confidence": 0.9826280474662781, "text": "lar video input. As a key idea, we represent both the world", "bbox": [63.0, 715.0, 384.0, 728.0]}, {"polygon": [[409.0, 722.0], [729.0, 722.0], [729.0, 736.0], [409.0, 736.0]], "confidence": 0.9806033372879028, "text": "quality and efficiency of our methods over alternative exist-", "bbox": [409.0, 722.0, 729.0, 736.0]}, {"polygon": [[63.0, 731.0], [383.0, 731.0], [383.0, 745.0], [63.0, 745.0]], "confidence": 0.9824687242507935, "text": "and multiple humans via the recently emerging 3D Gaussian", "bbox": [63.0, 731.0, 383.0, 745.0]}, {"polygon": [[409.0, 739.0], [496.0, 739.0], [496.0, 753.0], [409.0, 753.0]], "confidence": 0.9199827909469604, "text": "ing approaches.", "bbox": [409.0, 739.0, 496.0, 753.0]}, {"polygon": [[63.0, 747.0], [383.0, 747.0], [383.0, 761.0], [63.0, 761.0]], "confidence": 0.9809574484825134, "text": "Splatting (3D-GS) representation, enabling to conveniently", "bbox": [63.0, 747.0, 383.0, 761.0]}, {"polygon": [[63.0, 762.0], [384.0, 762.0], [384.0, 777.0], [63.0, 777.0]], "confidence": 0.9784663915634155, "text": "and efficiently compose and render them together. In par-", "bbox": [63.0, 762.0, 384.0, 777.0]}, {"polygon": [[63.0, 778.0], [384.0, 778.0], [384.0, 792.0], [63.0, 792.0]], "confidence": 0.9828931093215942, "text": "ticular, we address the scenarios with severely limited and", "bbox": [63.0, 778.0, 384.0, 792.0]}, {"polygon": [[410.0, 784.0], [515.0, 784.0], [515.0, 799.0], [410.0, 799.0]], "confidence": 0.935633659362793, "text": "1. Introduction", "bbox": [410.0, 784.0, 515.0, 799.0]}, {"polygon": [[63.0, 795.0], [382.0, 795.0], [382.0, 808.0], [63.0, 808.0]], "confidence": 0.9818542003631592, "text": "sparse observations in 3D human reconstruction, a common", "bbox": [63.0, 795.0, 382.0, 808.0]}, {"polygon": [[63.0, 810.0], [384.0, 810.0], [384.0, 825.0], [63.0, 825.0]], "confidence": 0.981886088848114, "text": "challenge encountered in the real world. To tackle this chal-", "bbox": [63.0, 810.0, 384.0, 825.0]}, {"polygon": [[425.0, 810.0], [728.0, 810.0], [728.0, 825.0], [425.0, 825.0]], "confidence": 0.9811261892318726, "text": "The process of digitizing our world in 3D necessitates", "bbox": [425.0, 810.0, 728.0, 825.0]}, {"polygon": [[63.0, 827.0], [384.0, 827.0], [384.0, 840.0], [63.0, 840.0]], "confidence": 0.9811491370201111, "text": "lenge, we introduce a novel approach to optimize the 3D-GS", "bbox": [63.0, 827.0, 384.0, 840.0]}, {"polygon": [[409.0, 827.0], [728.0, 827.0], [728.0, 840.0], [409.0, 840.0]], "confidence": 0.9828435182571411, "text": "the reconstruction of not only static environmental elements", "bbox": [409.0, 827.0, 728.0, 840.0]}, {"polygon": [[63.0, 843.0], [384.0, 843.0], [384.0, 857.0], [63.0, 857.0]], "confidence": 0.9831403493881226, "text": "representation in a canonical space by fusing the sparse cues", "bbox": [63.0, 843.0, 384.0, 857.0]}, {"polygon": [[408.0, 842.0], [728.0, 842.0], [728.0, 857.0], [408.0, 857.0]], "confidence": 0.982085645198822, "text": "but also dynamic objects, particularly humans. Given the", "bbox": [408.0, 842.0, 728.0, 857.0]}, {"polygon": [[63.0, 858.0], [384.0, 858.0], [384.0, 872.0], [63.0, 872.0]], "confidence": 0.9802869558334351, "text": "in the common space, where we leverage a pre-trained 2D", "bbox": [63.0, 858.0, 384.0, 872.0]}, {"polygon": [[409.0, 858.0], [729.0, 858.0], [729.0, 872.0], [409.0, 872.0]], "confidence": 0.9823116660118103, "text": "limited availability of multi-view camera setups, a ground-", "bbox": [409.0, 858.0, 729.0, 872.0]}, {"polygon": [[63.0, 874.0], [383.0, 874.0], [383.0, 888.0], [63.0, 888.0]], "confidence": 0.9805346727371216, "text": "diffusion model to synthesize unseen views while keeping the", "bbox": [63.0, 874.0, 383.0, 888.0]}, {"polygon": [[408.0, 874.0], [729.0, 874.0], [729.0, 888.0], [408.0, 888.0]], "confidence": 0.9813181757926941, "text": "breaking leap can be achieved by developing a 4D recon-", "bbox": [408.0, 874.0, 729.0, 888.0]}, {"polygon": [[63.0, 890.0], [384.0, 890.0], [384.0, 904.0], [63.0, 904.0]], "confidence": 0.9788724184036255, "text": "consistency with the observed 2D appearances. We demon-", "bbox": [63.0, 890.0, 384.0, 904.0]}, {"polygon": [[409.0, 890.0], [728.0, 890.0], [728.0, 903.0], [409.0, 903.0]], "confidence": 0.9825249910354614, "text": "struction method capable of rendering the scenes at novel", "bbox": [409.0, 890.0, 728.0, 903.0]}, {"polygon": [[63.0, 906.0], [382.0, 906.0], [382.0, 920.0], [63.0, 920.0]], "confidence": 0.9816443920135498, "text": "strate our method can reconstruct high-quality animatable", "bbox": [63.0, 906.0, 382.0, 920.0]}, {"polygon": [[409.0, 906.0], [729.0, 906.0], [729.0, 920.0], [409.0, 920.0]], "confidence": 0.9827311038970947, "text": "views and arbitrary times just using a monocular video in-", "bbox": [409.0, 906.0, 729.0, 920.0]}, {"polygon": [[63.0, 922.0], [382.0, 922.0], [382.0, 935.0], [63.0, 935.0]], "confidence": 0.9817981123924255, "text": "3D humans in various challenging examples, in the presence", "bbox": [63.0, 922.0, 382.0, 935.0]}, {"polygon": [[409.0, 922.0], [728.0, 922.0], [728.0, 935.0], [409.0, 935.0]], "confidence": 0.9832597970962524, "text": "put. Reconstructing static components (e.g., buildings) from", "bbox": [409.0, 922.0, 728.0, 935.0]}, {"polygon": [[63.0, 938.0], [382.0, 938.0], [382.0, 952.0], [63.0, 952.0]], "confidence": 0.9788494110107422, "text": "of occlusion, image crops, few-shot, and extremely sparse", "bbox": [63.0, 938.0, 382.0, 952.0]}, {"polygon": [[409.0, 938.0], [729.0, 938.0], [729.0, 952.0], [409.0, 952.0]], "confidence": 0.9762171506881714, "text": "monocular video benefits from the well-established multi-", "bbox": [409.0, 938.0, 729.0, 952.0]}], "languages": ["en"], "image_bbox": [0.0, 0.0, 816.0, 1056.0], "page": 1}, {"text_lines": [{"polygon": [[63.0, 96.0], [383.0, 96.0], [383.0, 111.0], [63.0, 111.0]], "confidence": 0.9803904891014099, "text": "view geometry principles [ 13 ], where epipolar geometry", "bbox": [63.0, 96.0, 383.0, 111.0]}, {"polygon": [[409.0, 96.0], [729.0, 96.0], [729.0, 111.0], [409.0, 111.0]], "confidence": 0.9820914268493652, "text": "incorporating SDS loss with a diffusion model and textual", "bbox": [409.0, 96.0, 729.0, 111.0]}, {"polygon": [[63.0, 113.0], [384.0, 113.0], [384.0, 127.0], [63.0, 127.0]], "confidence": 0.984590470790863, "text": "constraints across different views still hold at different times.", "bbox": [63.0, 113.0, 384.0, 127.0]}, {"polygon": [[409.0, 112.0], [729.0, 112.0], [729.0, 127.0], [409.0, 127.0]], "confidence": 0.981424868106842, "text": "inversion; and (3) introducing efficient 4D scene reconstruc-", "bbox": [409.0, 112.0, 729.0, 127.0]}, {"polygon": [[63.0, 129.0], [383.0, 129.0], [383.0, 143.0], [63.0, 143.0]], "confidence": 0.9825845956802368, "text": "Recent advances have further enhanced the quality of these", "bbox": [63.0, 129.0, 383.0, 143.0]}, {"polygon": [[408.0, 129.0], [544.0, 129.0], [544.0, 143.0], [408.0, 143.0]], "confidence": 0.9625899791717529, "text": "tion and editing pipeline.", "bbox": [408.0, 129.0, 544.0, 143.0]}, {"polygon": [[63.0, 145.0], [384.0, 145.0], [384.0, 160.0], [63.0, 160.0]], "confidence": 0.9834632873535156, "text": "reconstructions by leveraging implicit 3D representations, as", "bbox": [63.0, 145.0, 384.0, 160.0]}, {"polygon": [[410.0, 159.0], [523.0, 159.0], [523.0, 176.0], [410.0, 176.0]], "confidence": 0.9360901117324829, "text": "2. Related Work", "bbox": [410.0, 159.0, 523.0, 176.0]}, {"polygon": [[63.0, 161.0], [384.0, 161.0], [384.0, 176.0], [63.0, 176.0]], "confidence": 0.9813357591629028, "text": "demonstrated by NeRF [ 33 ], NeuS [ 52 ], and Gaussian Splat-", "bbox": [63.0, 161.0, 384.0, 176.0]}, {"polygon": [[63.0, 177.0], [369.0, 177.0], [369.0, 191.0], [63.0, 191.0]], "confidence": 0.9819411039352417, "text": "ting (3D-GS) [ 20 ], resulting in more realistic renderings.", "bbox": [63.0, 177.0, 369.0, 191.0]}, {"polygon": [[409.0, 186.0], [728.0, 186.0], [728.0, 201.0], [409.0, 201.0]], "confidence": 0.979101836681366, "text": "Human Reconstruction from Monocular Video. There", "bbox": [409.0, 186.0, 728.0, 201.0]}, {"polygon": [[80.0, 196.0], [384.0, 196.0], [384.0, 210.0], [80.0, 210.0]], "confidence": 0.9822561740875244, "text": "However, the same approach is not directly applicable to", "bbox": [80.0, 196.0, 384.0, 210.0]}, {"polygon": [[408.0, 203.0], [728.0, 203.0], [728.0, 217.0], [408.0, 217.0]], "confidence": 0.9841809272766113, "text": "has been a series of approaches [ 16 , 18 , 38 , 54 , 58 , 62 , 62 ]", "bbox": [408.0, 203.0, 728.0, 217.0]}, {"polygon": [[63.0, 212.0], [384.0, 212.0], [384.0, 227.0], [63.0, 227.0]], "confidence": 0.9785249829292297, "text": "dynamically moving components, specifically humans. Early", "bbox": [63.0, 212.0, 384.0, 227.0]}, {"polygon": [[408.0, 219.0], [729.0, 219.0], [729.0, 233.0], [408.0, 233.0]], "confidence": 0.9811719059944153, "text": "to reconstruct 3D human avatars from a monocular video", "bbox": [408.0, 219.0, 729.0, 233.0]}, {"polygon": [[63.0, 228.0], [383.0, 228.0], [383.0, 242.0], [63.0, 242.0]], "confidence": 0.9819480180740356, "text": "work addresses this problem within the context of general", "bbox": [63.0, 228.0, 383.0, 242.0]}, {"polygon": [[408.0, 234.0], [729.0, 234.0], [729.0, 249.0], [408.0, 249.0]], "confidence": 0.9810786843299866, "text": "capturing moving humans. Mostly, they tackle the prob-", "bbox": [408.0, 234.0, 729.0, 249.0]}, {"polygon": [[63.0, 244.0], [383.0, 244.0], [383.0, 258.0], [63.0, 258.0]], "confidence": 0.9798526167869568, "text": "non-rigid structure-from-motion approaches [ 1 , 37 ]. More", "bbox": [63.0, 244.0, 383.0, 258.0]}, {"polygon": [[409.0, 250.0], [728.0, 250.0], [728.0, 264.0], [409.0, 264.0]], "confidence": 0.9754525423049927, "text": "lem by finding the correspondences between each frame", "bbox": [409.0, 250.0, 728.0, 264.0]}, {"polygon": [[63.0, 260.0], [384.0, 260.0], [384.0, 274.0], [63.0, 274.0]], "confidence": 0.9814428091049194, "text": "recent breakthroughs leverage human prior models, such", "bbox": [63.0, 260.0, 384.0, 274.0]}, {"polygon": [[409.0, 266.0], [729.0, 266.0], [729.0, 280.0], [409.0, 280.0]], "confidence": 0.9755776524543762, "text": "and optimizing them in a common canonical space. To find", "bbox": [409.0, 266.0, 729.0, 280.0]}, {"polygon": [[63.0, 276.0], [384.0, 276.0], [384.0, 291.0], [63.0, 291.0]], "confidence": 0.9798764586448669, "text": "as SMPL [ 29 , 57 ], as a way to canonicalize the non-rigid", "bbox": [63.0, 276.0, 384.0, 291.0]}, {"polygon": [[408.0, 282.0], [730.0, 282.0], [730.0, 296.0], [408.0, 296.0]], "confidence": 0.9820365905761719, "text": "the correspondences across the frames, diverse prior knowl-", "bbox": [408.0, 282.0, 730.0, 296.0]}, {"polygon": [[63.0, 292.0], [384.0, 292.0], [384.0, 307.0], [63.0, 307.0]], "confidence": 0.9807431101799011, "text": "observations from multiple views of the monocular video", "bbox": [63.0, 292.0, 384.0, 307.0]}, {"polygon": [[408.0, 298.0], [730.0, 298.0], [730.0, 312.0], [408.0, 312.0]], "confidence": 0.9824195504188538, "text": "edge is leveraged such as parametric model [ 2 , 29 ], pixel-", "bbox": [408.0, 298.0, 730.0, 312.0]}, {"polygon": [[63.0, 308.0], [376.0, 308.0], [376.0, 322.0], [63.0, 322.0]], "confidence": 0.9825623631477356, "text": "and transform back into the posed spaces [ 7 , 8 , 12 , 17 , 54 ].", "bbox": [63.0, 308.0, 376.0, 322.0]}, {"polygon": [[408.0, 314.0], [729.0, 314.0], [729.0, 329.0], [408.0, 329.0]], "confidence": 0.9787068367004395, "text": "aligned features [ 35 ], or optical flow [ 60 ]. After the success", "bbox": [408.0, 314.0, 729.0, 329.0]}, {"polygon": [[82.0, 327.0], [383.0, 327.0], [383.0, 341.0], [82.0, 341.0]], "confidence": 0.9816162586212158, "text": "Yet, these approaches often assume the scenarios where", "bbox": [82.0, 327.0, 383.0, 341.0]}, {"polygon": [[408.0, 330.0], [729.0, 330.0], [729.0, 344.0], [408.0, 344.0]], "confidence": 0.9813064932823181, "text": "of NeRF [ 33 ], recent methods [ 16 , 38 , 54 , 62 ] use NeRF and", "bbox": [408.0, 330.0, 729.0, 344.0]}, {"polygon": [[63.0, 343.0], [384.0, 343.0], [384.0, 358.0], [63.0, 358.0]], "confidence": 0.980435311794281, "text": "the camera focuses on the human subject, capturing their", "bbox": [63.0, 343.0, 384.0, 358.0]}, {"polygon": [[408.0, 345.0], [730.0, 345.0], [730.0, 360.0], [408.0, 360.0]], "confidence": 0.9824351072311401, "text": "its variants to reconstruct a human by leveraging a para-", "bbox": [408.0, 345.0, 730.0, 360.0]}, {"polygon": [[63.0, 358.0], [384.0, 359.0], [384.0, 374.0], [63.0, 372.0]], "confidence": 0.9831514954566956, "text": "entire body while the target person revolves around the cam-", "bbox": [63.0, 358.0, 384.0, 374.0]}, {"polygon": [[409.0, 362.0], [730.0, 362.0], [730.0, 376.0], [409.0, 376.0]], "confidence": 0.9738206267356873, "text": "metric model SMPL [ 29 ]. HumanNeRF [ 54 ] and SelfRe-", "bbox": [409.0, 362.0, 730.0, 376.0]}, {"polygon": [[63.0, 374.0], [384.0, 374.0], [384.0, 389.0], [63.0, 389.0]], "confidence": 0.9795176386833191, "text": "era's field of view. While this approach is suitable for in-", "bbox": [63.0, 374.0, 384.0, 389.0]}, {"polygon": [[409.0, 378.0], [728.0, 378.0], [728.0, 392.0], [409.0, 392.0]], "confidence": 0.9820901155471802, "text": "con [ 16 ] improve the reconstruction quality by correcting", "bbox": [409.0, 378.0, 728.0, 392.0]}, {"polygon": [[63.0, 391.0], [384.0, 391.0], [384.0, 405.0], [63.0, 405.0]], "confidence": 0.9782516360282898, "text": "tentionally digitizing a specific individual, they encounter", "bbox": [63.0, 391.0, 384.0, 405.0]}, {"polygon": [[408.0, 394.0], [730.0, 394.0], [730.0, 408.0], [408.0, 408.0]], "confidence": 0.9794782400131226, "text": "the inaccurate canonicalization originated by non-rigid de-", "bbox": [408.0, 394.0, 730.0, 408.0]}, {"polygon": [[63.0, 408.0], [383.0, 408.0], [383.0, 421.0], [63.0, 421.0]], "confidence": 0.9828330278396606, "text": "substantial challenges in in-the-wild video scenarios, where", "bbox": [63.0, 408.0, 383.0, 421.0]}, {"polygon": [[408.0, 410.0], [729.0, 410.0], [729.0, 425.0], [408.0, 425.0]], "confidence": 0.9818646311759949, "text": "formation. Vid2Avatar [ 12 ] and Neuman [ 18 ] reconstruct", "bbox": [408.0, 410.0, 729.0, 425.0]}, {"polygon": [[63.0, 423.0], [383.0, 423.0], [383.0, 438.0], [63.0, 438.0]], "confidence": 0.9836094975471497, "text": "humans are captured in partial, occluded, cropped, and sparse", "bbox": [63.0, 423.0, 383.0, 438.0]}, {"polygon": [[408.0, 425.0], [730.0, 425.0], [730.0, 440.0], [408.0, 440.0]], "confidence": 0.9822680950164795, "text": "a human without a mask by learning a background jointly.", "bbox": [408.0, 425.0, 730.0, 440.0]}, {"polygon": [[63.0, 440.0], [384.0, 440.0], [384.0, 453.0], [63.0, 453.0]], "confidence": 0.9804463982582092, "text": "observed conditions. See the examples shown in Fig. 1 and", "bbox": [63.0, 440.0, 384.0, 453.0]}, {"polygon": [[409.0, 442.0], [729.0, 442.0], [729.0, 456.0], [409.0, 456.0]], "confidence": 0.9811111688613892, "text": "InstantAvatar [ 17 ] reduces the required time of optimiza-", "bbox": [409.0, 442.0, 729.0, 456.0]}, {"polygon": [[63.0, 454.0], [384.0, 454.0], [384.0, 469.0], [63.0, 469.0]], "confidence": 0.9798231720924377, "text": "Fig. 4. Moreover, reconstructing and rendering multiple in-", "bbox": [63.0, 454.0, 384.0, 469.0]}, {"polygon": [[408.0, 458.0], [730.0, 458.0], [730.0, 472.0], [408.0, 472.0]], "confidence": 0.975853681564331, "text": "tion from a few hours into a minute leveraging iNGP [ 34 ].", "bbox": [408.0, 458.0, 730.0, 472.0]}, {"polygon": [[63.0, 470.0], [382.0, 470.0], [382.0, 484.0], [63.0, 484.0]], "confidence": 0.9806241393089294, "text": "dividuals along with 3D backgrounds within the existing", "bbox": [63.0, 470.0, 382.0, 484.0]}, {"polygon": [[408.0, 474.0], [730.0, 474.0], [730.0, 488.0], [408.0, 488.0]], "confidence": 0.9570091366767883, "text": "OccNeRF [ 56 ] proposes a method that can reconstruct peo-", "bbox": [408.0, 474.0, 730.0, 488.0]}, {"polygon": [[63.0, 486.0], [383.0, 486.0], [383.0, 501.0], [63.0, 501.0]], "confidence": 0.9832783937454224, "text": "approaches is non-trivial, mainly due to the complexities of", "bbox": [63.0, 486.0, 383.0, 501.0]}, {"polygon": [[408.0, 488.0], [729.0, 490.0], [729.0, 505.0], [408.0, 503.0]], "confidence": 0.9816358685493469, "text": "ple even with occlusion, using surface-based rendering and", "bbox": [408.0, 488.0, 729.0, 505.0]}, {"polygon": [[63.0, 502.0], [375.0, 502.0], [375.0, 516.0], [63.0, 516.0]], "confidence": 0.9779810309410095, "text": "integrating multiple neural radiance field models [ 36 , 63 ].", "bbox": [63.0, 502.0, 375.0, 516.0]}, {"polygon": [[409.0, 505.0], [728.0, 505.0], [728.0, 520.0], [409.0, 520.0]], "confidence": 0.9833709001541138, "text": "visibility attention. However, unlike our method, all of the", "bbox": [409.0, 505.0, 728.0, 520.0]}, {"polygon": [[409.0, 520.0], [729.0, 520.0], [729.0, 535.0], [409.0, 535.0]], "confidence": 0.9816171526908875, "text": "above except OccNeRF assume the person is not occluded", "bbox": [409.0, 520.0, 729.0, 535.0]}, {"polygon": [[80.0, 521.0], [384.0, 520.0], [384.0, 535.0], [80.0, 537.0]], "confidence": 0.981510579586029, "text": "In this paper, we present a method to reconstruct both", "bbox": [80.0, 521.0, 384.0, 535.0]}, {"polygon": [[63.0, 537.0], [383.0, 537.0], [383.0, 551.0], [63.0, 551.0]], "confidence": 0.9814005494117737, "text": "the static world and multiple dynamically moving humans", "bbox": [63.0, 537.0, 383.0, 551.0]}, {"polygon": [[409.0, 537.0], [729.0, 537.0], [729.0, 551.0], [409.0, 551.0]], "confidence": 0.9834082722663879, "text": "and most of the body is shown in the video, which is rare in", "bbox": [409.0, 537.0, 729.0, 551.0]}, {"polygon": [[63.0, 553.0], [383.0, 553.0], [383.0, 568.0], [63.0, 568.0]], "confidence": 0.977777361869812, "text": "in 3D from a monocular video input, specifically focusing", "bbox": [63.0, 553.0, 383.0, 568.0]}, {"polygon": [[408.0, 553.0], [512.0, 553.0], [512.0, 567.0], [408.0, 567.0]], "confidence": 0.9478850364685059, "text": "in-the-wild videos.", "bbox": [408.0, 553.0, 512.0, 567.0]}, {"polygon": [[409.0, 568.0], [729.0, 568.0], [729.0, 583.0], [409.0, 583.0]], "confidence": 0.9806399941444397, "text": "Diffusion on 3D Tasks. After the recent breakthroughs", "bbox": [409.0, 568.0, 729.0, 583.0]}, {"polygon": [[63.0, 570.0], [384.0, 570.0], [384.0, 583.0], [63.0, 583.0]], "confidence": 0.9832094311714172, "text": "on scenarios with extremely limited and sparse observations.", "bbox": [63.0, 570.0, 384.0, 583.0]}, {"polygon": [[64.0, 585.0], [384.0, 585.0], [384.0, 600.0], [64.0, 600.0]], "confidence": 0.9827421307563782, "text": "To address this challenge, we represent both the world and", "bbox": [64.0, 585.0, 384.0, 600.0]}, {"polygon": [[408.0, 585.0], [729.0, 585.0], [729.0, 600.0], [408.0, 600.0]], "confidence": 0.9816263914108276, "text": "of diffusion models on image generation task [ 14 ], sev-", "bbox": [408.0, 585.0, 729.0, 600.0]}, {"polygon": [[63.0, 601.0], [384.0, 601.0], [384.0, 615.0], [63.0, 615.0]], "confidence": 0.9815515279769897, "text": "multiple humans in a common Gaussian splatting 3D repre-", "bbox": [63.0, 601.0, 384.0, 615.0]}, {"polygon": [[408.0, 601.0], [729.0, 601.0], [729.0, 615.0], [408.0, 615.0]], "confidence": 0.9818383455276489, "text": "eral methods suggest a way to use diffusion model on 3D", "bbox": [408.0, 601.0, 729.0, 615.0]}, {"polygon": [[63.0, 616.0], [383.0, 616.0], [383.0, 631.0], [63.0, 631.0]], "confidence": 0.9786309599876404, "text": "sentation [ 20 ]. Our approach allows to efficiently compose", "bbox": [63.0, 616.0, 383.0, 631.0]}, {"polygon": [[408.0, 616.0], [728.0, 616.0], [728.0, 631.0], [408.0, 631.0]], "confidence": 0.9540525674819946, "text": "tasks [9, 25, 28, 39, 47, 51, 66]. For example, RGBD2 [ 25 ]", "bbox": [408.0, 616.0, 728.0, 631.0]}, {"polygon": [[63.0, 633.0], [384.0, 633.0], [384.0, 647.0], [63.0, 647.0]], "confidence": 0.9820631146430969, "text": "them for novel view rendering or scene editing. Notably,", "bbox": [63.0, 633.0, 384.0, 647.0]}, {"polygon": [[408.0, 633.0], [728.0, 633.0], [728.0, 646.0], [408.0, 646.0]], "confidence": 0.9826518297195435, "text": "trains an RGB-D diffusion model to complete the unobserved", "bbox": [408.0, 633.0, 728.0, 646.0]}, {"polygon": [[63.0, 649.0], [384.0, 649.0], [384.0, 663.0], [63.0, 663.0]], "confidence": 0.9825400114059448, "text": "to tackle the scenarios with extremely limited and sparse", "bbox": [63.0, 649.0, 384.0, 663.0]}, {"polygon": [[409.0, 649.0], [729.0, 649.0], [729.0, 663.0], [409.0, 663.0]], "confidence": 0.9830362200737, "text": "area of a room using diffusion inpainting approach [ 30 ] and", "bbox": [409.0, 649.0, 729.0, 663.0]}, {"polygon": [[63.0, 665.0], [384.0, 665.0], [384.0, 679.0], [63.0, 679.0]], "confidence": 0.9810768961906433, "text": "observations in 3D human reconstruction, we introduce a", "bbox": [63.0, 665.0, 384.0, 679.0]}, {"polygon": [[409.0, 665.0], [729.0, 665.0], [729.0, 679.0], [409.0, 679.0]], "confidence": 0.976266622543335, "text": "DiffuStereo [ 47 ] trains diffusion-stereo network for higher", "bbox": [409.0, 665.0, 729.0, 679.0]}, {"polygon": [[408.0, 681.0], [729.0, 681.0], [729.0, 696.0], [408.0, 696.0]], "confidence": 0.9808390736579895, "text": "reconstruction quality in sparse multi-view settings. In par-", "bbox": [408.0, 681.0, 729.0, 696.0]}, {"polygon": [[63.0, 682.0], [384.0, 682.0], [384.0, 696.0], [63.0, 696.0]], "confidence": 0.9812883138656616, "text": "novel approach to optimize the 3D GS representation in a", "bbox": [63.0, 682.0, 384.0, 696.0]}, {"polygon": [[63.0, 696.0], [384.0, 696.0], [384.0, 711.0], [63.0, 711.0]], "confidence": 0.9812559485435486, "text": "canonical space by fusing the sparse cues in the common", "bbox": [63.0, 696.0, 384.0, 711.0]}, {"polygon": [[408.0, 696.0], [729.0, 696.0], [729.0, 711.0], [408.0, 711.0]], "confidence": 0.979334831237793, "text": "ticular, the SDS [ 39 ] method which leverages a pretrained", "bbox": [408.0, 696.0, 729.0, 711.0]}, {"polygon": [[63.0, 713.0], [384.0, 713.0], [384.0, 727.0], [63.0, 727.0]], "confidence": 0.9823381304740906, "text": "space. As a core idea, we leverage a pre-trained 2D diffusion", "bbox": [63.0, 713.0, 384.0, 727.0]}, {"polygon": [[408.0, 713.0], [730.0, 713.0], [730.0, 727.0], [408.0, 727.0]], "confidence": 0.9788762331008911, "text": "text-to-image diffusion model [ 45 ] has been applied widely,", "bbox": [408.0, 713.0, 730.0, 727.0]}, {"polygon": [[63.0, 729.0], [383.0, 729.0], [383.0, 744.0], [63.0, 744.0]], "confidence": 0.9819056987762451, "text": "model, equipped with Texture Inversion [ 10 ], to synthesize", "bbox": [63.0, 729.0, 383.0, 744.0]}, {"polygon": [[409.0, 729.0], [730.0, 729.0], [730.0, 744.0], [409.0, 744.0]], "confidence": 0.9811758399009705, "text": "such as text-to-3D [ 39 , 51 , 53 ] or single image-to-3D gen-", "bbox": [409.0, 729.0, 730.0, 744.0]}, {"polygon": [[63.0, 745.0], [384.0, 745.0], [384.0, 759.0], [63.0, 759.0]], "confidence": 0.9819450378417969, "text": "unseen views without losing the consistency with the ob-", "bbox": [63.0, 745.0, 384.0, 759.0]}, {"polygon": [[409.0, 745.0], [729.0, 745.0], [729.0, 759.0], [409.0, 759.0]], "confidence": 0.982164204120636, "text": "eration tasks [ 27 , 28 , 32 ]. However, the vanilla SDS loss", "bbox": [409.0, 745.0, 729.0, 759.0]}, {"polygon": [[63.0, 761.0], [384.0, 761.0], [384.0, 776.0], [63.0, 776.0]], "confidence": 0.9813744425773621, "text": "served 2D appearances [ 39 , 42 ]. Via thorough evaluations,", "bbox": [63.0, 761.0, 384.0, 776.0]}, {"polygon": [[408.0, 761.0], [729.0, 761.0], [729.0, 776.0], [408.0, 776.0]], "confidence": 0.9829427003860474, "text": "is not 3D consistent itself and prone to artifacts like the", "bbox": [408.0, 761.0, 729.0, 776.0]}, {"polygon": [[63.0, 777.0], [383.0, 777.0], [383.0, 791.0], [63.0, 791.0]], "confidence": 0.9783431887626648, "text": "we demonstrate that our approach successfully reconstructs", "bbox": [63.0, 777.0, 383.0, 791.0]}, {"polygon": [[409.0, 777.0], [728.0, 777.0], [728.0, 791.0], [409.0, 791.0]], "confidence": 0.9827203750610352, "text": "Janus effect. To improve the SDS loss, many techniques are", "bbox": [409.0, 777.0, 728.0, 791.0]}, {"polygon": [[63.0, 792.0], [383.0, 792.0], [383.0, 807.0], [63.0, 807.0]], "confidence": 0.9813762903213501, "text": "high-quality animatable 3D human avatars of dynamically", "bbox": [63.0, 792.0, 383.0, 807.0]}, {"polygon": [[409.0, 792.0], [730.0, 792.0], [730.0, 806.0], [409.0, 806.0]], "confidence": 0.9799906611442566, "text": "proposed such as leveraging 3D prior [ 28 ], fine-tuning diffu-", "bbox": [409.0, 792.0, 730.0, 806.0]}, {"polygon": [[408.0, 808.0], [729.0, 808.0], [729.0, 822.0], [408.0, 822.0]], "confidence": 0.9810941815376282, "text": "sion [ 53 ], giving better conditions [ 32 ] and using advanced", "bbox": [408.0, 808.0, 729.0, 822.0]}, {"polygon": [[63.0, 809.0], [383.0, 809.0], [383.0, 822.0], [63.0, 822.0]], "confidence": 0.9832517504692078, "text": "moving individuals from sparse and partial observations. The", "bbox": [63.0, 809.0, 383.0, 822.0]}, {"polygon": [[63.0, 824.0], [383.0, 824.0], [383.0, 838.0], [63.0, 838.0]], "confidence": 0.9821280241012573, "text": "animatable nature of our 3D human reconstruction outputs", "bbox": [63.0, 824.0, 383.0, 838.0]}, {"polygon": [[408.0, 824.0], [729.0, 824.0], [729.0, 839.0], [408.0, 839.0]], "confidence": 0.979896068572998, "text": "optimization schemes [ 5 , 6 , 6 , 27 ]. SDS loss is also applied", "bbox": [408.0, 824.0, 729.0, 839.0]}, {"polygon": [[63.0, 840.0], [383.0, 840.0], [383.0, 854.0], [63.0, 854.0]], "confidence": 0.9819358587265015, "text": "enables us to replay the observed motions of the humans", "bbox": [63.0, 840.0, 383.0, 854.0]}, {"polygon": [[408.0, 840.0], [728.0, 840.0], [728.0, 854.0], [408.0, 854.0]], "confidence": 0.9827609658241272, "text": "to make better 3D avatars recently [ 15 , 26 ]. However, there", "bbox": [408.0, 840.0, 728.0, 854.0]}, {"polygon": [[63.0, 856.0], [384.0, 856.0], [384.0, 871.0], [63.0, 871.0]], "confidence": 0.981939435005188, "text": "in novel views and edit the postures of the humans with", "bbox": [63.0, 856.0, 384.0, 871.0]}, {"polygon": [[408.0, 856.0], [729.0, 856.0], [729.0, 871.0], [408.0, 871.0]], "confidence": 0.9825438857078552, "text": "has been no trial on completing an imperfect reconstruction", "bbox": [408.0, 856.0, 729.0, 871.0]}, {"polygon": [[63.0, 872.0], [383.0, 872.0], [383.0, 886.0], [63.0, 886.0]], "confidence": 0.9824733138084412, "text": "arbitrary motions as well. Our contribution is summarized", "bbox": [63.0, 872.0, 383.0, 886.0]}, {"polygon": [[408.0, 872.0], [562.0, 872.0], [562.0, 886.0], [408.0, 886.0]], "confidence": 0.9647851586341858, "text": "of a human with a diffusion.", "bbox": [408.0, 872.0, 562.0, 886.0]}, {"polygon": [[63.0, 887.0], [383.0, 887.0], [383.0, 902.0], [63.0, 902.0]], "confidence": 0.9817831516265869, "text": "as: (1) representing both a 3D world and multiple humans", "bbox": [63.0, 887.0, 383.0, 902.0]}, {"polygon": [[408.0, 887.0], [728.0, 887.0], [728.0, 902.0], [408.0, 902.0]], "confidence": 0.9801470637321472, "text": "Compositional Human-Scene Reconstruction. Separating", "bbox": [408.0, 887.0, 728.0, 902.0]}, {"polygon": [[63.0, 904.0], [383.0, 904.0], [383.0, 918.0], [63.0, 918.0]], "confidence": 0.9795570373535156, "text": "in a common 3D GS representation for efficient composing", "bbox": [63.0, 904.0, 383.0, 918.0]}, {"polygon": [[409.0, 904.0], [729.0, 904.0], [729.0, 917.0], [409.0, 917.0]], "confidence": 0.9827880859375, "text": "4D scenes into static backgrounds and dynamic objects is a", "bbox": [409.0, 904.0, 729.0, 917.0]}, {"polygon": [[63.0, 920.0], [384.0, 920.0], [384.0, 934.0], [63.0, 934.0]], "confidence": 0.9828667640686035, "text": "and rendering; (2) reconstructing and canonicalizing animat-", "bbox": [63.0, 920.0, 384.0, 934.0]}, {"polygon": [[409.0, 920.0], [728.0, 920.0], [728.0, 934.0], [409.0, 934.0]], "confidence": 0.9777958989143372, "text": "common approach in the 4D reconstruction problem. The", "bbox": [409.0, 920.0, 728.0, 934.0]}, {"polygon": [[64.0, 936.0], [383.0, 936.0], [383.0, 951.0], [64.0, 951.0]], "confidence": 0.9822973012924194, "text": "able 3D humans from sparse and partial 2D observations by", "bbox": [64.0, 936.0, 383.0, 951.0]}, {"polygon": [[409.0, 936.0], [728.0, 936.0], [728.0, 951.0], [409.0, 951.0]], "confidence": 0.9817455410957336, "text": "static-dynamic separation can be done by prior knowledge", "bbox": [409.0, 936.0, 728.0, 951.0]}, {"polygon": [[392.0, 976.0], [401.0, 976.0], [401.0, 990.0], [392.0, 990.0]], "confidence": 0.47998034954071045, "text": "2", "bbox": [392.0, 976.0, 401.0, 990.0]}], "languages": ["en"], "image_bbox": [0.0, 0.0, 816.0, 1056.0], "page": 2}, {"text_lines": [{"polygon": [[63.0, 96.0], [384.0, 96.0], [384.0, 111.0], [63.0, 111.0]], "confidence": 0.9826997518539429, "text": "on the targets [ 24 , 36 ], such as cars and pedestrians are dy-", "bbox": [63.0, 96.0, 384.0, 111.0]}, {"polygon": [[461.0, 102.0], [595.0, 102.0], [595.0, 115.0], [461.0, 115.0]], "confidence": 0.9614650011062622, "text": "Compositional 3D Gaussians", "bbox": [461.0, 102.0, 595.0, 115.0]}, {"polygon": [[63.0, 113.0], [382.0, 113.0], [382.0, 127.0], [63.0, 127.0]], "confidence": 0.9795976877212524, "text": "namic, or can be performed automatically by minimizing", "bbox": [63.0, 113.0, 382.0, 127.0]}, {"polygon": [[422.0, 124.0], [445.0, 124.0], [445.0, 135.0], [422.0, 135.0]], "confidence": 0.7249171137809753, "text": "SfM", "bbox": [422.0, 124.0, 445.0, 135.0]}, {"polygon": [[656.0, 124.0], [718.0, 124.0], [718.0, 136.0], [656.0, 136.0]], "confidence": 0.9313872456550598, "text": "Reconstruction", "bbox": [656.0, 124.0, 718.0, 136.0]}, {"polygon": [[478.0, 125.0], [530.0, 125.0], [530.0, 136.0], [478.0, 136.0]], "confidence": 0.8246250152587891, "text": "Bckground", "bbox": [478.0, 125.0, 530.0, 136.0]}, {"polygon": [[63.0, 129.0], [383.0, 129.0], [383.0, 143.0], [63.0, 143.0]], "confidence": 0.9805789589881897, "text": "a certain energy term [ 55 ]. Recent human reconstruction", "bbox": [63.0, 129.0, 383.0, 143.0]}, {"polygon": [[420.0, 135.0], [446.0, 135.0], [446.0, 146.0], [420.0, 146.0]], "confidence": 0.8538851141929626, "text": "Points", "bbox": [420.0, 135.0, 446.0, 146.0]}, {"polygon": [[477.0, 138.0], [531.0, 138.0], [531.0, 148.0], [477.0, 148.0]], "confidence": 0.9044017791748047, "text": "3D Gaussians", "bbox": [477.0, 138.0, 531.0, 148.0]}, {"polygon": [[675.0, 138.0], [697.0, 138.0], [697.0, 148.0], [675.0, 148.0]], "confidence": 0.7959439754486084, "text": "Loss", "bbox": [675.0, 138.0, 697.0, 148.0]}, {"polygon": [[63.0, 145.0], [383.0, 145.0], [383.0, 160.0], [63.0, 160.0]], "confidence": 0.9814316630363464, "text": "methods also use compositional approaches to reconstruct", "bbox": [63.0, 145.0, 383.0, 160.0]}, {"polygon": [[63.0, 161.0], [384.0, 161.0], [384.0, 175.0], [63.0, 175.0]], "confidence": 0.9826542735099792, "text": "a human from videos [ 12 , 18 , 48 , 63 ]. For monocular recon-", "bbox": [63.0, 161.0, 384.0, 175.0]}, {"polygon": [[570.0, 161.0], [605.0, 161.0], [605.0, 171.0], [570.0, 171.0]], "confidence": 0.8873652219772339, "text": "Skeletal", "bbox": [570.0, 161.0, 605.0, 171.0]}, {"polygon": [[420.0, 162.0], [447.0, 162.0], [447.0, 174.0], [420.0, 174.0]], "confidence": 0.8732858896255493, "text": "Initial", "bbox": [420.0, 162.0, 447.0, 174.0]}, {"polygon": [[488.0, 163.0], [518.0, 163.0], [518.0, 174.0], [488.0, 174.0]], "confidence": 0.8537864089012146, "text": "Person", "bbox": [488.0, 163.0, 518.0, 174.0]}, {"polygon": [[666.0, 168.0], [707.0, 168.0], [707.0, 180.0], [666.0, 180.0]], "confidence": 0.8822599053382874, "text": "SDS Loss", "bbox": [666.0, 168.0, 707.0, 180.0]}, {"polygon": [[563.0, 173.0], [609.0, 173.0], [609.0, 182.0], [563.0, 182.0]], "confidence": 0.9152178168296814, "text": "Deformation", "bbox": [563.0, 173.0, 609.0, 182.0]}, {"polygon": [[420.0, 174.0], [446.0, 174.0], [446.0, 184.0], [420.0, 184.0]], "confidence": 0.7957571148872375, "text": "SMPL", "bbox": [420.0, 174.0, 446.0, 184.0]}, {"polygon": [[476.0, 174.0], [524.0, 174.0], [524.0, 185.0], [476.0, 185.0]], "confidence": 0.9134556651115417, "text": "3D Gaussian", "bbox": [476.0, 174.0, 524.0, 185.0]}, {"polygon": [[63.0, 176.0], [382.0, 176.0], [382.0, 191.0], [63.0, 191.0]], "confidence": 0.9825465679168701, "text": "struction, Neuman [ 18 ] and Vid2avatar [ 12 ] use two separate", "bbox": [63.0, 176.0, 382.0, 191.0]}, {"polygon": [[667.0, 181.0], [706.0, 181.0], [706.0, 191.0], [667.0, 191.0]], "confidence": 0.8908775448799133, "text": "(Sec 4.4)", "bbox": [667.0, 181.0, 706.0, 191.0]}, {"polygon": [[567.0, 184.0], [608.0, 184.0], [608.0, 195.0], [567.0, 195.0]], "confidence": 0.894172191619873, "text": "(Sec 4.3)", "bbox": [567.0, 184.0, 608.0, 195.0]}, {"polygon": [[418.0, 186.0], [449.0, 186.0], [449.0, 197.0], [418.0, 197.0]], "confidence": 0.8866444826126099, "text": "vertices", "bbox": [418.0, 186.0, 449.0, 197.0]}, {"polygon": [[483.0, 186.0], [523.0, 186.0], [523.0, 197.0], [483.0, 197.0]], "confidence": 0.8908337354660034, "text": "(Sec 4.1)", "bbox": [483.0, 186.0, 523.0, 197.0]}, {"polygon": [[63.0, 193.0], [382.0, 193.0], [382.0, 206.0], [63.0, 206.0]], "confidence": 0.9828646779060364, "text": "implicit functions each of which represents background and", "bbox": [63.0, 193.0, 382.0, 206.0]}, {"polygon": [[63.0, 208.0], [384.0, 208.0], [384.0, 223.0], [63.0, 223.0]], "confidence": 0.9823810458183289, "text": "person respectively. While they allow the model to recon-", "bbox": [63.0, 208.0, 384.0, 223.0]}, {"polygon": [[539.0, 212.0], [599.0, 212.0], [599.0, 224.0], [539.0, 224.0]], "confidence": 0.8768922686576843, "text": "SMPL Pose", "bbox": [539.0, 212.0, 599.0, 224.0]}, {"polygon": [[63.0, 225.0], [383.0, 225.0], [383.0, 239.0], [63.0, 239.0]], "confidence": 0.9833202958106995, "text": "struct a person regardless of the person's mask quality, these", "bbox": [63.0, 225.0, 383.0, 239.0]}, {"polygon": [[414.0, 230.0], [722.0, 230.0], [722.0, 244.0], [414.0, 244.0]], "confidence": 0.9767916798591614, "text": "Figure 2. Method overview. Overview of our pipeline. (Sec. 4 ).", "bbox": [414.0, 230.0, 722.0, 244.0]}, {"polygon": [[63.0, 241.0], [384.0, 241.0], [384.0, 256.0], [63.0, 256.0]], "confidence": 0.982756495475769, "text": "models cannot handle occlusion and multiple people at once.", "bbox": [63.0, 241.0, 384.0, 256.0]}, {"polygon": [[63.0, 256.0], [384.0, 256.0], [384.0, 271.0], [63.0, 271.0]], "confidence": 0.9800180792808533, "text": "In a multi-view setting, Shuai et al. [48] tackles more prac-", "bbox": [63.0, 256.0, 384.0, 271.0]}, {"polygon": [[409.0, 265.0], [580.0, 265.0], [580.0, 279.0], [409.0, 279.0]], "confidence": 0.9634424448013306, "text": "α -blended rendering as follows:", "bbox": [409.0, 265.0, 580.0, 279.0]}, {"polygon": [[63.0, 273.0], [384.0, 273.0], [384.0, 287.0], [63.0, 287.0]], "confidence": 0.983703076839447, "text": "tical situations where multiple people interact with objects.", "bbox": [63.0, 273.0, 384.0, 287.0]}, {"polygon": [[63.0, 289.0], [384.0, 289.0], [384.0, 304.0], [63.0, 304.0]], "confidence": 0.9821116924285889, "text": "It models each person, object, and background with a Neu-", "bbox": [63.0, 289.0, 384.0, 304.0]}, {"polygon": [[708.0, 302.0], [729.0, 302.0], [729.0, 317.0], [708.0, 317.0]], "confidence": 0.7176854610443115, "text": "(3)", "bbox": [708.0, 302.0, 729.0, 317.0]}, {"polygon": [[63.0, 304.0], [383.0, 304.0], [383.0, 319.0], [63.0, 319.0]], "confidence": 0.9765306711196899, "text": "ralBody [ 38 ] and NeRF [ 33 ] respectively and renders them", "bbox": [63.0, 304.0, 383.0, 319.0]}, {"polygon": [[485.0, 304.0], [517.0, 304.0], [517.0, 316.0], [485.0, 316.0]], "confidence": 0.6202574968338013, "text": "C(p)", "bbox": [485.0, 304.0, 517.0, 316.0]}, {"polygon": [[63.0, 320.0], [384.0, 320.0], [384.0, 335.0], [63.0, 335.0]], "confidence": 0.9779790639877319, "text": "compositionally through ST-NeRF [ 63 ] pipeline. However,", "bbox": [63.0, 320.0, 384.0, 335.0]}, {"polygon": [[63.0, 336.0], [382.0, 336.0], [382.0, 351.0], [63.0, 351.0]], "confidence": 0.9792641401290894, "text": "it requires 8 multi-view videos as input, which is unavailable", "bbox": [63.0, 336.0, 382.0, 351.0]}, {"polygon": [[710.0, 339.0], [730.0, 339.0], [730.0, 352.0], [710.0, 352.0]], "confidence": 0.7213289737701416, "text": "(4)", "bbox": [710.0, 339.0, 730.0, 352.0]}, {"polygon": [[63.0, 351.0], [197.0, 352.0], [197.0, 367.0], [63.0, 366.0]], "confidence": 0.9577762484550476, "text": "in in-the-wild situations.", "bbox": [63.0, 351.0, 197.0, 367.0]}, {"polygon": [[409.0, 362.0], [729.0, 363.0], [729.0, 380.0], [409.0, 378.0]], "confidence": 0.9670778512954712, "text": "where g 2D is the weight after the 2D projection of 3D Gaus-", "bbox": [409.0, 362.0, 729.0, 380.0]}, {"polygon": [[409.0, 380.0], [728.0, 380.0], [728.0, 394.0], [409.0, 394.0]], "confidence": 0.9774503111839294, "text": "sian g i to the image plane, and we use the Jacobian of the", "bbox": [409.0, 380.0, 728.0, 394.0]}, {"polygon": [[63.0, 392.0], [175.0, 392.0], [175.0, 409.0], [63.0, 409.0]], "confidence": 0.9389068484306335, "text": "3. Preliminaries", "bbox": [63.0, 392.0, 175.0, 409.0]}, {"polygon": [[409.0, 396.0], [729.0, 396.0], [729.0, 410.0], [409.0, 410.0]], "confidence": 0.9779194593429565, "text": "affine approximation of the projective transformation, fol-", "bbox": [409.0, 396.0, 729.0, 410.0]}, {"polygon": [[408.0, 411.0], [728.0, 411.0], [728.0, 426.0], [408.0, 426.0]], "confidence": 0.9813011288642883, "text": "lowing previous approaches [ 20 , 68 ]. As the output of 3D", "bbox": [408.0, 411.0, 728.0, 426.0]}, {"polygon": [[80.0, 422.0], [384.0, 422.0], [384.0, 436.0], [80.0, 436.0]], "confidence": 0.9800398349761963, "text": "In our method, we use 3D Gaussian Splatting [ 20 ] to", "bbox": [80.0, 422.0, 384.0, 436.0]}, {"polygon": [[409.0, 428.0], [728.0, 428.0], [728.0, 442.0], [409.0, 442.0]], "confidence": 0.9826200604438782, "text": "scene reconstruction, we obtain the parameters of 3D Gaus-", "bbox": [409.0, 428.0, 728.0, 442.0]}, {"polygon": [[63.0, 438.0], [382.0, 438.0], [382.0, 452.0], [63.0, 452.0]], "confidence": 0.981939971446991, "text": "represent 4D scenes, and use Score Distillation Sampling", "bbox": [63.0, 438.0, 382.0, 452.0]}, {"polygon": [[409.0, 444.0], [729.0, 444.0], [729.0, 460.0], [409.0, 460.0]], "confidence": 0.9539567828178406, "text": "sians G = { G i } i = 1 by optimizing them with reconstruction", "bbox": [409.0, 444.0, 729.0, 460.0]}, {"polygon": [[63.0, 454.0], [384.0, 454.0], [384.0, 468.0], [63.0, 468.0]], "confidence": 0.9825937151908875, "text": "(SDS) as a tool to estimate unseen human body parts. Here,", "bbox": [63.0, 454.0, 384.0, 468.0]}, {"polygon": [[409.0, 460.0], [684.0, 460.0], [684.0, 475.0], [409.0, 475.0]], "confidence": 0.9811125993728638, "text": "loss which is calculated from the rendering Eq. ( 3 ).", "bbox": [409.0, 460.0, 684.0, 475.0]}, {"polygon": [[63.0, 470.0], [357.0, 470.0], [357.0, 484.0], [63.0, 484.0]], "confidence": 0.9733080863952637, "text": "we provide an overview of these preliminary concepts.", "bbox": [63.0, 470.0, 357.0, 484.0]}, {"polygon": [[409.0, 483.0], [608.0, 483.0], [608.0, 498.0], [409.0, 498.0]], "confidence": 0.9685332775115967, "text": "3.2. Score Distillation Sampling", "bbox": [409.0, 483.0, 608.0, 498.0]}, {"polygon": [[63.0, 505.0], [232.0, 505.0], [232.0, 520.0], [63.0, 520.0]], "confidence": 0.9610393047332764, "text": "3.1. 3D Gaussian Splatting", "bbox": [63.0, 505.0, 232.0, 520.0]}, {"polygon": [[425.0, 507.0], [728.0, 507.0], [728.0, 522.0], [425.0, 522.0]], "confidence": 0.9799782633781433, "text": "Score Distillation Sampling (SDS) method [ 39 ] is an", "bbox": [425.0, 507.0, 728.0, 522.0]}, {"polygon": [[409.0, 524.0], [729.0, 524.0], [729.0, 538.0], [409.0, 538.0]], "confidence": 0.9833153486251831, "text": "approach that leverages the prior knowledge underlying text-", "bbox": [409.0, 524.0, 729.0, 538.0]}, {"polygon": [[80.0, 532.0], [384.0, 532.0], [384.0, 547.0], [80.0, 547.0]], "confidence": 0.9811541438102722, "text": "3D Gaussian Splatting (3D-GS) is an explicit 3D repre-", "bbox": [80.0, 532.0, 384.0, 547.0]}, {"polygon": [[408.0, 540.0], [729.0, 540.0], [729.0, 554.0], [408.0, 554.0]], "confidence": 0.9815149903297424, "text": "to-image (T2I) diffusion models to generate 3D content.", "bbox": [408.0, 540.0, 729.0, 554.0]}, {"polygon": [[63.0, 549.0], [382.0, 549.0], [382.0, 563.0], [63.0, 563.0]], "confidence": 0.9731593132019043, "text": "sentation to model a radiance field of a static 3D scene", "bbox": [63.0, 549.0, 382.0, 563.0]}, {"polygon": [[409.0, 556.0], [728.0, 556.0], [728.0, 571.0], [409.0, 571.0]], "confidence": 0.9713152050971985, "text": "SDS optimizes any differentiable 3D representation Θ by", "bbox": [409.0, 556.0, 728.0, 571.0]}, {"polygon": [[63.0, 565.0], [383.0, 565.0], [383.0, 579.0], [63.0, 579.0]], "confidence": 0.9818823337554932, "text": "with a set of 3D Gaussians and their attributes [ 20 ]. A", "bbox": [63.0, 565.0, 383.0, 579.0]}, {"polygon": [[409.0, 571.0], [728.0, 571.0], [728.0, 586.0], [409.0, 586.0]], "confidence": 0.9785571098327637, "text": "aligning rendered output { I } from arbitrary views to be on", "bbox": [409.0, 571.0, 728.0, 586.0]}, {"polygon": [[64.0, 581.0], [384.0, 581.0], [384.0, 595.0], [64.0, 595.0]], "confidence": 0.9797932505607605, "text": "3D static scene can be modeled by a set of 3D Gaus-", "bbox": [64.0, 581.0, 384.0, 595.0]}, {"polygon": [[408.0, 587.0], [728.0, 587.0], [728.0, 602.0], [408.0, 602.0]], "confidence": 0.980056881904602, "text": "the distribution of diffusion model φ . This can be achieved", "bbox": [408.0, 587.0, 728.0, 602.0]}, {"polygon": [[63.0, 596.0], [382.0, 596.0], [382.0, 613.0], [63.0, 613.0]], "confidence": 0.9477206468582153, "text": "sians { G i } i =1 where the i -th Gaussian is represented by", "bbox": [63.0, 596.0, 382.0, 613.0]}, {"polygon": [[408.0, 604.0], [728.0, 604.0], [728.0, 618.0], [408.0, 618.0]], "confidence": 0.9704763889312744, "text": "by minimizing the residual between noise ε , which perturbs", "bbox": [408.0, 604.0, 728.0, 618.0]}, {"polygon": [[63.0, 613.0], [383.0, 613.0], [383.0, 628.0], [63.0, 628.0]], "confidence": 0.9192068576812744, "text": "G i  = { μ i , q i , s i , c i , o i } , where μ ∈ R 3 is the Gaussian", "bbox": [63.0, 613.0, 383.0, 628.0]}, {"polygon": [[410.0, 620.0], [728.0, 620.0], [728.0, 635.0], [410.0, 635.0]], "confidence": 0.9356560707092285, "text": "z into z r , and predicted noise c ( z r ; y , τ ) where z is a latent", "bbox": [410.0, 620.0, 728.0, 635.0]}, {"polygon": [[63.0, 629.0], [382.0, 629.0], [382.0, 643.0], [63.0, 643.0]], "confidence": 0.9684371948242188, "text": "center, s ∈ R 3 and q ∈ SO (3) are respectively the scaling", "bbox": [63.0, 629.0, 382.0, 643.0]}, {"polygon": [[409.0, 636.0], [728.0, 636.0], [728.0, 650.0], [409.0, 650.0]], "confidence": 0.9788416624069214, "text": "of { I } encoded by VAE of latent diffusion model [ 43 ]. By", "bbox": [409.0, 636.0, 728.0, 650.0]}, {"polygon": [[63.0, 645.0], [382.0, 645.0], [382.0, 660.0], [63.0, 660.0]], "confidence": 0.9795846343040466, "text": "factor and the rotation represented in quaternion to define", "bbox": [63.0, 645.0, 382.0, 660.0]}, {"polygon": [[408.0, 652.0], [728.0, 652.0], [728.0, 667.0], [408.0, 667.0]], "confidence": 0.9769851565361023, "text": "omitting gradient through diffusion model φ , the SDS loss", "bbox": [408.0, 652.0, 728.0, 667.0]}, {"polygon": [[63.0, 659.0], [383.0, 659.0], [383.0, 674.0], [63.0, 674.0]], "confidence": 0.9610505104064941, "text": "the covariance matrix Σ ∈ R 3×3 , c i ∈ R 3 is the color, and", "bbox": [63.0, 659.0, 383.0, 674.0]}, {"polygon": [[409.0, 668.0], [548.0, 668.0], [548.0, 682.0], [409.0, 682.0]], "confidence": 0.9601989984512329, "text": "can be written as follows:", "bbox": [409.0, 668.0, 548.0, 682.0]}, {"polygon": [[63.0, 675.0], [382.0, 675.0], [382.0, 690.0], [63.0, 690.0]], "confidence": 0.9512571096420288, "text": "o i ∈ R is opacity. For a 3D query location x ∈ R 3 , its", "bbox": [63.0, 675.0, 382.0, 690.0]}, {"polygon": [[648.0, 690.0], [687.0, 690.0], [687.0, 702.0], [648.0, 702.0]], "confidence": 0.3205473721027374, "text": "I I I", "bbox": [648.0, 690.0, 687.0, 702.0]}, {"polygon": [[63.0, 692.0], [280.0, 692.0], [280.0, 706.0], [63.0, 706.0]], "confidence": 0.9531391859054565, "text": "Gaussian weight g( ) is represented as:", "bbox": [63.0, 692.0, 280.0, 706.0]}, {"polygon": [[421.0, 697.0], [649.0, 697.0], [649.0, 713.0], [421.0, 713.0]], "confidence": 0.7028752565383911, "text": "VOLSDS = Et,e |w(T)(et)(z,;y,T) - E)", "bbox": [421.0, 697.0, 649.0, 713.0]}, {"polygon": [[708.0, 697.0], [729.0, 697.0], [729.0, 711.0], [708.0, 711.0]], "confidence": 0.718276858329773, "text": "(5)", "bbox": [708.0, 697.0, 729.0, 711.0]}, {"polygon": [[409.0, 729.0], [728.0, 729.0], [728.0, 743.0], [409.0, 743.0]], "confidence": 0.9493598937988281, "text": "where z + is a noised latent with time step τ and y is text", "bbox": [409.0, 729.0, 728.0, 743.0]}, {"polygon": [[140.0, 730.0], [302.0, 723.0], [302.0, 739.0], [140.0, 747.0]], "confidence": 0.6521554589271545, "text": "g(x) = e- 1(x- p) T E- 1(x- p)", "bbox": [140.0, 730.0, 302.0, 739.0]}, {"polygon": [[364.0, 732.0], [384.0, 732.0], [384.0, 746.0], [364.0, 746.0]], "confidence": 0.714425802230835, "text": "(1)", "bbox": [364.0, 732.0, 384.0, 746.0]}, {"polygon": [[409.0, 745.0], [728.0, 745.0], [728.0, 759.0], [409.0, 759.0]], "confidence": 0.959891140460968, "text": "prompt conditioning diffusion model φ . The ω(τ) denotes", "bbox": [409.0, 745.0, 728.0, 759.0]}, {"polygon": [[408.0, 761.0], [729.0, 761.0], [729.0, 776.0], [408.0, 776.0]], "confidence": 0.9791329503059387, "text": "the weighting function defined by the scheduler of the diffu-", "bbox": [408.0, 761.0, 729.0, 776.0]}, {"polygon": [[63.0, 766.0], [383.0, 764.0], [383.0, 780.0], [63.0, 782.0]], "confidence": 0.9627398252487183, "text": "where the symmetric 3D covariance matrix Σ ∈ R 3×3 is", "bbox": [63.0, 766.0, 383.0, 780.0]}, {"polygon": [[409.0, 777.0], [474.0, 777.0], [474.0, 791.0], [409.0, 791.0]], "confidence": 0.914858341217041, "text": "sion model.", "bbox": [409.0, 777.0, 474.0, 791.0]}, {"polygon": [[63.0, 782.0], [145.0, 782.0], [145.0, 797.0], [63.0, 797.0]], "confidence": 0.9319380521774292, "text": "represented by", "bbox": [63.0, 782.0, 145.0, 797.0]}, {"polygon": [[409.0, 806.0], [482.0, 806.0], [482.0, 821.0], [409.0, 821.0]], "confidence": 0.8969640731811523, "text": "4. Method", "bbox": [409.0, 806.0, 482.0, 821.0]}, {"polygon": [[176.0, 816.0], [265.0, 816.0], [265.0, 832.0], [176.0, 832.0]], "confidence": 0.7740792632102966, "text": "Σ = RSST RT", "bbox": [176.0, 816.0, 265.0, 832.0]}, {"polygon": [[364.0, 818.0], [384.0, 818.0], [384.0, 833.0], [364.0, 833.0]], "confidence": 0.7118839025497437, "text": "(2)", "bbox": [364.0, 818.0, 384.0, 833.0]}, {"polygon": [[409.0, 831.0], [497.0, 831.0], [497.0, 846.0], [409.0, 846.0]], "confidence": 0.9232919812202454, "text": "4.1. Overview", "bbox": [409.0, 831.0, 497.0, 846.0]}, {"polygon": [[65.0, 852.0], [383.0, 852.0], [383.0, 867.0], [65.0, 867.0]], "confidence": 0.9640634059906006, "text": "R = quat2rot(q) is a rotation matrix converted from q , and", "bbox": [65.0, 852.0, 383.0, 867.0]}, {"polygon": [[425.0, 856.0], [729.0, 856.0], [729.0, 871.0], [425.0, 871.0]], "confidence": 0.9790054559707642, "text": "Our model reconstructs dynamically moving multi-", "bbox": [425.0, 856.0, 729.0, 871.0]}, {"polygon": [[65.0, 869.0], [361.0, 869.0], [361.0, 884.0], [65.0, 884.0]], "confidence": 0.9711202383041382, "text": "S = diag( s ) is a diagonal matrix from scaling factor s .", "bbox": [65.0, 869.0, 361.0, 884.0]}, {"polygon": [[409.0, 872.0], [728.0, 872.0], [728.0, 886.0], [409.0, 886.0]], "confidence": 0.982067883014679, "text": "humans and the static background jointly from a casually", "bbox": [409.0, 872.0, 728.0, 886.0]}, {"polygon": [[80.0, 887.0], [382.0, 887.0], [382.0, 903.0], [80.0, 903.0]], "confidence": 0.9565994739532471, "text": "3D-GS rasterizes these 3D Gaussians { G i } i =1 by sorting", "bbox": [80.0, 887.0, 382.0, 903.0]}, {"polygon": [[408.0, 887.0], [726.0, 887.0], [726.0, 902.0], [408.0, 902.0]], "confidence": 0.9632439613342285, "text": "captured monocular video. Our system takes, as input, T ′", "bbox": [408.0, 887.0, 726.0, 902.0]}, {"polygon": [[63.0, 904.0], [383.0, 904.0], [383.0, 918.0], [63.0, 918.0]], "confidence": 0.982478678226471, "text": "them in depth order in camera space and projecting them to", "bbox": [63.0, 904.0, 383.0, 918.0]}, {"polygon": [[409.0, 904.0], [729.0, 904.0], [729.0, 920.0], [409.0, 920.0]], "confidence": 0.9602314829826355, "text": "frames of images { I t } t = 1 ... T with corresponding camera pa-", "bbox": [409.0, 904.0, 729.0, 920.0]}, {"polygon": [[63.0, 919.0], [383.0, 919.0], [383.0, 934.0], [63.0, 934.0]], "confidence": 0.9811850190162659, "text": "the image plane. If N number of Gaussians are projected", "bbox": [63.0, 919.0, 383.0, 934.0]}, {"polygon": [[408.0, 920.0], [729.0, 918.0], [729.0, 934.0], [408.0, 936.0]], "confidence": 0.9569217562675476, "text": "rameters {P t } t = 1 ... T , and outputs the 3D scenes in the rep-", "bbox": [408.0, 920.0, 729.0, 934.0]}, {"polygon": [[409.0, 933.0], [730.0, 935.0], [730.0, 952.0], [409.0, 950.0]], "confidence": 0.9317939877510071, "text": "resentation of 3D Gaussian Splatting GBG and { G h } j = 1 ... N ,", "bbox": [409.0, 933.0, 730.0, 952.0]}, {"polygon": [[63.0, 936.0], [382.0, 936.0], [382.0, 951.0], [63.0, 951.0]], "confidence": 0.9638482928276062, "text": "on 2D location P ∈ R 2 , the pixel color C ( P ) is given by", "bbox": [63.0, 936.0, 382.0, 951.0]}], "languages": ["en"], "image_bbox": [0.0, 0.0, 816.0, 1056.0], "page": 3}, {"text_lines": [{"polygon": [[409.0, 95.0], [671.0, 95.0], [671.0, 111.0], [409.0, 111.0]], "confidence": 0.9755452871322632, "text": "4.2. Initializing and Densifying Gaussians", "bbox": [409.0, 95.0, 671.0, 111.0]}, {"polygon": [[249.0, 99.0], [282.0, 99.0], [282.0, 106.0], [249.0, 106.0]], "confidence": 0.9058555364608765, "text": "Input View", "bbox": [249.0, 99.0, 282.0, 106.0]}, {"polygon": [[425.0, 120.0], [729.0, 120.0], [729.0, 135.0], [425.0, 135.0]], "confidence": 0.9793883562088013, "text": "To represent the background and humans via 3D-GSs,", "bbox": [425.0, 120.0, 729.0, 135.0]}, {"polygon": [[409.0, 137.0], [728.0, 137.0], [728.0, 151.0], [409.0, 151.0]], "confidence": 0.9834288358688354, "text": "we initialize each representation via the available cues. The", "bbox": [409.0, 137.0, 728.0, 151.0]}, {"polygon": [[408.0, 151.0], [728.0, 151.0], [728.0, 167.0], [408.0, 167.0]], "confidence": 0.9793497920036316, "text": "background Gaussians GBG are initialized with point cloud", "bbox": [408.0, 151.0, 728.0, 167.0]}, {"polygon": [[408.0, 168.0], [728.0, 168.0], [728.0, 183.0], [408.0, 183.0]], "confidence": 0.9789772629737854, "text": "obtained by Structure-from-Motion (SfM) [ 46 ]. In the cases", "bbox": [408.0, 168.0, 728.0, 183.0]}, {"polygon": [[408.0, 185.0], [729.0, 185.0], [729.0, 199.0], [408.0, 199.0]], "confidence": 0.9775341153144836, "text": "of a fixed camera input where SfM cannot be applicable,", "bbox": [408.0, 185.0, 729.0, 199.0]}, {"polygon": [[408.0, 201.0], [728.0, 201.0], [728.0, 215.0], [408.0, 215.0]], "confidence": 0.9775980710983276, "text": "we assume that the background is a large 3D sphere with", "bbox": [408.0, 201.0, 728.0, 215.0]}, {"polygon": [[408.0, 218.0], [729.0, 218.0], [729.0, 231.0], [408.0, 231.0]], "confidence": 0.9809715151786804, "text": "background texture, centered on the mean position of hu-", "bbox": [408.0, 218.0, 729.0, 231.0]}, {"polygon": [[408.0, 233.0], [728.0, 233.0], [728.0, 248.0], [408.0, 248.0]], "confidence": 0.9800696969032288, "text": "mans. We represent a j -th human via 3D-GS in a canonical", "bbox": [408.0, 233.0, 728.0, 248.0]}, {"polygon": [[109.0, 236.0], [172.0, 236.0], [172.0, 249.0], [109.0, 249.0]], "confidence": 0.7853819131851196, "text": "() wo/ L SDS", "bbox": [109.0, 236.0, 172.0, 249.0]}, {"polygon": [[271.0, 236.0], [330.0, 236.0], [330.0, 249.0], [271.0, 249.0]], "confidence": 0.8199310302734375, "text": "(b) w/ L SDS", "bbox": [271.0, 236.0, 330.0, 249.0]}, {"polygon": [[409.0, 249.0], [727.0, 249.0], [727.0, 264.0], [409.0, 264.0]], "confidence": 0.9451732635498047, "text": "space (denoted as subscript c ), G i ,c , which is initialized by", "bbox": [409.0, 249.0, 727.0, 264.0]}, {"polygon": [[63.0, 260.0], [384.0, 260.0], [384.0, 274.0], [63.0, 274.0]], "confidence": 0.9791196584701538, "text": "Figure 3. Failure examples of optimizing 3D-GS naively. (a)", "bbox": [63.0, 260.0, 384.0, 274.0]}, {"polygon": [[408.0, 264.0], [727.0, 264.0], [727.0, 279.0], [408.0, 279.0]], "confidence": 0.9562351107597351, "text": "the vertices of A-posed SMPL mesh V ( β j , θ c ), where β j", "bbox": [408.0, 264.0, 727.0, 279.0]}, {"polygon": [[63.0, 275.0], [383.0, 275.0], [383.0, 289.0], [63.0, 289.0]], "confidence": 0.9830982089042664, "text": "shows that naively optimizing 3D-GS suffers from artifacts shaped", "bbox": [63.0, 275.0, 383.0, 289.0]}, {"polygon": [[409.0, 281.0], [728.0, 281.0], [728.0, 295.0], [409.0, 295.0]], "confidence": 0.9799665212631226, "text": "and θ c are the SMPL shape and canonical pose parameters", "bbox": [409.0, 281.0, 728.0, 295.0]}, {"polygon": [[63.0, 290.0], [383.0, 290.0], [383.0, 304.0], [63.0, 304.0]], "confidence": 0.9845085144042969, "text": "like a hedgehog in unseen view and input view. (b) shows that our", "bbox": [63.0, 290.0, 383.0, 304.0]}, {"polygon": [[408.0, 296.0], [729.0, 296.0], [729.0, 311.0], [408.0, 311.0]], "confidence": 0.9818294048309326, "text": "respectively, regressed using a monocular 3D pose regres-", "bbox": [408.0, 296.0, 729.0, 311.0]}, {"polygon": [[63.0, 306.0], [382.0, 306.0], [382.0, 319.0], [63.0, 319.0]], "confidence": 0.9783031344413757, "text": "SDS loss effectively removes the artifacts observed in both input", "bbox": [63.0, 306.0, 382.0, 319.0]}, {"polygon": [[409.0, 313.0], [729.0, 313.0], [729.0, 326.0], [409.0, 326.0]], "confidence": 0.9755707383155823, "text": "sor [ 40 , 49 ]. The color and opacity are set in grey and 0.9", "bbox": [409.0, 313.0, 729.0, 326.0]}, {"polygon": [[63.0, 320.0], [152.0, 320.0], [152.0, 333.0], [63.0, 333.0]], "confidence": 0.9431032538414001, "text": "and unseen views.", "bbox": [63.0, 320.0, 152.0, 333.0]}, {"polygon": [[408.0, 328.0], [728.0, 328.0], [728.0, 343.0], [408.0, 343.0]], "confidence": 0.9754255414009094, "text": "respectively. We assume the SMPL shape parameter β j from", "bbox": [408.0, 328.0, 728.0, 343.0]}, {"polygon": [[63.0, 343.0], [382.0, 343.0], [382.0, 358.0], [63.0, 358.0]], "confidence": 0.95437091588974, "text": "where GBG is to represent the 3D background and G is for", "bbox": [63.0, 343.0, 382.0, 358.0]}, {"polygon": [[409.0, 344.0], [718.0, 344.0], [718.0, 358.0], [409.0, 358.0]], "confidence": 0.97532057762146, "text": "the pose regressor is fixed for each human while training.", "bbox": [409.0, 344.0, 718.0, 358.0]}, {"polygon": [[425.0, 359.0], [729.0, 359.0], [729.0, 374.0], [425.0, 374.0]], "confidence": 0.9772599339485168, "text": "To capture the fine details of the background and human,", "bbox": [425.0, 359.0, 729.0, 374.0]}, {"polygon": [[63.0, 360.0], [150.0, 360.0], [150.0, 374.0], [63.0, 374.0]], "confidence": 0.9200498461723328, "text": "the j -th human.", "bbox": [63.0, 360.0, 150.0, 374.0]}, {"polygon": [[409.0, 376.0], [726.0, 376.0], [726.0, 390.0], [409.0, 390.0]], "confidence": 0.9805842041969299, "text": "we densify the initial Gaussians adaptively [ 20 ] every Nden", "bbox": [409.0, 376.0, 726.0, 390.0]}, {"polygon": [[80.0, 377.0], [384.0, 377.0], [384.0, 392.0], [80.0, 392.0]], "confidence": 0.9804719686508179, "text": "Importantly, we represent the individual human in a", "bbox": [80.0, 377.0, 384.0, 392.0]}, {"polygon": [[409.0, 392.0], [728.0, 392.0], [728.0, 406.0], [409.0, 406.0]], "confidence": 0.9796876311302185, "text": "iteration. The Gaussians to be densified are chosen based on", "bbox": [409.0, 392.0, 728.0, 406.0]}, {"polygon": [[63.0, 394.0], [383.0, 394.0], [383.0, 407.0], [63.0, 407.0]], "confidence": 0.9811870455741882, "text": "canonical space mapped to the rest pose (or A-pose) of", "bbox": [63.0, 394.0, 383.0, 407.0]}, {"polygon": [[409.0, 408.0], [728.0, 408.0], [728.0, 423.0], [409.0, 423.0]], "confidence": 0.9676690697669983, "text": "the accumulated gradients ∇ µ i , by summing the gradient", "bbox": [409.0, 408.0, 728.0, 423.0]}, {"polygon": [[63.0, 409.0], [383.0, 409.0], [383.0, 424.0], [63.0, 424.0]], "confidence": 0.9798345565795898, "text": "SMPL model [ 29 ], which can be transformed to any \"posed\"", "bbox": [63.0, 409.0, 383.0, 424.0]}, {"polygon": [[409.0, 424.0], [729.0, 424.0], [729.0, 439.0], [409.0, 439.0]], "confidence": 0.9711986184120178, "text": "on the center of Gaussians µ i computed in each iteration.", "bbox": [409.0, 424.0, 729.0, 439.0]}, {"polygon": [[63.0, 426.0], [382.0, 425.0], [382.0, 439.0], [63.0, 440.0]], "confidence": 0.9732210636138916, "text": "space parameterized by SMPL pose parameter θ ∈ R 72 .", "bbox": [63.0, 426.0, 382.0, 439.0]}, {"polygon": [[409.0, 434.0], [610.0, 440.0], [609.0, 459.0], [409.0, 453.0]], "confidence": 0.9314674735069275, "text": "If the accumulated gradients", "bbox": [409.0, 434.0, 610.0, 459.0]}, {"polygon": [[63.0, 440.0], [384.0, 440.0], [384.0, 455.0], [63.0, 455.0]], "confidence": 0.9803003072738647, "text": "Thus, the appearance of the j -th human at time t can be rep-", "bbox": [63.0, 440.0, 384.0, 455.0]}, {"polygon": [[609.0, 440.0], [729.0, 440.0], [729.0, 454.0], [609.0, 454.0]], "confidence": 0.9127706289291382, "text": "∇ µ i is bigger than a", "bbox": [609.0, 440.0, 729.0, 454.0]}, {"polygon": [[409.0, 455.0], [728.0, 455.0], [728.0, 470.0], [409.0, 470.0]], "confidence": 0.9788089990615845, "text": "predefined threshold, we densify the Gaussian G i by cloning", "bbox": [409.0, 455.0, 728.0, 470.0]}, {"polygon": [[63.0, 457.0], [384.0, 457.0], [384.0, 472.0], [63.0, 472.0]], "confidence": 0.9291852712631226, "text": "resented as G t i ( θ j,t ) by inputting the corresponding SMPL", "bbox": [63.0, 457.0, 384.0, 472.0]}, {"polygon": [[63.0, 472.0], [384.0, 472.0], [384.0, 487.0], [63.0, 487.0]], "confidence": 0.9653862714767456, "text": "parameters θ j,t for j -th human at time t . Note that within", "bbox": [63.0, 472.0, 384.0, 487.0]}, {"polygon": [[409.0, 472.0], [485.0, 472.0], [485.0, 487.0], [409.0, 487.0]], "confidence": 0.9401055574417114, "text": "or splitting it.", "bbox": [409.0, 472.0, 485.0, 487.0]}, {"polygon": [[63.0, 489.0], [384.0, 489.0], [384.0, 502.0], [63.0, 502.0]], "confidence": 0.9835357069969177, "text": "our representation, the posture of each individual can be con-", "bbox": [63.0, 489.0, 384.0, 502.0]}, {"polygon": [[409.0, 497.0], [648.0, 497.0], [648.0, 513.0], [409.0, 513.0]], "confidence": 0.9707085490226746, "text": "4.3. Canonicalizing Dynamic Humans", "bbox": [409.0, 497.0, 648.0, 513.0]}, {"polygon": [[63.0, 504.0], [382.0, 504.0], [382.0, 519.0], [63.0, 519.0]], "confidence": 0.9829092025756836, "text": "trolled independently from other scene parts. This provides", "bbox": [63.0, 504.0, 382.0, 519.0]}, {"polygon": [[63.0, 520.0], [382.0, 520.0], [382.0, 535.0], [63.0, 535.0]], "confidence": 0.9787788987159729, "text": "us the flexibility to edit people's body motions using various", "bbox": [63.0, 520.0, 382.0, 535.0]}, {"polygon": [[425.0, 521.0], [728.0, 521.0], [728.0, 536.0], [425.0, 536.0]], "confidence": 0.981580913066864, "text": "To fuse the cues of an individual with different poses", "bbox": [425.0, 521.0, 728.0, 536.0]}, {"polygon": [[63.0, 536.0], [251.0, 536.0], [251.0, 550.0], [63.0, 550.0]], "confidence": 0.9727837443351746, "text": "available motion capture data [ 31 ].", "bbox": [63.0, 536.0, 251.0, 550.0]}, {"polygon": [[409.0, 538.0], [728.0, 538.0], [728.0, 552.0], [409.0, 552.0]], "confidence": 0.9831322431564331, "text": "across different frames, we model each person with a single", "bbox": [409.0, 538.0, 728.0, 552.0]}, {"polygon": [[80.0, 553.0], [383.0, 553.0], [383.0, 567.0], [80.0, 567.0]], "confidence": 0.9800311923027039, "text": "Since we build both dynamic humans and backgrounds", "bbox": [80.0, 553.0, 383.0, 567.0]}, {"polygon": [[409.0, 553.0], [728.0, 553.0], [728.0, 570.0], [409.0, 570.0]], "confidence": 0.9533183574676514, "text": "canonical model G n,c (for brevity, we drop the person index", "bbox": [409.0, 553.0, 728.0, 570.0]}, {"polygon": [[63.0, 569.0], [384.0, 569.0], [384.0, 583.0], [63.0, 583.0]], "confidence": 0.9822502732276917, "text": "in the same 3D-GS representation, we can effectively ren-", "bbox": [63.0, 569.0, 384.0, 583.0]}, {"polygon": [[410.0, 570.0], [728.0, 570.0], [728.0, 584.0], [410.0, 584.0]], "confidence": 0.9790720343589783, "text": "j in the subscript). We also model the deformation function", "bbox": [410.0, 570.0, 728.0, 584.0]}, {"polygon": [[63.0, 585.0], [384.0, 585.0], [384.0, 600.0], [63.0, 600.0]], "confidence": 0.9815651178359985, "text": "der the whole scene by compositing 3D-GS representations,", "bbox": [63.0, 585.0, 384.0, 600.0]}, {"polygon": [[408.0, 585.0], [729.0, 585.0], [729.0, 601.0], [408.0, 601.0]], "confidence": 0.9133048057556152, "text": "G ) (t) = F ) (G ) which transforms Gaussians in canon-", "bbox": [408.0, 585.0, 729.0, 601.0]}, {"polygon": [[63.0, 600.0], [382.0, 600.0], [382.0, 616.0], [63.0, 616.0]], "confidence": 0.8912845849990845, "text": "GA = {GBG} U {G n } j =1... N , where we can use the same", "bbox": [63.0, 600.0, 382.0, 616.0]}, {"polygon": [[409.0, 601.0], [728.0, 601.0], [728.0, 616.0], [409.0, 616.0]], "confidence": 0.9622727632522583, "text": "ical into posed G ∂ ( t ) at time t , following the SMPL pose", "bbox": [409.0, 601.0, 728.0, 616.0]}, {"polygon": [[63.0, 616.0], [382.0, 616.0], [382.0, 632.0], [63.0, 632.0]], "confidence": 0.9753397703170776, "text": "rendering function Eq. ( 3 ) without any modification. This", "bbox": [63.0, 616.0, 382.0, 632.0]}, {"polygon": [[409.0, 618.0], [730.0, 616.0], [730.0, 631.0], [409.0, 633.0]], "confidence": 0.9458436965942383, "text": "parameter θ t . Our deformation function F h : R 3 → SE (3),", "bbox": [409.0, 618.0, 730.0, 631.0]}, {"polygon": [[63.0, 633.0], [382.0, 633.0], [382.0, 647.0], [63.0, 647.0]], "confidence": 0.9822496175765991, "text": "shows the major advantage over the alternative approaches", "bbox": [63.0, 633.0, 382.0, 647.0]}, {"polygon": [[409.0, 633.0], [729.0, 633.0], [729.0, 647.0], [409.0, 647.0]], "confidence": 0.9798858761787415, "text": "which maps canonical space into posed space, is defined via", "bbox": [409.0, 633.0, 729.0, 647.0]}, {"polygon": [[63.0, 648.0], [384.0, 648.0], [384.0, 663.0], [63.0, 663.0]], "confidence": 0.9807922840118408, "text": "such as NeRF-based representation [ 12 , 54 ], where com-", "bbox": [63.0, 648.0, 384.0, 663.0]}, {"polygon": [[408.0, 649.0], [729.0, 649.0], [729.0, 664.0], [408.0, 664.0]], "confidence": 0.9808445572853088, "text": "the Linear Blend Skinning (LBS) based on the SMPL [ 29 ].", "bbox": [408.0, 649.0, 729.0, 664.0]}, {"polygon": [[63.0, 665.0], [383.0, 665.0], [383.0, 678.0], [63.0, 678.0]], "confidence": 0.9830745458602905, "text": "positing multiple humans is not trivial. Our method is much", "bbox": [63.0, 665.0, 383.0, 678.0]}, {"polygon": [[409.0, 665.0], [729.0, 665.0], [729.0, 679.0], [409.0, 679.0]], "confidence": 0.9136801958084106, "text": "It translates the center of Gaussian 𝑠𝑠 𝑗 and rotates the covari-", "bbox": [409.0, 665.0, 729.0, 679.0]}, {"polygon": [[63.0, 680.0], [384.0, 680.0], [384.0, 695.0], [63.0, 695.0]], "confidence": 0.978669285774231, "text": "more convenient and efficient, showing much faster render-", "bbox": [63.0, 680.0, 384.0, 695.0]}, {"polygon": [[409.0, 681.0], [554.0, 681.0], [554.0, 696.0], [409.0, 696.0]], "confidence": 0.9406881928443909, "text": "ance matrix Σ i as follows:", "bbox": [409.0, 681.0, 554.0, 696.0]}, {"polygon": [[63.0, 696.0], [383.0, 696.0], [383.0, 711.0], [63.0, 711.0]], "confidence": 0.9826905727386475, "text": "ing speed (e.g., 40 times) than the competing approaches as", "bbox": [63.0, 696.0, 383.0, 711.0]}, {"polygon": [[63.0, 712.0], [244.0, 712.0], [244.0, 726.0], [63.0, 726.0]], "confidence": 0.969322144985199, "text": "demonstrated in our experiments.", "bbox": [63.0, 712.0, 244.0, 726.0]}, {"polygon": [[539.0, 722.0], [673.0, 722.0], [673.0, 737.0], [539.0, 737.0]], "confidence": 0.8684406876564026, "text": "wk ( x i,c )( R k × i,c + T k ),", "bbox": [539.0, 722.0, 673.0, 737.0]}, {"polygon": [[709.0, 722.0], [729.0, 722.0], [729.0, 737.0], [709.0, 737.0]], "confidence": 0.740467369556427, "text": "(6)", "bbox": [709.0, 722.0, 729.0, 737.0]}, {"polygon": [[80.0, 729.0], [384.0, 729.0], [384.0, 743.0], [80.0, 743.0]], "confidence": 0.980824887752533, "text": "Notably, we mainly focus on reconstructing the 3D hu-", "bbox": [80.0, 729.0, 384.0, 743.0]}, {"polygon": [[63.0, 745.0], [383.0, 745.0], [383.0, 759.0], [63.0, 759.0]], "confidence": 0.9816879630088806, "text": "mans from sparse monocular observations, in the presence of", "bbox": [63.0, 745.0, 383.0, 759.0]}, {"polygon": [[464.0, 757.0], [587.0, 757.0], [587.0, 772.0], [464.0, 772.0]], "confidence": 0.7706942558288574, "text": "Ei,p = Rwei Ei,cRwei", "bbox": [464.0, 757.0, 587.0, 772.0]}, {"polygon": [[708.0, 757.0], [730.0, 757.0], [730.0, 772.0], [708.0, 772.0]], "confidence": 0.725307047367096, "text": "(7)", "bbox": [708.0, 757.0, 730.0, 772.0]}, {"polygon": [[63.0, 761.0], [382.0, 761.0], [382.0, 776.0], [63.0, 776.0]], "confidence": 0.982050359249115, "text": "severe occlusions, cropped views, and few shots, which are", "bbox": [63.0, 761.0, 382.0, 776.0]}, {"polygon": [[63.0, 777.0], [383.0, 777.0], [383.0, 791.0], [63.0, 791.0]], "confidence": 0.9823758602142334, "text": "commonly observable in the wild. We address this challenge", "bbox": [63.0, 777.0, 383.0, 791.0]}, {"polygon": [[409.0, 786.0], [728.0, 784.0], [728.0, 800.0], [409.0, 801.0]], "confidence": 0.9740095734596252, "text": "where R k and T k are rotation and translation of k th joint", "bbox": [409.0, 786.0, 728.0, 800.0]}, {"polygon": [[63.0, 792.0], [382.0, 792.0], [382.0, 807.0], [63.0, 807.0]], "confidence": 0.9826375842094421, "text": "by fusing the observed cues into the canonical spaces, for", "bbox": [63.0, 792.0, 382.0, 807.0]}, {"polygon": [[409.0, 802.0], [728.0, 802.0], [728.0, 816.0], [409.0, 816.0]], "confidence": 0.9726447463035583, "text": "of SMPL skeleton which is computed from θ and β j . The", "bbox": [409.0, 802.0, 728.0, 816.0]}, {"polygon": [[63.0, 809.0], [383.0, 809.0], [383.0, 822.0], [63.0, 822.0]], "confidence": 0.9810776710510254, "text": "which we introduce the transformation between the posed", "bbox": [63.0, 809.0, 383.0, 822.0]}, {"polygon": [[410.0, 818.0], [728.0, 818.0], [728.0, 832.0], [410.0, 832.0]], "confidence": 0.9782989621162415, "text": "R wei is a derivation of LBS equal to the weighted sum of", "bbox": [410.0, 818.0, 728.0, 832.0]}, {"polygon": [[63.0, 824.0], [384.0, 824.0], [384.0, 839.0], [63.0, 839.0]], "confidence": 0.9827112555503845, "text": "space to the canonical space, while we leverage 3D-GS repre-", "bbox": [63.0, 824.0, 384.0, 839.0]}, {"polygon": [[409.0, 834.0], [586.0, 834.0], [586.0, 852.0], [409.0, 852.0]], "confidence": 0.9183275699615479, "text": "rotations { R k } k = 1 as follows:", "bbox": [409.0, 834.0, 586.0, 852.0]}, {"polygon": [[63.0, 841.0], [384.0, 841.0], [384.0, 854.0], [63.0, 854.0]], "confidence": 0.9794095158576965, "text": "sentation (Sec. 4.3 ). As a core contribution, we also present a", "bbox": [63.0, 841.0, 384.0, 854.0]}, {"polygon": [[63.0, 856.0], [382.0, 856.0], [382.0, 871.0], [63.0, 871.0]], "confidence": 0.9828127026557922, "text": "solution to include 2D diffusion prior as a way to synthesize", "bbox": [63.0, 856.0, 382.0, 871.0]}, {"polygon": [[63.0, 872.0], [384.0, 872.0], [384.0, 886.0], [63.0, 886.0]], "confidence": 0.9819587469100952, "text": "the missing and unobserved part of target human while keep-", "bbox": [63.0, 872.0, 384.0, 886.0]}, {"polygon": [[494.0, 879.0], [524.0, 882.0], [523.0, 896.0], [493.0, 893.0]], "confidence": 0.7934367060661316, "text": "R wei", "bbox": [494.0, 879.0, 524.0, 896.0]}, {"polygon": [[572.0, 880.0], [643.0, 880.0], [643.0, 894.0], [572.0, 894.0]], "confidence": 0.8052627444267273, "text": "wk ( x i,c ) R k .", "bbox": [572.0, 880.0, 643.0, 894.0]}, {"polygon": [[709.0, 880.0], [730.0, 880.0], [730.0, 894.0], [709.0, 894.0]], "confidence": 0.727729082107544, "text": "(8)", "bbox": [709.0, 880.0, 730.0, 894.0]}, {"polygon": [[63.0, 887.0], [382.0, 887.0], [382.0, 902.0], [63.0, 902.0]], "confidence": 0.981142520904541, "text": "ing the consistency to the observed parts (Sec. 4.4 ), where", "bbox": [63.0, 887.0, 382.0, 902.0]}, {"polygon": [[63.0, 904.0], [384.0, 904.0], [384.0, 918.0], [63.0, 918.0]], "confidence": 0.9781923294067383, "text": "we further enhance the quality by incorporating Texture In-", "bbox": [63.0, 904.0, 384.0, 918.0]}, {"polygon": [[63.0, 920.0], [383.0, 920.0], [383.0, 934.0], [63.0, 934.0]], "confidence": 0.9812748432159424, "text": "version technique [ 23 ] to better preserve the target identity.", "bbox": [63.0, 920.0, 383.0, 934.0]}, {"polygon": [[409.0, 920.0], [728.0, 920.0], [728.0, 935.0], [409.0, 935.0]], "confidence": 0.9579758048057556, "text": "The skinning weight Wk ( x ) is calculated from the aligned", "bbox": [409.0, 920.0, 728.0, 935.0]}, {"polygon": [[63.0, 936.0], [374.0, 936.0], [374.0, 951.0], [63.0, 951.0]], "confidence": 0.9812006950378418, "text": "The Fig. 2 shows the overall pipeline of our optimization.", "bbox": [63.0, 936.0, 374.0, 951.0]}, {"polygon": [[409.0, 936.0], [728.0, 937.0], [728.0, 952.0], [409.0, 950.0]], "confidence": 0.9773005247116089, "text": "SMPL vertices defined in canonical space. Here we get", "bbox": [409.0, 936.0, 728.0, 952.0]}], "languages": ["en"], "image_bbox": [0.0, 0.0, 816.0, 1056.0], "page": 4}, {"text_lines": [{"polygon": [[63.0, 96.0], [383.0, 96.0], [383.0, 111.0], [63.0, 111.0]], "confidence": 0.9665916562080383, "text": "w k ( x ) by summing the skinning weight of the 30 nearest", "bbox": [63.0, 96.0, 383.0, 111.0]}, {"polygon": [[408.0, 96.0], [729.0, 96.0], [729.0, 111.0], [408.0, 111.0]], "confidence": 0.9826599359512329, "text": "the consistent human appearance at the virtual viewpoints.", "bbox": [408.0, 96.0, 729.0, 111.0]}, {"polygon": [[63.0, 113.0], [382.0, 113.0], [382.0, 127.0], [63.0, 127.0]], "confidence": 0.9822609424591064, "text": "vertices with Inverse Distance Weight (IDW). To accelerate", "bbox": [63.0, 113.0, 382.0, 127.0]}, {"polygon": [[410.0, 113.0], [729.0, 113.0], [729.0, 128.0], [410.0, 128.0]], "confidence": 0.9830802083015442, "text": "To incorporate this idea, we leverage the Textual Inversion", "bbox": [410.0, 113.0, 729.0, 128.0]}, {"polygon": [[408.0, 129.0], [728.0, 129.0], [728.0, 143.0], [408.0, 143.0]], "confidence": 0.9697592258453369, "text": "(TI) by finding the text-embedding specific to our target", "bbox": [408.0, 129.0, 728.0, 143.0]}, {"polygon": [[63.0, 130.0], [382.0, 130.0], [382.0, 143.0], [63.0, 143.0]], "confidence": 0.9805864095687866, "text": "rendering speed, we pre-calculate and store the skinning", "bbox": [63.0, 130.0, 382.0, 143.0]}, {"polygon": [[63.0, 145.0], [383.0, 145.0], [383.0, 160.0], [63.0, 160.0]], "confidence": 0.9824925661087036, "text": "weight in a voxel grid, similar to SelfRecon [ 16 ]. In each", "bbox": [63.0, 145.0, 383.0, 160.0]}, {"polygon": [[408.0, 145.0], [729.0, 145.0], [729.0, 160.0], [408.0, 160.0]], "confidence": 0.9794527888298035, "text": "human [ 10 , 44 ]. Here, we use CustomDiffusion [ 23 ] to in-", "bbox": [408.0, 145.0, 729.0, 160.0]}, {"polygon": [[63.0, 161.0], [382.0, 161.0], [382.0, 175.0], [63.0, 175.0]], "confidence": 0.9827988743782043, "text": "rendering time, we obtain the skinning weight by trilinear", "bbox": [63.0, 161.0, 382.0, 175.0]}, {"polygon": [[409.0, 161.0], [728.0, 161.0], [728.0, 176.0], [409.0, 176.0]], "confidence": 0.9839258790016174, "text": "vert observations of the target individual into the text token", "bbox": [409.0, 161.0, 728.0, 176.0]}, {"polygon": [[63.0, 177.0], [382.0, 177.0], [382.0, 191.0], [63.0, 191.0]], "confidence": 0.9825448393821716, "text": "interpolation on the weight grid instead of searching the", "bbox": [63.0, 177.0, 382.0, 191.0]}, {"polygon": [[410.0, 177.0], [728.0, 177.0], [728.0, 191.0], [410.0, 191.0]], "confidence": 0.961275041103363, "text": "< person-j> , where we collect the images of the target", "bbox": [410.0, 177.0, 728.0, 191.0]}, {"polygon": [[63.0, 193.0], [189.0, 193.0], [189.0, 207.0], [63.0, 207.0]], "confidence": 0.9553771615028381, "text": "nearest SMPL vertices.", "bbox": [63.0, 193.0, 189.0, 207.0]}, {"polygon": [[408.0, 193.0], [728.0, 193.0], [728.0, 207.0], [408.0, 207.0]], "confidence": 0.9828455448150635, "text": "human from input frames by cropping and masking the target", "bbox": [408.0, 193.0, 728.0, 207.0]}, {"polygon": [[409.0, 209.0], [729.0, 209.0], [729.0, 224.0], [409.0, 224.0]], "confidence": 0.9826733469963074, "text": "person only. Together with Textual Inversion, CustomDiffu-", "bbox": [409.0, 209.0, 729.0, 224.0]}, {"polygon": [[63.0, 220.0], [300.0, 220.0], [300.0, 235.0], [63.0, 235.0]], "confidence": 0.971649169921875, "text": "4.4. Diffusion-Guided Reconstruction", "bbox": [63.0, 220.0, 300.0, 235.0]}, {"polygon": [[409.0, 224.0], [728.0, 226.0], [728.0, 241.0], [409.0, 239.0]], "confidence": 0.9800523519515991, "text": "sion fine-tunes the attention and text embedding layer of the", "bbox": [409.0, 224.0, 728.0, 241.0]}, {"polygon": [[409.0, 241.0], [728.0, 241.0], [728.0, 256.0], [409.0, 256.0]], "confidence": 0.9746298789978027, "text": "diffusion φ which makes a person-specific diffusion φ j . By", "bbox": [409.0, 241.0, 728.0, 256.0]}, {"polygon": [[81.0, 245.0], [384.0, 245.0], [384.0, 259.0], [81.0, 259.0]], "confidence": 0.9799081683158875, "text": "We employ the 2D diffusion prior [ 42 ] in optimizing 3D-", "bbox": [81.0, 245.0, 384.0, 259.0]}, {"polygon": [[409.0, 257.0], [728.0, 257.0], [728.0, 271.0], [409.0, 271.0]], "confidence": 0.983130931854248, "text": "adding the inverted text-token to the text-prompt, we can get", "bbox": [409.0, 257.0, 728.0, 271.0]}, {"polygon": [[63.0, 261.0], [382.0, 261.0], [382.0, 276.0], [63.0, 276.0]], "confidence": 0.9818974137306213, "text": "GS to represent a human model, as a key idea to overcome", "bbox": [63.0, 261.0, 382.0, 276.0]}, {"polygon": [[409.0, 273.0], [729.0, 273.0], [729.0, 287.0], [409.0, 287.0]], "confidence": 0.9825488328933716, "text": "diffusion-generated images consistent with the observations.", "bbox": [409.0, 273.0, 729.0, 287.0]}, {"polygon": [[63.0, 278.0], [383.0, 278.0], [383.0, 291.0], [63.0, 291.0]], "confidence": 0.9835330843925476, "text": "sparse observations for the target human in input videos. The", "bbox": [63.0, 278.0, 383.0, 291.0]}, {"polygon": [[409.0, 289.0], [728.0, 289.0], [728.0, 303.0], [409.0, 303.0]], "confidence": 0.9769167304039001, "text": "We perform the diffusion fine-tuning and Textual Inversion", "bbox": [409.0, 289.0, 728.0, 303.0]}, {"polygon": [[63.0, 293.0], [383.0, 293.0], [383.0, 308.0], [63.0, 308.0]], "confidence": 0.982505202293396, "text": "intuition behind our approach is that the quality of a 3D", "bbox": [63.0, 293.0, 383.0, 308.0]}, {"polygon": [[409.0, 305.0], [728.0, 305.0], [728.0, 319.0], [409.0, 319.0]], "confidence": 0.9773178696632385, "text": "per person separately and apply the subject-specific fine-", "bbox": [409.0, 305.0, 728.0, 319.0]}, {"polygon": [[63.0, 309.0], [382.0, 309.0], [382.0, 322.0], [63.0, 322.0]], "confidence": 0.9819042086601257, "text": "human model can be measured by assessing the realism of", "bbox": [63.0, 309.0, 382.0, 322.0]}, {"polygon": [[409.0, 321.0], [646.0, 321.0], [646.0, 335.0], [409.0, 335.0]], "confidence": 0.9774439334869385, "text": "tuned version of SDS for each target person.", "bbox": [409.0, 321.0, 646.0, 335.0]}, {"polygon": [[63.0, 324.0], [382.0, 324.0], [382.0, 339.0], [63.0, 339.0]], "confidence": 0.9811207056045532, "text": "the rendered images in novel views. For example, naively", "bbox": [63.0, 324.0, 382.0, 339.0]}, {"polygon": [[425.0, 337.0], [729.0, 337.0], [729.0, 351.0], [425.0, 351.0]], "confidence": 0.9800152778625488, "text": "Furthermore, we also utilize the OpenPose Control-", "bbox": [425.0, 337.0, 729.0, 351.0]}, {"polygon": [[63.0, 341.0], [382.0, 341.0], [382.0, 354.0], [63.0, 354.0]], "confidence": 0.9799750447273254, "text": "optimizing 3D Gaussians G from sparse and occluded views", "bbox": [63.0, 341.0, 382.0, 354.0]}, {"polygon": [[409.0, 352.0], [728.0, 352.0], [728.0, 367.0], [409.0, 367.0]], "confidence": 0.9698752164840698, "text": "Net [ 64 ] to align the body pose of a person generated by", "bbox": [409.0, 352.0, 728.0, 367.0]}, {"polygon": [[63.0, 356.0], [384.0, 356.0], [384.0, 371.0], [63.0, 371.0]], "confidence": 0.9804195761680603, "text": "results in artifacts or missing parts, as shown in Fig. 3 (a).", "bbox": [63.0, 356.0, 384.0, 371.0]}, {"polygon": [[409.0, 368.0], [728.0, 368.0], [728.0, 382.0], [409.0, 382.0]], "confidence": 0.9786638617515564, "text": "diffusion and our person Gaussians G h . For this, when we", "bbox": [409.0, 368.0, 728.0, 382.0]}, {"polygon": [[63.0, 373.0], [384.0, 373.0], [384.0, 387.0], [63.0, 387.0]], "confidence": 0.9825972318649292, "text": "The use of the diffusion model, particularly the SDS loss [ 39 ],", "bbox": [63.0, 373.0, 384.0, 387.0]}, {"polygon": [[409.0, 384.0], [728.0, 384.0], [728.0, 399.0], [409.0, 399.0]], "confidence": 0.9804946184158325, "text": "compute the SDS loss, we project the 3D SMPL joints", "bbox": [409.0, 384.0, 728.0, 399.0]}, {"polygon": [[63.0, 388.0], [383.0, 388.0], [383.0, 403.0], [63.0, 403.0]], "confidence": 0.9782726168632507, "text": "can be beneficial to improving the quality of the desired 3D", "bbox": [63.0, 388.0, 383.0, 403.0]}, {"polygon": [[410.0, 401.0], [728.0, 401.0], [728.0, 414.0], [410.0, 414.0]], "confidence": 0.9561089277267456, "text": "J smpl (θ,β j ) into the viewpoint, convert to OpenPose [ 4 ]", "bbox": [410.0, 401.0, 728.0, 414.0]}, {"polygon": [[63.0, 404.0], [382.0, 404.0], [382.0, 418.0], [63.0, 418.0]], "confidence": 0.9824110269546509, "text": "model by enforcing realism in the rendered images at novel", "bbox": [63.0, 404.0, 382.0, 418.0]}, {"polygon": [[409.0, 416.0], [728.0, 416.0], [728.0, 431.0], [409.0, 431.0]], "confidence": 0.98309326171875, "text": "format, and query them into the ControlNet. We additionally", "bbox": [409.0, 416.0, 728.0, 431.0]}, {"polygon": [[63.0, 420.0], [382.0, 420.0], [382.0, 434.0], [63.0, 434.0]], "confidence": 0.9828711152076721, "text": "views. The SDS loss can be considered as additional virtual", "bbox": [63.0, 420.0, 382.0, 434.0]}, {"polygon": [[409.0, 432.0], [729.0, 432.0], [729.0, 446.0], [409.0, 446.0]], "confidence": 0.982372522354126, "text": "add a view-augmented language prompt [ 39 ] for stable opti-", "bbox": [409.0, 432.0, 729.0, 446.0]}, {"polygon": [[63.0, 436.0], [384.0, 436.0], [384.0, 450.0], [63.0, 450.0]], "confidence": 0.981427788734436, "text": "cameras guided by the pre-trained 2D diffusion model [ 42 ].", "bbox": [63.0, 436.0, 384.0, 450.0]}, {"polygon": [[409.0, 448.0], [728.0, 448.0], [728.0, 462.0], [409.0, 462.0]], "confidence": 0.96930992603302, "text": "mization. We sample the noise time-step τ from U [0.2, 0.98]", "bbox": [409.0, 448.0, 728.0, 462.0]}, {"polygon": [[80.0, 452.0], [384.0, 452.0], [384.0, 467.0], [80.0, 467.0]], "confidence": 0.9712240099906921, "text": "For each iteration, we make rendering R h of the target hu-", "bbox": [80.0, 452.0, 384.0, 467.0]}, {"polygon": [[409.0, 464.0], [729.0, 464.0], [729.0, 478.0], [409.0, 478.0]], "confidence": 0.9792447686195374, "text": "for the first 2000 iterations and then we decay the maxi-", "bbox": [409.0, 464.0, 729.0, 478.0]}, {"polygon": [[63.0, 468.0], [382.0, 468.0], [382.0, 483.0], [63.0, 483.0]], "confidence": 0.969617486000061, "text": "man's 3D-GS G h with a virtual camera v which is randomly", "bbox": [63.0, 468.0, 382.0, 483.0]}, {"polygon": [[409.0, 480.0], [728.0, 480.0], [728.0, 494.0], [409.0, 494.0]], "confidence": 0.9809306263923645, "text": "mum time-step from 0.98 to 0.3 for 2000 iterations. Also to", "bbox": [409.0, 480.0, 728.0, 494.0]}, {"polygon": [[63.0, 484.0], [383.0, 484.0], [383.0, 498.0], [63.0, 498.0]], "confidence": 0.9818500876426697, "text": "sampled from a sphere that is centered on each human and", "bbox": [63.0, 484.0, 383.0, 498.0]}, {"polygon": [[409.0, 496.0], [728.0, 496.0], [728.0, 510.0], [409.0, 510.0]], "confidence": 0.9761837124824524, "text": "enhance the fine details of the body, we randomly sample", "bbox": [409.0, 496.0, 728.0, 510.0]}, {"polygon": [[63.0, 500.0], [384.0, 500.0], [384.0, 514.0], [63.0, 514.0]], "confidence": 0.982810914516449, "text": "viewing its body. To give more diversity to the rendering,", "bbox": [63.0, 500.0, 384.0, 514.0]}, {"polygon": [[409.0, 513.0], [728.0, 513.0], [728.0, 527.0], [409.0, 527.0]], "confidence": 0.9684303998947144, "text": "camera v j  which zooms in the face, lower body, and upper", "bbox": [409.0, 513.0, 728.0, 527.0]}, {"polygon": [[63.0, 516.0], [382.0, 516.0], [382.0, 530.0], [63.0, 530.0]], "confidence": 0.9803163409233093, "text": "we also randomly sample the body pose θ of the person and", "bbox": [63.0, 516.0, 382.0, 530.0]}, {"polygon": [[409.0, 528.0], [658.0, 528.0], [658.0, 543.0], [409.0, 543.0]], "confidence": 0.9785872101783752, "text": "body. Refer to our supp. mat. for more details.", "bbox": [409.0, 528.0, 658.0, 543.0]}, {"polygon": [[63.0, 531.0], [383.0, 531.0], [383.0, 547.0], [63.0, 547.0]], "confidence": 0.9768737554550171, "text": "transform the 3D-GS G h into the posed space. We randomly", "bbox": [63.0, 531.0, 383.0, 547.0]}, {"polygon": [[63.0, 549.0], [384.0, 549.0], [384.0, 563.0], [63.0, 563.0]], "confidence": 0.9771897196769714, "text": "sample θ either among observed poses or the canonical A-", "bbox": [63.0, 549.0, 384.0, 563.0]}, {"polygon": [[409.0, 552.0], [561.0, 552.0], [561.0, 569.0], [409.0, 569.0]], "confidence": 0.9590810537338257, "text": "4.5. Training Objectives", "bbox": [409.0, 552.0, 561.0, 569.0]}, {"polygon": [[63.0, 564.0], [382.0, 564.0], [382.0, 579.0], [63.0, 579.0]], "confidence": 0.9357507228851318, "text": "pose( θ c ), which can be written as {θ t } t ∈ [1... T ] ∪ {θ c } . With", "bbox": [63.0, 564.0, 382.0, 579.0]}, {"polygon": [[425.0, 578.0], [728.0, 578.0], [728.0, 593.0], [425.0, 593.0]], "confidence": 0.9811263084411621, "text": "For every iteration, we render the image R t corresponding", "bbox": [425.0, 578.0, 728.0, 593.0]}, {"polygon": [[63.0, 580.0], [382.0, 580.0], [382.0, 595.0], [63.0, 595.0]], "confidence": 0.9548431634902954, "text": "the rendered image R th at viewpoint v , we compute the SDS", "bbox": [63.0, 580.0, 382.0, 595.0]}, {"polygon": [[409.0, 594.0], [728.0, 594.0], [728.0, 608.0], [409.0, 608.0]], "confidence": 0.9798856973648071, "text": "to frame t and calculate MSE loss, SSIM loss, and LPIPS", "bbox": [409.0, 594.0, 728.0, 608.0]}, {"polygon": [[63.0, 597.0], [382.0, 597.0], [382.0, 611.0], [63.0, 611.0]], "confidence": 0.9823000431060791, "text": "loss [ 39 ], which is proportional to the difference between", "bbox": [63.0, 597.0, 382.0, 611.0]}, {"polygon": [[409.0, 610.0], [689.0, 610.0], [689.0, 624.0], [409.0, 624.0]], "confidence": 0.9768545627593994, "text": "loss by comparing it with the ground truth image I t :", "bbox": [409.0, 610.0, 689.0, 624.0]}, {"polygon": [[64.0, 613.0], [382.0, 613.0], [382.0, 628.0], [64.0, 628.0]], "confidence": 0.9572421908378601, "text": "added noise ε and estimated noise ε ﻪ by diffusion model φ :", "bbox": [64.0, 613.0, 382.0, 628.0]}, {"polygon": [[422.0, 643.0], [692.0, 642.0], [692.0, 657.0], [422.0, 658.0]], "confidence": 0.9347851276397705, "text": "Lrecon = λ rgb MSE( R t , I t ) + λ ssim SSIM( R t , I t )", "bbox": [422.0, 643.0, 692.0, 657.0]}, {"polygon": [[294.0, 645.0], [347.0, 643.0], [347.0, 658.0], [294.0, 660.0]], "confidence": 0.6719588041305542, "text": "∂z ∂R n", "bbox": [294.0, 645.0, 347.0, 658.0]}, {"polygon": [[79.0, 646.0], [347.0, 656.0], [347.0, 678.0], [78.0, 667.0]], "confidence": 0.7380881905555725, "text": "VLSDS = Et,e w(T)(et)(z,;y,r) - c) BR, aGh", "bbox": [79.0, 646.0, 347.0, 678.0]}, {"polygon": [[364.0, 652.0], [384.0, 652.0], [384.0, 667.0], [364.0, 667.0]], "confidence": 0.7342057228088379, "text": "(9)", "bbox": [364.0, 652.0, 384.0, 667.0]}, {"polygon": [[567.0, 660.0], [700.0, 660.0], [700.0, 677.0], [567.0, 677.0]], "confidence": 0.8806530833244324, "text": "+ λ t pips LPIPS( R t , I t ).", "bbox": [567.0, 660.0, 700.0, 677.0]}, {"polygon": [[699.0, 663.0], [729.0, 663.0], [729.0, 675.0], [699.0, 675.0]], "confidence": 0.7894598841667175, "text": "(10)", "bbox": [699.0, 663.0, 729.0, 675.0]}, {"polygon": [[409.0, 688.0], [729.0, 690.0], [729.0, 704.0], [409.0, 703.0]], "confidence": 0.9575656652450562, "text": "Then we compute SDS Loss L SDS for each person's Gaus-", "bbox": [409.0, 688.0, 729.0, 704.0]}, {"polygon": [[64.0, 691.0], [382.0, 691.0], [382.0, 706.0], [64.0, 706.0]], "confidence": 0.9226610064506531, "text": ", where z r is a noised latent of rendering R h , τ ∈ [0 , 1]", "bbox": [64.0, 691.0, 382.0, 706.0]}, {"polygon": [[409.0, 705.0], [640.0, 705.0], [640.0, 719.0], [409.0, 719.0]], "confidence": 0.9343000650405884, "text": "sians G h with person-specific diffusion φ j :", "bbox": [409.0, 705.0, 640.0, 719.0]}, {"polygon": [[63.0, 707.0], [382.0, 707.0], [382.0, 722.0], [63.0, 722.0]], "confidence": 0.9786503911018372, "text": "is a time-step of noise and y is conditions applied on the", "bbox": [63.0, 707.0, 382.0, 722.0]}, {"polygon": [[63.0, 723.0], [382.0, 723.0], [382.0, 738.0], [63.0, 738.0]], "confidence": 0.9828325510025024, "text": "diffusion model. The SDS loss mitigates the artifacts in our", "bbox": [63.0, 723.0, 382.0, 738.0]}, {"polygon": [[65.0, 740.0], [381.0, 740.0], [381.0, 754.0], [65.0, 754.0]], "confidence": 0.9692656993865967, "text": "3D-GS human model by enforcing the rendered output R h", "bbox": [65.0, 740.0, 381.0, 754.0]}, {"polygon": [[494.0, 747.0], [642.0, 749.0], [641.0, 765.0], [494.0, 763.0]], "confidence": 0.9022083282470703, "text": "Lsds =", "bbox": [494.0, 747.0, 642.0, 765.0]}, {"polygon": [[702.0, 748.0], [730.0, 748.0], [730.0, 763.0], [702.0, 763.0]], "confidence": 0.784684956073761, "text": "(11)", "bbox": [702.0, 748.0, 730.0, 763.0]}, {"polygon": [[63.0, 755.0], [229.0, 755.0], [229.0, 770.0], [63.0, 770.0]], "confidence": 0.969312846660614, "text": "in a novel pose to be plausible.", "bbox": [63.0, 755.0, 229.0, 770.0]}, {"polygon": [[409.0, 790.0], [729.0, 790.0], [729.0, 805.0], [409.0, 805.0]], "confidence": 0.982510507106781, "text": "To avoid transparent artifacts, we additionally add hard-", "bbox": [409.0, 790.0, 729.0, 805.0]}, {"polygon": [[64.0, 792.0], [384.0, 792.0], [384.0, 807.0], [64.0, 807.0]], "confidence": 0.9719205498695374, "text": "Textual Inversion on SDS. We further improve the effi-", "bbox": [64.0, 792.0, 384.0, 807.0]}, {"polygon": [[409.0, 806.0], [729.0, 806.0], [729.0, 821.0], [409.0, 821.0]], "confidence": 0.9818529486656189, "text": "surface regularization on human rendering following LOL-", "bbox": [409.0, 806.0, 729.0, 821.0]}, {"polygon": [[63.0, 809.0], [382.0, 809.0], [382.0, 822.0], [63.0, 822.0]], "confidence": 0.9828325510025024, "text": "cacy of our SDS method by leveraging the concept of Texture", "bbox": [63.0, 809.0, 382.0, 822.0]}, {"polygon": [[409.0, 821.0], [473.0, 821.0], [473.0, 836.0], [409.0, 836.0]], "confidence": 0.8308617472648621, "text": "NeRF [41] :", "bbox": [409.0, 821.0, 473.0, 836.0]}, {"polygon": [[63.0, 824.0], [384.0, 824.0], [384.0, 839.0], [63.0, 839.0]], "confidence": 0.9803553223609924, "text": "Inversion (TI) [ 10 ], as a way to make the SDS loss to syn-", "bbox": [63.0, 824.0, 384.0, 839.0]}, {"polygon": [[63.0, 841.0], [384.0, 841.0], [384.0, 854.0], [63.0, 854.0]], "confidence": 0.9832589030265808, "text": "thesize the human appearance similar to our target identity,", "bbox": [63.0, 841.0, 384.0, 854.0]}, {"polygon": [[452.0, 849.0], [684.0, 847.0], [684.0, 864.0], [452.0, 866.0]], "confidence": 0.9003974199295044, "text": "Lhard = − log(exp− lα l + exp lα l ) + const.", "bbox": [452.0, 849.0, 684.0, 864.0]}, {"polygon": [[702.0, 850.0], [730.0, 850.0], [730.0, 865.0], [702.0, 865.0]], "confidence": 0.7888199090957642, "text": "(12)", "bbox": [702.0, 850.0, 730.0, 865.0]}, {"polygon": [[63.0, 856.0], [382.0, 856.0], [382.0, 871.0], [63.0, 871.0]], "confidence": 0.9813917279243469, "text": "rather than generating arbitrary appearance. Applying the", "bbox": [63.0, 856.0, 382.0, 871.0]}, {"polygon": [[63.0, 872.0], [384.0, 872.0], [384.0, 887.0], [63.0, 887.0]], "confidence": 0.9795774817466736, "text": "SDS loss only with text prompts, such as \"A photo of a per-", "bbox": [63.0, 872.0, 384.0, 887.0]}, {"polygon": [[410.0, 878.0], [729.0, 878.0], [729.0, 893.0], [410.0, 893.0]], "confidence": 0.9619511365890503, "text": ", where α is the rendered alpha map of person Gaussian G h .", "bbox": [410.0, 878.0, 729.0, 893.0]}, {"polygon": [[63.0, 888.0], [382.0, 888.0], [382.0, 902.0], [63.0, 902.0]], "confidence": 0.9806753396987915, "text": "son\", may easily converge to a random human appearance", "bbox": [63.0, 888.0, 382.0, 902.0]}, {"polygon": [[409.0, 894.0], [629.0, 894.0], [629.0, 909.0], [409.0, 909.0]], "confidence": 0.9711237549781799, "text": "Our final training objective is as follows:", "bbox": [409.0, 894.0, 629.0, 909.0]}, {"polygon": [[63.0, 904.0], [384.0, 904.0], [384.0, 918.0], [63.0, 918.0]], "confidence": 0.9827228784561157, "text": "due to the diversity of diffusion prior model [ 42 ]. Ideally,", "bbox": [63.0, 904.0, 384.0, 918.0]}, {"polygon": [[63.0, 920.0], [382.0, 920.0], [382.0, 935.0], [63.0, 935.0]], "confidence": 0.9761564135551453, "text": "we want to specify the target individual observed from our", "bbox": [63.0, 920.0, 382.0, 935.0]}, {"polygon": [[701.0, 923.0], [729.0, 923.0], [729.0, 937.0], [701.0, 937.0]], "confidence": 0.7912651300430298, "text": "(13)", "bbox": [701.0, 923.0, 729.0, 937.0]}, {"polygon": [[425.0, 924.0], [691.0, 924.0], [691.0, 938.0], [425.0, 938.0]], "confidence": 0.918696403503418, "text": "Ltot = XreconLrecon + XsdsLsds + XhardChard-", "bbox": [425.0, 924.0, 691.0, 938.0]}, {"polygon": [[63.0, 936.0], [382.0, 936.0], [382.0, 951.0], [63.0, 951.0]], "confidence": 0.9833621382713318, "text": "input images, to encourage the diffusion model to synthesize", "bbox": [63.0, 936.0, 382.0, 951.0]}], "languages": ["en"], "image_bbox": [0.0, 0.0, 816.0, 1056.0], "page": 5}, {"text_lines": [{"polygon": [[371.0, 95.0], [452.0, 95.0], [452.0, 108.0], [371.0, 108.0]], "confidence": 0.939565122127533, "text": "(b) ST-NeuralBody", "bbox": [371.0, 95.0, 452.0, 108.0]}, {"polygon": [[123.0, 96.0], [171.0, 96.0], [171.0, 108.0], [123.0, 108.0]], "confidence": 0.9075853228569031, "text": "Input View", "bbox": [123.0, 96.0, 171.0, 108.0]}, {"polygon": [[256.0, 96.0], [327.0, 96.0], [327.0, 108.0], [256.0, 108.0]], "confidence": 0.9179977178573608, "text": "(a) HumanNeRF", "bbox": [256.0, 96.0, 327.0, 108.0]}, {"polygon": [[518.0, 96.0], [555.0, 96.0], [555.0, 108.0], [518.0, 108.0]], "confidence": 0.8703183531761169, "text": "(c) Ours", "bbox": [518.0, 96.0, 555.0, 108.0]}, {"polygon": [[622.0, 96.0], [696.0, 96.0], [696.0, 108.0], [622.0, 108.0]], "confidence": 0.9325101375579834, "text": "(d) Ground Truth", "bbox": [622.0, 96.0, 696.0, 108.0]}, {"polygon": [[233.0, 411.0], [559.0, 411.0], [559.0, 425.0], [233.0, 425.0]], "confidence": 0.9807009100914001, "text": "Figure 4. Novel view synthesized results of Panoptic Dataset [ 19 ]", "bbox": [233.0, 411.0, 559.0, 425.0]}, {"polygon": [[63.0, 440.0], [170.0, 440.0], [170.0, 455.0], [63.0, 455.0]], "confidence": 0.9319074153900146, "text": "5. Experiments", "bbox": [63.0, 440.0, 170.0, 455.0]}, {"polygon": [[409.0, 440.0], [728.0, 440.0], [728.0, 455.0], [409.0, 455.0]], "confidence": 0.982737123966217, "text": "single side of the people is visible and use all other 7 views", "bbox": [409.0, 440.0, 728.0, 455.0]}, {"polygon": [[409.0, 456.0], [728.0, 456.0], [728.0, 470.0], [409.0, 470.0]], "confidence": 0.98246169090271, "text": "as novel GT views for the evaluation. We use the provided", "bbox": [409.0, 456.0, 728.0, 470.0]}, {"polygon": [[80.0, 467.0], [384.0, 467.0], [384.0, 481.0], [80.0, 481.0]], "confidence": 0.9817718267440796, "text": "We perform rigorous quantitative and qualitative evalua-", "bbox": [80.0, 467.0, 384.0, 481.0]}, {"polygon": [[409.0, 472.0], [574.0, 472.0], [574.0, 487.0], [409.0, 487.0]], "confidence": 0.9616543650627136, "text": "pseudo-GT SMPL parameters.", "bbox": [409.0, 472.0, 574.0, 487.0]}, {"polygon": [[63.0, 483.0], [383.0, 483.0], [383.0, 498.0], [63.0, 498.0]], "confidence": 0.9788805246353149, "text": "tions to show the strengths of our method. We first test our", "bbox": [63.0, 483.0, 383.0, 498.0]}, {"polygon": [[408.0, 488.0], [728.0, 488.0], [728.0, 502.0], [408.0, 502.0]], "confidence": 0.9802855253219604, "text": "Single Human Benchmarks We also conduct experiments", "bbox": [408.0, 488.0, 728.0, 502.0]}, {"polygon": [[63.0, 499.0], [383.0, 499.0], [383.0, 513.0], [63.0, 513.0]], "confidence": 0.9822040796279907, "text": "method on challenging scenes capturing multiple people with", "bbox": [63.0, 499.0, 383.0, 513.0]}, {"polygon": [[408.0, 504.0], [728.0, 504.0], [728.0, 519.0], [408.0, 519.0]], "confidence": 0.9795506000518799, "text": "on an existing single human benchmark ZJU-Mocap [ 38 ]", "bbox": [408.0, 504.0, 728.0, 519.0]}, {"polygon": [[63.0, 514.0], [382.0, 514.0], [382.0, 529.0], [63.0, 529.0]], "confidence": 0.9826098680496216, "text": "sparse 2D observations, to show the major advantage of our", "bbox": [63.0, 514.0, 382.0, 529.0]}, {"polygon": [[409.0, 520.0], [728.0, 520.0], [728.0, 534.0], [409.0, 534.0]], "confidence": 0.9832106232643127, "text": "dataset, to check the performance of our model on the single", "bbox": [409.0, 520.0, 728.0, 534.0]}, {"polygon": [[63.0, 530.0], [383.0, 530.0], [383.0, 544.0], [63.0, 544.0]], "confidence": 0.981667160987854, "text": "method. We also apply our method to existing single human", "bbox": [63.0, 530.0, 383.0, 544.0]}, {"polygon": [[409.0, 535.0], [728.0, 535.0], [728.0, 550.0], [409.0, 550.0]], "confidence": 0.9811331629753113, "text": "human reconstruction task with sufficient observations. We", "bbox": [409.0, 535.0, 728.0, 550.0]}, {"polygon": [[63.0, 546.0], [384.0, 546.0], [384.0, 561.0], [63.0, 561.0]], "confidence": 0.9826225638389587, "text": "reconstruction benchmarks. Additionally, by performing ab-", "bbox": [63.0, 546.0, 384.0, 561.0]}, {"polygon": [[409.0, 552.0], [711.0, 552.0], [711.0, 566.0], [409.0, 566.0]], "confidence": 0.982467532157898, "text": "follow the evaluation pipeline used in baselines [ 17 , 54 ].", "bbox": [409.0, 552.0, 711.0, 566.0]}, {"polygon": [[63.0, 563.0], [382.0, 563.0], [382.0, 576.0], [63.0, 576.0]], "confidence": 0.9787105917930603, "text": "lation studies, we demonstrate the efficacy of each module", "bbox": [63.0, 563.0, 382.0, 576.0]}, {"polygon": [[409.0, 576.0], [640.0, 576.0], [640.0, 591.0], [409.0, 591.0]], "confidence": 0.9709272384643555, "text": "5.2. Baseline and Evaluation Metrics", "bbox": [409.0, 576.0, 640.0, 591.0]}, {"polygon": [[63.0, 578.0], [384.0, 578.0], [384.0, 593.0], [63.0, 593.0]], "confidence": 0.978542685508728, "text": "within our pipeline. We also show the computational effi-", "bbox": [63.0, 578.0, 384.0, 593.0]}, {"polygon": [[63.0, 594.0], [384.0, 594.0], [384.0, 608.0], [63.0, 608.0]], "confidence": 0.981334388256073, "text": "ciency of our method by comparing the rendering time to", "bbox": [63.0, 594.0, 384.0, 608.0]}, {"polygon": [[425.0, 601.0], [730.0, 601.0], [730.0, 615.0], [425.0, 615.0]], "confidence": 0.9784822463989258, "text": "We compare our model with three methods, Human-", "bbox": [425.0, 601.0, 730.0, 615.0]}, {"polygon": [[63.0, 609.0], [175.0, 609.0], [175.0, 624.0], [63.0, 624.0]], "confidence": 0.9495965838432312, "text": "the existing method.", "bbox": [63.0, 609.0, 175.0, 624.0]}, {"polygon": [[409.0, 616.0], [730.0, 616.0], [730.0, 631.0], [409.0, 631.0]], "confidence": 0.9580971002578735, "text": "NeRF [54] , InstantAvatar [ 17 ] and Shuai et al. [48 ]. Hu-", "bbox": [409.0, 616.0, 730.0, 631.0]}, {"polygon": [[409.0, 633.0], [729.0, 633.0], [729.0, 647.0], [409.0, 647.0]], "confidence": 0.9687871336936951, "text": "manNeRF [ 54 ] shows SOTA quality on the monocular hu-", "bbox": [409.0, 633.0, 729.0, 647.0]}, {"polygon": [[63.0, 638.0], [140.0, 638.0], [140.0, 653.0], [63.0, 653.0]], "confidence": 0.9207156896591187, "text": "5.1. Dataset", "bbox": [63.0, 638.0, 140.0, 653.0]}, {"polygon": [[409.0, 649.0], [728.0, 649.0], [728.0, 664.0], [409.0, 664.0]], "confidence": 0.9802685976028442, "text": "man reconstruction task. As HumanNeRF cannot handle", "bbox": [409.0, 649.0, 728.0, 664.0]}, {"polygon": [[63.0, 664.0], [384.0, 664.0], [384.0, 678.0], [63.0, 678.0]], "confidence": 0.9761663675308228, "text": "Panoptic Dataset [19] This dataset captures socially inter-", "bbox": [63.0, 664.0, 384.0, 678.0]}, {"polygon": [[408.0, 665.0], [729.0, 665.0], [729.0, 680.0], [408.0, 680.0]], "confidence": 0.9827277660369873, "text": "multiple people at once, we optimize it separately for each", "bbox": [408.0, 665.0, 729.0, 680.0]}, {"polygon": [[63.0, 680.0], [384.0, 680.0], [384.0, 695.0], [63.0, 695.0]], "confidence": 0.9823279976844788, "text": "acting multiple individuals via a multi-view camera system.", "bbox": [63.0, 680.0, 384.0, 695.0]}, {"polygon": [[409.0, 682.0], [729.0, 682.0], [729.0, 696.0], [409.0, 696.0]], "confidence": 0.9800543189048767, "text": "person and merge them in the final evaluation. We get a ren-", "bbox": [409.0, 682.0, 729.0, 696.0]}, {"polygon": [[63.0, 696.0], [384.0, 696.0], [384.0, 710.0], [63.0, 710.0]], "confidence": 0.9820295572280884, "text": "We simulate the sparse 2D view scenario by choosing a sin-", "bbox": [63.0, 696.0, 384.0, 710.0]}, {"polygon": [[408.0, 697.0], [728.0, 697.0], [728.0, 711.0], [408.0, 711.0]], "confidence": 0.9828864336013794, "text": "dering of each individual separately and accumulate them in", "bbox": [408.0, 697.0, 728.0, 711.0]}, {"polygon": [[63.0, 711.0], [382.0, 711.0], [382.0, 726.0], [63.0, 726.0]], "confidence": 0.9834963083267212, "text": "gle camera view as the input for the reconstruction and using", "bbox": [63.0, 711.0, 382.0, 726.0]}, {"polygon": [[409.0, 713.0], [729.0, 713.0], [729.0, 727.0], [409.0, 727.0]], "confidence": 0.9784397482872009, "text": "an α blending manner. The order of accumulation is deter-", "bbox": [409.0, 713.0, 729.0, 727.0]}, {"polygon": [[63.0, 728.0], [382.0, 728.0], [382.0, 742.0], [63.0, 742.0]], "confidence": 0.9811779260635376, "text": "other views for GT views. We use an ultimatum sequence", "bbox": [63.0, 728.0, 382.0, 742.0]}, {"polygon": [[408.0, 729.0], [728.0, 729.0], [728.0, 744.0], [408.0, 744.0]], "confidence": 0.9814119338989258, "text": "mined by the distance of the SMPL pelvis from the center", "bbox": [408.0, 729.0, 728.0, 744.0]}, {"polygon": [[63.0, 744.0], [384.0, 744.0], [384.0, 758.0], [63.0, 758.0]], "confidence": 0.9812470078468323, "text": "with 6 people (540 frames in ultimatum 160422 sequence),", "bbox": [63.0, 744.0, 384.0, 758.0]}, {"polygon": [[409.0, 746.0], [728.0, 746.0], [728.0, 759.0], [409.0, 759.0]], "confidence": 0.981286883354187, "text": "of the camera. Because HumanNeRF requires a foreground", "bbox": [409.0, 746.0, 728.0, 759.0]}, {"polygon": [[63.0, 760.0], [383.0, 760.0], [383.0, 774.0], [63.0, 774.0]], "confidence": 0.9814223647117615, "text": "and choose an HD camera view in a common view angle as", "bbox": [63.0, 760.0, 383.0, 774.0]}, {"polygon": [[409.0, 761.0], [728.0, 761.0], [728.0, 776.0], [409.0, 776.0]], "confidence": 0.9815584421157837, "text": "mask for processing, we use GT masks if available (Hi4D", "bbox": [409.0, 761.0, 728.0, 776.0]}, {"polygon": [[63.0, 776.0], [382.0, 776.0], [382.0, 791.0], [63.0, 791.0]], "confidence": 0.9811553955078125, "text": "the input video. We select other 7 HD cameras in diverse", "bbox": [63.0, 776.0, 382.0, 791.0]}, {"polygon": [[409.0, 777.0], [730.0, 777.0], [730.0, 791.0], [409.0, 791.0]], "confidence": 0.9804090857505798, "text": "and ZJU-Mocap), or use an off-the-shelf method [ 22 ]. In-", "bbox": [409.0, 777.0, 730.0, 791.0]}, {"polygon": [[63.0, 792.0], [384.0, 792.0], [384.0, 806.0], [63.0, 806.0]], "confidence": 0.9817036986351013, "text": "novel viewpoints as the GTs for the evaluations, as shown", "bbox": [63.0, 792.0, 384.0, 806.0]}, {"polygon": [[408.0, 792.0], [728.0, 792.0], [728.0, 807.0], [408.0, 807.0]], "confidence": 0.9791916608810425, "text": "stantAvatar [ 17 ] is the most efficient method to reconstruct", "bbox": [408.0, 792.0, 728.0, 807.0]}, {"polygon": [[63.0, 808.0], [383.0, 808.0], [383.0, 822.0], [63.0, 822.0]], "confidence": 0.9740427136421204, "text": "in Fig. 4. As a pre-processing, we fit SMPL models on the", "bbox": [63.0, 808.0, 383.0, 822.0]}, {"polygon": [[409.0, 809.0], [729.0, 809.0], [729.0, 822.0], [409.0, 822.0]], "confidence": 0.9826628565788269, "text": "a human from a monocular video with high quality. We eval-", "bbox": [409.0, 809.0, 729.0, 822.0]}, {"polygon": [[63.0, 823.0], [384.0, 823.0], [384.0, 838.0], [63.0, 838.0]], "confidence": 0.980883002281189, "text": "provided pseudo-GT 3D skeletons of the dataset using a", "bbox": [63.0, 823.0, 384.0, 838.0]}, {"polygon": [[409.0, 824.0], [728.0, 824.0], [728.0, 839.0], [409.0, 839.0]], "confidence": 0.982390820980072, "text": "uate the InstantAvatar using the same pipeline applied for", "bbox": [409.0, 824.0, 728.0, 839.0]}, {"polygon": [[63.0, 840.0], [384.0, 840.0], [384.0, 854.0], [63.0, 854.0]], "confidence": 0.9807494282722473, "text": "pose-prior [ 3 ]. See the supp. mat. for more details of pro-", "bbox": [63.0, 840.0, 384.0, 854.0]}, {"polygon": [[409.0, 840.0], [728.0, 840.0], [728.0, 854.0], [409.0, 854.0]], "confidence": 0.9805483818054199, "text": "HumanNeRF evaluation, as it's only capable of mono-human", "bbox": [409.0, 840.0, 728.0, 854.0]}, {"polygon": [[63.0, 856.0], [108.0, 856.0], [108.0, 870.0], [63.0, 870.0]], "confidence": 0.7612906694412231, "text": "cssing.", "bbox": [63.0, 856.0, 108.0, 870.0]}, {"polygon": [[408.0, 856.0], [728.0, 856.0], [728.0, 871.0], [408.0, 871.0]], "confidence": 0.9709075689315796, "text": "cases. Shuai et al. [48] is a compositional method similar to", "bbox": [408.0, 856.0, 728.0, 871.0]}, {"polygon": [[63.0, 872.0], [383.0, 872.0], [383.0, 887.0], [63.0, 887.0]], "confidence": 0.9803237915039062, "text": "Hi4D [61] This dataset contains multiple individuals at close", "bbox": [63.0, 872.0, 383.0, 887.0]}, {"polygon": [[409.0, 872.0], [729.0, 872.0], [729.0, 886.0], [409.0, 886.0]], "confidence": 0.9832715392112732, "text": "ours, which optimizes implicit functions of people and back-", "bbox": [409.0, 872.0, 729.0, 886.0]}, {"polygon": [[63.0, 888.0], [384.0, 888.0], [384.0, 902.0], [63.0, 902.0]], "confidence": 0.9802739024162292, "text": "distances, which are captured with synchronized 8 cameras.", "bbox": [63.0, 888.0, 384.0, 902.0]}, {"polygon": [[409.0, 887.0], [728.0, 887.0], [728.0, 902.0], [409.0, 902.0]], "confidence": 0.9834040403366089, "text": "ground from sparse view input [ 48 ]. It represents each person", "bbox": [409.0, 887.0, 728.0, 902.0]}, {"polygon": [[63.0, 904.0], [382.0, 904.0], [382.0, 918.0], [63.0, 918.0]], "confidence": 0.9646433591842651, "text": "We consider challenging two sequences pair00–dance", "bbox": [63.0, 904.0, 382.0, 918.0]}, {"polygon": [[409.0, 904.0], [728.0, 904.0], [728.0, 918.0], [409.0, 918.0]], "confidence": 0.9759160280227661, "text": "with NeuralBody [ 38 ] and background with NeRF [ 33 ] and", "bbox": [409.0, 904.0, 728.0, 918.0]}, {"polygon": [[63.0, 920.0], [382.0, 920.0], [382.0, 935.0], [63.0, 935.0]], "confidence": 0.9718082547187805, "text": "and pair01-hug . Similar to Panoptic DB, we choose the", "bbox": [63.0, 920.0, 382.0, 935.0]}, {"polygon": [[408.0, 920.0], [728.0, 920.0], [728.0, 935.0], [408.0, 935.0]], "confidence": 0.982685923576355, "text": "renders them together by using the compositional rendering", "bbox": [408.0, 920.0, 728.0, 935.0]}, {"polygon": [[63.0, 936.0], [384.0, 936.0], [384.0, 951.0], [63.0, 951.0]], "confidence": 0.9804748296737671, "text": "video from a camera (camera 76) as input where only a", "bbox": [63.0, 936.0, 384.0, 951.0]}, {"polygon": [[409.0, 936.0], [729.0, 936.0], [729.0, 951.0], [409.0, 951.0]], "confidence": 0.9806841015815735, "text": "pipeline of ST-NeRF [ 63 ]. Different from the original paper,", "bbox": [409.0, 936.0, 729.0, 951.0]}], "languages": ["en"], "image_bbox": [0.0, 0.0, 816.0, 1056.0], "page": 6}, {"text_lines": [{"polygon": [[314.0, 100.0], [368.0, 100.0], [368.0, 116.0], [314.0, 116.0]], "confidence": 0.8220995664596558, "text": "LPIPS*", "bbox": [314.0, 100.0, 368.0, 116.0]}, {"polygon": [[76.0, 101.0], [129.0, 101.0], [129.0, 115.0], [76.0, 115.0]], "confidence": 0.8737624287605286, "text": "Methods", "bbox": [76.0, 101.0, 129.0, 115.0]}, {"polygon": [[196.0, 101.0], [239.0, 101.0], [239.0, 116.0], [196.0, 116.0]], "confidence": 0.58868008852005, "text": "PSNR ↑", "bbox": [196.0, 101.0, 239.0, 116.0]}, {"polygon": [[255.0, 101.0], [297.0, 101.0], [297.0, 116.0], [255.0, 116.0]], "confidence": 0.7048752903938293, "text": "SSIM ↑", "bbox": [255.0, 101.0, 297.0, 116.0]}, {"polygon": [[422.0, 101.0], [469.0, 101.0], [469.0, 115.0], [422.0, 115.0]], "confidence": 0.8543210625648499, "text": "Method", "bbox": [422.0, 101.0, 469.0, 115.0]}, {"polygon": [[544.0, 101.0], [588.0, 101.0], [588.0, 116.0], [544.0, 116.0]], "confidence": 0.5923566222190857, "text": "PSNR ↑", "bbox": [544.0, 101.0, 588.0, 116.0]}, {"polygon": [[604.0, 101.0], [646.0, 101.0], [646.0, 116.0], [604.0, 116.0]], "confidence": 0.6897550225257874, "text": "SSIM ↑", "bbox": [604.0, 101.0, 646.0, 116.0]}, {"polygon": [[662.0, 101.0], [714.0, 101.0], [714.0, 116.0], [662.0, 116.0]], "confidence": 0.7349574565887451, "text": "LPIPS*†", "bbox": [662.0, 101.0, 714.0, 116.0]}, {"polygon": [[76.0, 126.0], [180.0, 126.0], [180.0, 139.0], [76.0, 139.0]], "confidence": 0.9123814702033997, "text": "HumanNeRF [54]", "bbox": [76.0, 126.0, 180.0, 139.0]}, {"polygon": [[201.0, 126.0], [235.0, 126.0], [235.0, 139.0], [201.0, 139.0]], "confidence": 0.8251612186431885, "text": "19.59", "bbox": [201.0, 126.0, 235.0, 139.0]}, {"polygon": [[255.0, 126.0], [297.0, 126.0], [297.0, 139.0], [255.0, 139.0]], "confidence": 0.8455365300178528, "text": "0.6514", "bbox": [255.0, 126.0, 297.0, 139.0]}, {"polygon": [[324.0, 126.0], [359.0, 126.0], [359.0, 139.0], [324.0, 139.0]], "confidence": 0.8260529041290283, "text": "38.69", "bbox": [324.0, 126.0, 359.0, 139.0]}, {"polygon": [[422.0, 126.0], [526.0, 126.0], [526.0, 140.0], [422.0, 140.0]], "confidence": 0.9049304127693176, "text": "HumanNeRF [54]", "bbox": [422.0, 126.0, 526.0, 140.0]}, {"polygon": [[550.0, 126.0], [583.0, 126.0], [583.0, 139.0], [550.0, 139.0]], "confidence": 0.8208306431770325, "text": "30.23", "bbox": [550.0, 126.0, 583.0, 139.0]}, {"polygon": [[604.0, 126.0], [646.0, 126.0], [646.0, 139.0], [604.0, 139.0]], "confidence": 0.8529708981513977, "text": "0.9554", "bbox": [604.0, 126.0, 646.0, 139.0]}, {"polygon": [[674.0, 126.0], [703.0, 126.0], [703.0, 139.0], [674.0, 139.0]], "confidence": 0.7861511707305908, "text": "3.36", "bbox": [674.0, 126.0, 703.0, 139.0]}, {"polygon": [[76.0, 143.0], [180.0, 143.0], [180.0, 157.0], [76.0, 157.0]], "confidence": 0.9369388818740845, "text": "InstantAvatar [ 17 ]", "bbox": [76.0, 143.0, 180.0, 157.0]}, {"polygon": [[201.0, 143.0], [235.0, 143.0], [235.0, 157.0], [201.0, 157.0]], "confidence": 0.8297839164733887, "text": "15.03", "bbox": [201.0, 143.0, 235.0, 157.0]}, {"polygon": [[255.0, 142.0], [297.0, 142.0], [297.0, 157.0], [255.0, 157.0]], "confidence": 0.8497173190116882, "text": "0.4163", "bbox": [255.0, 142.0, 297.0, 157.0]}, {"polygon": [[323.0, 142.0], [359.0, 142.0], [359.0, 157.0], [323.0, 157.0]], "confidence": 0.8221353888511658, "text": "65.95", "bbox": [323.0, 142.0, 359.0, 157.0]}, {"polygon": [[422.0, 143.0], [529.0, 143.0], [529.0, 157.0], [422.0, 157.0]], "confidence": 0.9228157997131348, "text": "InstantAvatar [ 17 ]", "bbox": [422.0, 143.0, 529.0, 157.0]}, {"polygon": [[549.0, 142.0], [584.0, 142.0], [584.0, 157.0], [549.0, 157.0]], "confidence": 0.8261547088623047, "text": "28.55", "bbox": [549.0, 142.0, 584.0, 157.0]}, {"polygon": [[603.0, 142.0], [646.0, 142.0], [646.0, 157.0], [603.0, 157.0]], "confidence": 0.8507378697395325, "text": "0.9282", "bbox": [603.0, 142.0, 646.0, 157.0]}, {"polygon": [[672.0, 143.0], [705.0, 143.0], [705.0, 157.0], [672.0, 157.0]], "confidence": 0.8294208645820618, "text": "10.60", "bbox": [672.0, 143.0, 705.0, 157.0]}, {"polygon": [[201.0, 159.0], [235.0, 159.0], [235.0, 174.0], [201.0, 174.0]], "confidence": 0.8302918076515198, "text": "15.79", "bbox": [201.0, 159.0, 235.0, 174.0]}, {"polygon": [[256.0, 159.0], [297.0, 159.0], [297.0, 174.0], [256.0, 174.0]], "confidence": 0.8473780751228333, "text": "0.8370", "bbox": [256.0, 159.0, 297.0, 174.0]}, {"polygon": [[324.0, 159.0], [358.0, 159.0], [358.0, 174.0], [324.0, 174.0]], "confidence": 0.8244670033454895, "text": "25.77", "bbox": [324.0, 159.0, 358.0, 174.0]}, {"polygon": [[76.0, 160.0], [173.0, 160.0], [173.0, 175.0], [76.0, 175.0]], "confidence": 0.9267425537109375, "text": "Shuai et al. [48]", "bbox": [76.0, 160.0, 173.0, 175.0]}, {"polygon": [[422.0, 168.0], [510.0, 168.0], [510.0, 182.0], [422.0, 182.0]], "confidence": 0.905198872089386, "text": "Ours wo L SDS", "bbox": [422.0, 168.0, 510.0, 182.0]}, {"polygon": [[550.0, 167.0], [585.0, 167.0], [585.0, 182.0], [550.0, 182.0]], "confidence": 0.8222501873970032, "text": "30.10", "bbox": [550.0, 167.0, 585.0, 182.0]}, {"polygon": [[603.0, 167.0], [646.0, 167.0], [646.0, 181.0], [603.0, 181.0]], "confidence": 0.8528189063072205, "text": "0.9529", "bbox": [603.0, 167.0, 646.0, 181.0]}, {"polygon": [[674.0, 167.0], [703.0, 167.0], [703.0, 181.0], [674.0, 181.0]], "confidence": 0.7842027544975281, "text": "4.68", "bbox": [674.0, 167.0, 703.0, 181.0]}, {"polygon": [[201.0, 183.0], [235.0, 183.0], [235.0, 198.0], [201.0, 198.0]], "confidence": 0.8238858580589294, "text": "23.60", "bbox": [201.0, 183.0, 235.0, 198.0]}, {"polygon": [[255.0, 183.0], [297.0, 183.0], [297.0, 198.0], [255.0, 198.0]], "confidence": 0.846589207649231, "text": "0.8323", "bbox": [255.0, 183.0, 297.0, 198.0]}, {"polygon": [[421.0, 183.0], [453.0, 183.0], [453.0, 198.0], [421.0, 198.0]], "confidence": 0.7900315523147583, "text": "Ours", "bbox": [421.0, 183.0, 453.0, 198.0]}, {"polygon": [[550.0, 183.0], [584.0, 183.0], [584.0, 198.0], [550.0, 198.0]], "confidence": 0.8203824162483215, "text": "30.13", "bbox": [550.0, 183.0, 584.0, 198.0]}, {"polygon": [[603.0, 183.0], [646.0, 183.0], [646.0, 197.0], [603.0, 197.0]], "confidence": 0.8522201180458069, "text": "0.9535", "bbox": [603.0, 183.0, 646.0, 197.0]}, {"polygon": [[674.0, 183.0], [703.0, 183.0], [703.0, 198.0], [674.0, 198.0]], "confidence": 0.7588202357292175, "text": "4.50", "bbox": [674.0, 183.0, 703.0, 198.0]}, {"polygon": [[76.0, 184.0], [108.0, 184.0], [108.0, 198.0], [76.0, 198.0]], "confidence": 0.7938511967658997, "text": "Ours", "bbox": [76.0, 184.0, 108.0, 198.0]}, {"polygon": [[324.0, 184.0], [358.0, 184.0], [358.0, 198.0], [324.0, 198.0]], "confidence": 0.8277077674865723, "text": "25.41", "bbox": [324.0, 184.0, 358.0, 198.0]}, {"polygon": [[64.0, 211.0], [382.0, 211.0], [382.0, 225.0], [64.0, 225.0]], "confidence": 0.9781407117843628, "text": "Table 1. Quantitative results on Panoptic dataset. LPIPS* =", "bbox": [64.0, 211.0, 382.0, 225.0]}, {"polygon": [[409.0, 211.0], [728.0, 211.0], [728.0, 225.0], [409.0, 225.0]], "confidence": 0.9790310263633728, "text": "Table 3. Quantitative results on 6 subjects in ZJU-Mocap", "bbox": [409.0, 211.0, 728.0, 225.0]}, {"polygon": [[409.0, 225.0], [728.0, 226.0], [728.0, 241.0], [409.0, 240.0]], "confidence": 0.9736994504928589, "text": "dataset [ 38 ]. LPIPS* = 100 × LPIPS. Our method shows compara-", "bbox": [409.0, 225.0, 728.0, 241.0]}, {"polygon": [[64.0, 227.0], [382.0, 227.0], [382.0, 241.0], [64.0, 241.0]], "confidence": 0.9809463024139404, "text": "100 × LPIPS. Our method shows better performance in PSNR and", "bbox": [64.0, 227.0, 382.0, 241.0]}, {"polygon": [[408.0, 241.0], [728.0, 241.0], [728.0, 255.0], [408.0, 255.0]], "confidence": 0.9784314632415771, "text": "ble quality compared to HumanNeRF [54] even if the observation", "bbox": [408.0, 241.0, 728.0, 255.0]}, {"polygon": [[63.0, 242.0], [256.0, 242.0], [256.0, 256.0], [63.0, 256.0]], "confidence": 0.972752034664154, "text": "LPIPS and comparable results in SSIM.", "bbox": [63.0, 242.0, 256.0, 256.0]}, {"polygon": [[409.0, 256.0], [469.0, 256.0], [469.0, 270.0], [409.0, 270.0]], "confidence": 0.9276100397109985, "text": "is sufficient.", "bbox": [409.0, 256.0, 469.0, 270.0]}, {"polygon": [[179.0, 272.0], [267.0, 272.0], [267.0, 286.0], [179.0, 286.0]], "confidence": 0.9175412654876709, "text": "pair00-dance", "bbox": [179.0, 272.0, 267.0, 286.0]}, {"polygon": [[288.0, 272.0], [365.0, 272.0], [365.0, 286.0], [288.0, 286.0]], "confidence": 0.89911288022995, "text": "pair01-hug", "bbox": [288.0, 272.0, 365.0, 286.0]}, {"polygon": [[74.0, 282.0], [116.0, 282.0], [116.0, 297.0], [74.0, 297.0]], "confidence": 0.8547943830490112, "text": "Method", "bbox": [74.0, 282.0, 116.0, 297.0]}, {"polygon": [[178.0, 293.0], [218.0, 293.0], [218.0, 307.0], [178.0, 307.0]], "confidence": 0.6453061699867249, "text": "PSNR ↑", "bbox": [178.0, 293.0, 218.0, 307.0]}, {"polygon": [[231.0, 293.0], [268.0, 293.0], [268.0, 307.0], [231.0, 307.0]], "confidence": 0.6669133901596069, "text": "SSIM ↑", "bbox": [231.0, 293.0, 268.0, 307.0]}, {"polygon": [[281.0, 292.0], [321.0, 292.0], [321.0, 307.0], [281.0, 307.0]], "confidence": 0.6831609606742859, "text": "PSNR ↑", "bbox": [281.0, 292.0, 321.0, 307.0]}, {"polygon": [[333.0, 293.0], [371.0, 293.0], [371.0, 307.0], [333.0, 307.0]], "confidence": 0.6312023997306824, "text": "SSIM †", "bbox": [333.0, 293.0, 371.0, 307.0]}, {"polygon": [[74.0, 313.0], [165.0, 313.0], [165.0, 327.0], [74.0, 327.0]], "confidence": 0.8993874788284302, "text": "HumanNeRF [54]", "bbox": [74.0, 313.0, 165.0, 327.0]}, {"polygon": [[183.0, 313.0], [214.0, 313.0], [214.0, 327.0], [183.0, 327.0]], "confidence": 0.8231640458106995, "text": "18.79", "bbox": [183.0, 313.0, 214.0, 327.0]}, {"polygon": [[231.0, 313.0], [268.0, 313.0], [268.0, 326.0], [231.0, 326.0]], "confidence": 0.8466293215751648, "text": "0.8552", "bbox": [231.0, 313.0, 268.0, 326.0]}, {"polygon": [[286.0, 313.0], [317.0, 313.0], [317.0, 327.0], [286.0, 327.0]], "confidence": 0.7898308634757996, "text": "21.40", "bbox": [286.0, 313.0, 317.0, 327.0]}, {"polygon": [[333.0, 313.0], [371.0, 313.0], [371.0, 326.0], [333.0, 326.0]], "confidence": 0.8452802300453186, "text": "0.8238", "bbox": [333.0, 313.0, 371.0, 326.0]}, {"polygon": [[663.0, 318.0], [721.0, 318.0], [721.0, 330.0], [663.0, 330.0]], "confidence": 0.9141485691070557, "text": "Novel View2", "bbox": [663.0, 318.0, 721.0, 330.0]}, {"polygon": [[184.0, 328.0], [214.0, 328.0], [214.0, 343.0], [184.0, 343.0]], "confidence": 0.8130515217781067, "text": "18.60", "bbox": [184.0, 328.0, 214.0, 343.0]}, {"polygon": [[231.0, 328.0], [268.0, 328.0], [268.0, 342.0], [231.0, 342.0]], "confidence": 0.8430495262145996, "text": "0.8646", "bbox": [231.0, 328.0, 268.0, 342.0]}, {"polygon": [[333.0, 328.0], [371.0, 328.0], [371.0, 342.0], [333.0, 342.0]], "confidence": 0.8416444063186646, "text": "0.8254", "bbox": [333.0, 328.0, 371.0, 342.0]}, {"polygon": [[75.0, 329.0], [165.0, 329.0], [165.0, 343.0], [75.0, 343.0]], "confidence": 0.938999354839325, "text": "InstantAvatar [ 17 ]", "bbox": [75.0, 329.0, 165.0, 343.0]}, {"polygon": [[287.0, 330.0], [316.0, 330.0], [316.0, 342.0], [287.0, 342.0]], "confidence": 0.8134433627128601, "text": "19.07", "bbox": [287.0, 330.0, 316.0, 342.0]}, {"polygon": [[75.0, 344.0], [159.0, 344.0], [159.0, 358.0], [75.0, 358.0]], "confidence": 0.9369738698005676, "text": "Shuai et al. [48]", "bbox": [75.0, 344.0, 159.0, 358.0]}, {"polygon": [[183.0, 344.0], [214.0, 344.0], [214.0, 358.0], [183.0, 358.0]], "confidence": 0.8211982250213623, "text": "20.78", "bbox": [183.0, 344.0, 214.0, 358.0]}, {"polygon": [[231.0, 344.0], [268.0, 344.0], [268.0, 357.0], [231.0, 357.0]], "confidence": 0.8512471318244934, "text": "0.9165", "bbox": [231.0, 344.0, 268.0, 357.0]}, {"polygon": [[287.0, 344.0], [316.0, 344.0], [316.0, 357.0], [287.0, 357.0]], "confidence": 0.8174911141395569, "text": "19.72", "bbox": [287.0, 344.0, 316.0, 357.0]}, {"polygon": [[333.0, 344.0], [371.0, 344.0], [371.0, 357.0], [333.0, 357.0]], "confidence": 0.8441653847694397, "text": "0.9078", "bbox": [333.0, 344.0, 371.0, 357.0]}, {"polygon": [[630.0, 347.0], [688.0, 347.0], [688.0, 359.0], [630.0, 359.0]], "confidence": 0.9078603386878967, "text": "Novel View1", "bbox": [630.0, 347.0, 688.0, 359.0]}, {"polygon": [[183.0, 365.0], [214.0, 365.0], [214.0, 378.0], [183.0, 378.0]], "confidence": 0.8250670433044434, "text": "23.76", "bbox": [183.0, 365.0, 214.0, 378.0]}, {"polygon": [[74.0, 366.0], [102.0, 366.0], [102.0, 378.0], [74.0, 378.0]], "confidence": 0.7956429719924927, "text": "Ours", "bbox": [74.0, 366.0, 102.0, 378.0]}, {"polygon": [[232.0, 366.0], [268.0, 366.0], [268.0, 378.0], [232.0, 378.0]], "confidence": 0.8491528630256653, "text": "0.9328", "bbox": [232.0, 366.0, 268.0, 378.0]}, {"polygon": [[287.0, 366.0], [317.0, 366.0], [317.0, 378.0], [287.0, 378.0]], "confidence": 0.8255007863044739, "text": "25.14", "bbox": [287.0, 366.0, 317.0, 378.0]}, {"polygon": [[335.0, 366.0], [371.0, 366.0], [371.0, 378.0], [335.0, 378.0]], "confidence": 0.8505586385726929, "text": "0.9289", "bbox": [335.0, 366.0, 371.0, 378.0]}, {"polygon": [[63.0, 390.0], [382.0, 390.0], [382.0, 403.0], [63.0, 403.0]], "confidence": 0.9771029353141785, "text": "Table 2. Quantitative results on Hi4D dataset [ 61 ]. Ours show", "bbox": [63.0, 390.0, 382.0, 403.0]}, {"polygon": [[63.0, 405.0], [382.0, 405.0], [382.0, 419.0], [63.0, 419.0]], "confidence": 0.9829333424568176, "text": "better performance on both PSNR and SSIM in situations when", "bbox": [63.0, 405.0, 382.0, 419.0]}, {"polygon": [[63.0, 420.0], [334.0, 420.0], [334.0, 434.0], [63.0, 434.0]], "confidence": 0.9822381734848022, "text": "people closely interact like hugging or dancing together", "bbox": [63.0, 420.0, 334.0, 434.0]}, {"polygon": [[63.0, 446.0], [304.0, 446.0], [304.0, 460.0], [63.0, 460.0]], "confidence": 0.9782970547676086, "text": "we optimize it using only a single train view.", "bbox": [63.0, 446.0, 304.0, 460.0]}, {"polygon": [[651.0, 455.0], [702.0, 455.0], [702.0, 467.0], [651.0, 467.0]], "confidence": 0.9063727855682373, "text": "Train View", "bbox": [651.0, 455.0, 702.0, 467.0]}, {"polygon": [[80.0, 461.0], [382.0, 461.0], [382.0, 476.0], [80.0, 476.0]], "confidence": 0.980465292930603, "text": "We compare the quality of human rendering by masking", "bbox": [80.0, 461.0, 382.0, 476.0]}, {"polygon": [[650.0, 469.0], [708.0, 469.0], [708.0, 481.0], [650.0, 481.0]], "confidence": 0.9132941365242004, "text": "(140frames)", "bbox": [650.0, 469.0, 708.0, 481.0]}, {"polygon": [[63.0, 478.0], [382.0, 478.0], [382.0, 492.0], [63.0, 492.0]], "confidence": 0.9824198484420776, "text": "out the background of GT images. For evaluation metric, we", "bbox": [63.0, 478.0, 382.0, 492.0]}, {"polygon": [[63.0, 493.0], [382.0, 493.0], [382.0, 508.0], [63.0, 508.0]], "confidence": 0.9829933047294617, "text": "use peak signal-to-noise ratio (PSNR), structural similarity", "bbox": [63.0, 493.0, 382.0, 508.0]}, {"polygon": [[629.0, 496.0], [725.0, 496.0], [725.0, 509.0], [629.0, 509.0]], "confidence": 0.9475290179252625, "text": "(d) Camera Position", "bbox": [629.0, 496.0, 725.0, 509.0]}, {"polygon": [[410.0, 498.0], [486.0, 498.0], [486.0, 509.0], [410.0, 509.0]], "confidence": 0.9141252636909485, "text": "(a) HumanNeRF", "bbox": [410.0, 498.0, 486.0, 509.0]}, {"polygon": [[501.0, 498.0], [543.0, 498.0], [543.0, 509.0], [501.0, 509.0]], "confidence": 0.8810052871704102, "text": "(b) Ours", "bbox": [501.0, 498.0, 543.0, 509.0]}, {"polygon": [[567.0, 498.0], [600.0, 498.0], [600.0, 509.0], [567.0, 509.0]], "confidence": 0.7912999987602234, "text": "(c) GT", "bbox": [567.0, 498.0, 600.0, 509.0]}, {"polygon": [[63.0, 510.0], [382.0, 510.0], [382.0, 524.0], [63.0, 524.0]], "confidence": 0.9813603162765503, "text": "(SSIM), and perceptual similarity (LPIPS) [ 65 ] following", "bbox": [63.0, 510.0, 382.0, 524.0]}, {"polygon": [[408.0, 516.0], [727.0, 516.0], [727.0, 530.0], [408.0, 530.0]], "confidence": 0.979767918586731, "text": "Figure 5. Novel view synthesis output of Hi4D pair00-dance", "bbox": [408.0, 516.0, 727.0, 530.0]}, {"polygon": [[63.0, 526.0], [172.0, 526.0], [172.0, 541.0], [63.0, 541.0]], "confidence": 0.9445940852165222, "text": "the prior work [ 54 ].", "bbox": [63.0, 526.0, 172.0, 541.0]}, {"polygon": [[408.0, 531.0], [728.0, 531.0], [728.0, 544.0], [408.0, 544.0]], "confidence": 0.9802336692810059, "text": "sequence. While HumanNeRF [54] fails to reconstruct a face, ours", "bbox": [408.0, 531.0, 728.0, 544.0]}, {"polygon": [[409.0, 546.0], [729.0, 546.0], [729.0, 559.0], [409.0, 559.0]], "confidence": 0.9827544093132019, "text": "synthesizes a plausible face guided by diffusion model [ 42 ]. (d)", "bbox": [409.0, 546.0, 729.0, 559.0]}, {"polygon": [[63.0, 553.0], [240.0, 553.0], [240.0, 569.0], [63.0, 569.0]], "confidence": 0.9634739756584167, "text": "5.3. Implementation Details", "bbox": [63.0, 553.0, 240.0, 569.0]}, {"polygon": [[408.0, 560.0], [728.0, 560.0], [728.0, 574.0], [408.0, 574.0]], "confidence": 0.9849075078964233, "text": "plots camera position relative to the front viewing female body. As", "bbox": [408.0, 560.0, 728.0, 574.0]}, {"polygon": [[409.0, 575.0], [728.0, 575.0], [728.0, 589.0], [409.0, 589.0]], "confidence": 0.9838592410087585, "text": "shown here, the majority of the rendered output shown here has", "bbox": [409.0, 575.0, 728.0, 589.0]}, {"polygon": [[80.0, 579.0], [383.0, 579.0], [383.0, 594.0], [80.0, 594.0]], "confidence": 0.9787756204605103, "text": "To stabilize optimization, we first train background Gaus-", "bbox": [80.0, 579.0, 383.0, 594.0]}, {"polygon": [[410.0, 591.0], [591.0, 591.0], [591.0, 603.0], [410.0, 603.0]], "confidence": 0.9676662087440491, "text": "been never observed in the train view.", "bbox": [410.0, 591.0, 591.0, 603.0]}, {"polygon": [[63.0, 594.0], [382.0, 595.0], [382.0, 610.0], [63.0, 609.0]], "confidence": 0.9753737449645996, "text": "sians GBG solely with background images where people", "bbox": [63.0, 594.0, 382.0, 610.0]}, {"polygon": [[63.0, 611.0], [382.0, 611.0], [382.0, 625.0], [63.0, 625.0]], "confidence": 0.9532463550567627, "text": "are masked out. We then optimize human Gaussians G Pi ,c", "bbox": [63.0, 611.0, 382.0, 625.0]}, {"polygon": [[63.0, 627.0], [382.0, 627.0], [382.0, 642.0], [63.0, 642.0]], "confidence": 0.9719340801239014, "text": "without L SDS for the first 1000 iterations and then, optimize", "bbox": [63.0, 627.0, 382.0, 642.0]}, {"polygon": [[409.0, 628.0], [728.0, 628.0], [728.0, 643.0], [409.0, 643.0]], "confidence": 0.9830894470214844, "text": "shown in the table, our method outperforms the baselines in", "bbox": [409.0, 628.0, 728.0, 643.0]}, {"polygon": [[63.0, 644.0], [383.0, 644.0], [383.0, 657.0], [63.0, 657.0]], "confidence": 0.961184561252594, "text": "human Gaussians G P i and background Gaussians G bq simul-", "bbox": [63.0, 644.0, 383.0, 657.0]}, {"polygon": [[409.0, 645.0], [507.0, 645.0], [507.0, 659.0], [409.0, 659.0]], "confidence": 0.9305957555770874, "text": "PSNR and LPIPS.", "bbox": [409.0, 645.0, 507.0, 659.0]}, {"polygon": [[63.0, 659.0], [384.0, 659.0], [384.0, 674.0], [63.0, 674.0]], "confidence": 0.9763778448104858, "text": "taneously together with L SDS for 10k iterations. Refer to", "bbox": [63.0, 659.0, 384.0, 674.0]}, {"polygon": [[409.0, 661.0], [729.0, 663.0], [728.0, 679.0], [409.0, 677.0]], "confidence": 0.9817192554473877, "text": "Hi4D dataset. We show the quantitative results in Tab. 2", "bbox": [409.0, 661.0, 729.0, 679.0]}, {"polygon": [[63.0, 675.0], [233.0, 675.0], [233.0, 689.0], [63.0, 689.0]], "confidence": 0.9687151908874512, "text": "our supp. mat. for more details.", "bbox": [63.0, 675.0, 233.0, 689.0]}, {"polygon": [[409.0, 679.0], [728.0, 679.0], [728.0, 694.0], [409.0, 694.0]], "confidence": 0.9749638438224792, "text": "and qualitative examples in Fig. 5 from pair00–dance", "bbox": [409.0, 679.0, 728.0, 694.0]}, {"polygon": [[409.0, 696.0], [729.0, 696.0], [729.0, 710.0], [409.0, 710.0]], "confidence": 0.9815797209739685, "text": "sequence. As shown in Tab. 2 , our method outperforms base-", "bbox": [409.0, 696.0, 729.0, 710.0]}, {"polygon": [[63.0, 702.0], [382.0, 702.0], [382.0, 718.0], [63.0, 718.0]], "confidence": 0.979945182800293, "text": "5.4. Evaluations on Multiple People Reconstruction", "bbox": [63.0, 702.0, 382.0, 718.0]}, {"polygon": [[409.0, 711.0], [728.0, 711.0], [728.0, 726.0], [409.0, 726.0]], "confidence": 0.9841846823692322, "text": "lines in all metrics. Interestingly, in this dataset, some body", "bbox": [409.0, 711.0, 728.0, 726.0]}, {"polygon": [[63.0, 728.0], [384.0, 728.0], [384.0, 742.0], [63.0, 742.0]], "confidence": 0.9820306301116943, "text": "Panoptic dataset. We show the qualitative comparison be-", "bbox": [63.0, 728.0, 384.0, 742.0]}, {"polygon": [[409.0, 727.0], [728.0, 727.0], [728.0, 741.0], [409.0, 741.0]], "confidence": 0.9833470582962036, "text": "parts of the individual are never observed in the input view", "bbox": [409.0, 727.0, 728.0, 741.0]}, {"polygon": [[409.0, 743.0], [728.0, 743.0], [728.0, 758.0], [409.0, 758.0]], "confidence": 0.9823974967002869, "text": "due to the severe occlusions, e.g., the face of the female", "bbox": [409.0, 743.0, 728.0, 758.0]}, {"polygon": [[63.0, 744.0], [382.0, 744.0], [382.0, 758.0], [63.0, 758.0]], "confidence": 0.9787500500679016, "text": "tween our results and the outputs of baselines in Fig. 4 . As", "bbox": [63.0, 744.0, 382.0, 758.0]}, {"polygon": [[409.0, 759.0], [728.0, 759.0], [728.0, 773.0], [409.0, 773.0]], "confidence": 0.9793182611465454, "text": "person in Fig. 5 , where our method \"hallucinates\" realistic", "bbox": [409.0, 759.0, 728.0, 773.0]}, {"polygon": [[63.0, 760.0], [383.0, 760.0], [383.0, 774.0], [63.0, 774.0]], "confidence": 0.9820517301559448, "text": "shown, our method reconstructs the appearances of people", "bbox": [63.0, 760.0, 383.0, 774.0]}, {"polygon": [[409.0, 775.0], [612.0, 775.0], [612.0, 790.0], [409.0, 790.0]], "confidence": 0.9721505641937256, "text": "human face without any observations.", "bbox": [409.0, 775.0, 612.0, 790.0]}, {"polygon": [[63.0, 776.0], [384.0, 776.0], [384.0, 791.0], [63.0, 791.0]], "confidence": 0.9823163151741028, "text": "even in the presence of severe occlusions and image crop-", "bbox": [63.0, 776.0, 384.0, 791.0]}, {"polygon": [[63.0, 792.0], [383.0, 792.0], [383.0, 806.0], [63.0, 806.0]], "confidence": 0.972807765007019, "text": "ping. In contrast, both HumanNeRF [54] and Shuai et al. [48]", "bbox": [63.0, 792.0, 383.0, 806.0]}, {"polygon": [[63.0, 808.0], [382.0, 808.0], [382.0, 822.0], [63.0, 822.0]], "confidence": 0.9838324785232544, "text": "fail to reconstruct details showing noticeable artifacts since", "bbox": [63.0, 808.0, 382.0, 822.0]}, {"polygon": [[409.0, 813.0], [719.0, 813.0], [719.0, 828.0], [409.0, 828.0]], "confidence": 0.9790881276130676, "text": "5.5. Evaluations on Single Person Reconstruction", "bbox": [409.0, 813.0, 719.0, 828.0]}, {"polygon": [[63.0, 824.0], [383.0, 824.0], [383.0, 839.0], [63.0, 839.0]], "confidence": 0.9832974076271057, "text": "many body parts are not visible in the input view. In partic-", "bbox": [63.0, 824.0, 383.0, 839.0]}, {"polygon": [[63.0, 841.0], [382.0, 841.0], [382.0, 854.0], [63.0, 854.0]], "confidence": 0.9814285039901733, "text": "ular, HumanNeRF suffers from severe occlusions because", "bbox": [63.0, 841.0, 382.0, 854.0]}, {"polygon": [[425.0, 841.0], [730.0, 840.0], [730.0, 854.0], [425.0, 855.0]], "confidence": 0.9753261804580688, "text": "We show the quantitative comparisons on ZJU-", "bbox": [425.0, 841.0, 730.0, 854.0]}, {"polygon": [[63.0, 856.0], [383.0, 856.0], [383.0, 871.0], [63.0, 871.0]], "confidence": 0.9827803373336792, "text": "it requires accurate foreground masks containing whole hu-", "bbox": [63.0, 856.0, 383.0, 871.0]}, {"polygon": [[409.0, 856.0], [728.0, 856.0], [728.0, 871.0], [409.0, 871.0]], "confidence": 0.978050172328949, "text": "Mocap [ 38 ] dataset. As shown in Tab. 3 , our method shows", "bbox": [409.0, 856.0, 728.0, 871.0]}, {"polygon": [[63.0, 872.0], [382.0, 872.0], [382.0, 886.0], [63.0, 886.0]], "confidence": 0.9802936911582947, "text": "man shapes, which cannot be obtained due to the occluder", "bbox": [63.0, 872.0, 382.0, 886.0]}, {"polygon": [[409.0, 872.0], [728.0, 872.0], [728.0, 886.0], [409.0, 886.0]], "confidence": 0.9768889546394348, "text": "comparable performance over baselines when sufficient 2D", "bbox": [409.0, 872.0, 728.0, 886.0]}, {"polygon": [[63.0, 887.0], [382.0, 887.0], [382.0, 902.0], [63.0, 902.0]], "confidence": 0.9840613007545471, "text": "in front of the target individual. In contrast, our method does", "bbox": [63.0, 887.0, 382.0, 902.0]}, {"polygon": [[408.0, 887.0], [728.0, 887.0], [728.0, 902.0], [408.0, 902.0]], "confidence": 0.9834675192832947, "text": "observations are available. This result demonstrates that our", "bbox": [408.0, 887.0, 728.0, 902.0]}, {"polygon": [[63.0, 904.0], [212.0, 904.0], [212.0, 917.0], [63.0, 917.0]], "confidence": 0.964307963848114, "text": "not suffer from such issues.", "bbox": [63.0, 904.0, 212.0, 917.0]}, {"polygon": [[408.0, 904.0], [728.0, 904.0], [728.0, 918.0], [408.0, 918.0]], "confidence": 0.9822483658790588, "text": "pipeline based on 3D-GS with SDS loss does not negatively", "bbox": [408.0, 904.0, 728.0, 918.0]}, {"polygon": [[80.0, 920.0], [384.0, 920.0], [384.0, 934.0], [80.0, 934.0]], "confidence": 0.9807637333869934, "text": "We also quantify the output qualities in Tab. 1 by render-", "bbox": [80.0, 920.0, 384.0, 934.0]}, {"polygon": [[409.0, 920.0], [728.0, 920.0], [728.0, 934.0], [409.0, 934.0]], "confidence": 0.9834442734718323, "text": "affect the single-person reconstruction scenarios while show-", "bbox": [409.0, 920.0, 728.0, 934.0]}, {"polygon": [[63.0, 936.0], [382.0, 936.0], [382.0, 950.0], [63.0, 950.0]], "confidence": 0.9820665121078491, "text": "ing the reconstructed scenes into unseen novel views. As", "bbox": [63.0, 936.0, 382.0, 950.0]}, {"polygon": [[409.0, 936.0], [716.0, 936.0], [716.0, 950.0], [409.0, 950.0]], "confidence": 0.9831497073173523, "text": "ing its major strengths in the case of sparse observations.", "bbox": [409.0, 936.0, 716.0, 950.0]}], "languages": ["en"], "image_bbox": [0.0, 0.0, 816.0, 1056.0], "page": 7}, {"text_lines": [{"polygon": [[526.0, 99.0], [651.0, 99.0], [651.0, 114.0], [526.0, 114.0]], "confidence": 0.9416171908378601, "text": "Rendering Speed (FPS †)", "bbox": [526.0, 99.0, 651.0, 114.0]}, {"polygon": [[419.0, 110.0], [461.0, 110.0], [461.0, 124.0], [419.0, 124.0]], "confidence": 0.8551063537597656, "text": "Method", "bbox": [419.0, 110.0, 461.0, 124.0]}, {"polygon": [[669.0, 110.0], [716.0, 110.0], [716.0, 124.0], [669.0, 124.0]], "confidence": 0.8233561515808105, "text": "VRAM ↓", "bbox": [669.0, 110.0, 716.0, 124.0]}, {"polygon": [[588.0, 120.0], [655.0, 120.0], [655.0, 134.0], [588.0, 134.0]], "confidence": 0.9123842716217041, "text": "1024 × 1024", "bbox": [588.0, 120.0, 655.0, 134.0]}, {"polygon": [[522.0, 121.0], [578.0, 121.0], [578.0, 134.0], [522.0, 134.0]], "confidence": 0.8905653953552246, "text": "512 × 512", "bbox": [522.0, 121.0, 578.0, 134.0]}, {"polygon": [[535.0, 140.0], [564.0, 140.0], [564.0, 154.0], [535.0, 154.0]], "confidence": 0.8153727650642395, "text": "18.03", "bbox": [535.0, 140.0, 564.0, 154.0]}, {"polygon": [[669.0, 140.0], [716.0, 140.0], [716.0, 154.0], [669.0, 154.0]], "confidence": 0.87062668800354, "text": "4972MiB", "bbox": [669.0, 140.0, 716.0, 154.0]}, {"polygon": [[611.0, 141.0], [634.0, 141.0], [634.0, 154.0], [611.0, 154.0]], "confidence": 0.7713114023208618, "text": "7.09", "bbox": [611.0, 141.0, 634.0, 154.0]}, {"polygon": [[420.0, 142.0], [509.0, 142.0], [509.0, 154.0], [420.0, 154.0]], "confidence": 0.935746431350708, "text": "InstantAvatar [ 17 ]", "bbox": [420.0, 142.0, 509.0, 154.0]}, {"polygon": [[419.0, 155.0], [475.0, 155.0], [475.0, 169.0], [419.0, 169.0]], "confidence": 0.905794084072113, "text": "Ours (15k)", "bbox": [419.0, 155.0, 475.0, 169.0]}, {"polygon": [[531.0, 155.0], [566.0, 155.0], [566.0, 169.0], [531.0, 169.0]], "confidence": 0.8399723768234253, "text": "361.01", "bbox": [531.0, 155.0, 566.0, 169.0]}, {"polygon": [[668.0, 155.0], [717.0, 155.0], [717.0, 168.0], [668.0, 168.0]], "confidence": 0.8711700439453125, "text": "1496MiB", "bbox": [668.0, 155.0, 717.0, 168.0]}, {"polygon": [[605.0, 156.0], [639.0, 156.0], [639.0, 168.0], [605.0, 168.0]], "confidence": 0.8541291356086731, "text": "277.01", "bbox": [605.0, 156.0, 639.0, 168.0]}, {"polygon": [[410.0, 181.0], [727.0, 181.0], [727.0, 195.0], [410.0, 195.0]], "confidence": 0.9808244705200195, "text": "Table 5. Novel pose rendering speed. We compared novel pose", "bbox": [410.0, 181.0, 727.0, 195.0]}, {"polygon": [[410.0, 197.0], [729.0, 197.0], [729.0, 210.0], [410.0, 210.0]], "confidence": 0.9806188941001892, "text": "rendering speed between InstantAvatar [ 17 ] and ours with ZJU-", "bbox": [410.0, 197.0, 729.0, 210.0]}, {"polygon": [[409.0, 211.0], [728.0, 211.0], [728.0, 224.0], [409.0, 224.0]], "confidence": 0.982666015625, "text": "Mocap [ 38 ] 377 subject. It shows that our method consists of a 15k", "bbox": [409.0, 211.0, 728.0, 224.0]}, {"polygon": [[408.0, 226.0], [729.0, 226.0], [729.0, 240.0], [408.0, 240.0]], "confidence": 0.984033465385437, "text": "Gaussians renders faster on both low and high resolutions, with", "bbox": [408.0, 226.0, 729.0, 240.0]}, {"polygon": [[408.0, 240.0], [535.0, 240.0], [535.0, 254.0], [408.0, 254.0]], "confidence": 0.9556134343147278, "text": "less VRAM consumption.", "bbox": [408.0, 240.0, 535.0, 254.0]}, {"polygon": [[409.0, 271.0], [568.0, 271.0], [568.0, 286.0], [409.0, 286.0]], "confidence": 0.954322874546051, "text": "5.7. Rendering Efficiency", "bbox": [409.0, 271.0, 568.0, 286.0]}, {"polygon": [[425.0, 299.0], [728.0, 299.0], [728.0, 313.0], [425.0, 313.0]], "confidence": 0.9759596586227417, "text": "We show the computational efficiency of our method by", "bbox": [425.0, 299.0, 728.0, 313.0]}, {"polygon": [[150.0, 304.0], [282.0, 304.0], [282.0, 317.0], [150.0, 317.0]], "confidence": 0.8443147540092468, "text": "(a) wo/ L SDS (b) wo/ TI", "bbox": [150.0, 304.0, 282.0, 317.0]}, {"polygon": [[280.0, 305.0], [324.0, 305.0], [324.0, 315.0], [280.0, 315.0]], "confidence": 0.8415207862854004, "text": "(c) Full", "bbox": [280.0, 305.0, 324.0, 315.0]}, {"polygon": [[99.0, 307.0], [127.0, 307.0], [127.0, 318.0], [99.0, 318.0]], "confidence": 0.8249258399009705, "text": "Input", "bbox": [99.0, 307.0, 127.0, 318.0]}, {"polygon": [[409.0, 315.0], [729.0, 315.0], [729.0, 330.0], [409.0, 330.0]], "confidence": 0.9817877411842346, "text": "comparing the rendering time of novel pose synthesis to", "bbox": [409.0, 315.0, 729.0, 330.0]}, {"polygon": [[63.0, 330.0], [382.0, 330.0], [382.0, 343.0], [63.0, 343.0]], "confidence": 0.9819428324699402, "text": "Figure 6. Ablation studies. From (a) shows that using only recon-", "bbox": [63.0, 330.0, 382.0, 343.0]}, {"polygon": [[409.0, 330.0], [729.0, 330.0], [729.0, 344.0], [409.0, 344.0]], "confidence": 0.9809330701828003, "text": "InstantAvatar [ 17 ], which is known as the most efficient ex-", "bbox": [409.0, 330.0, 729.0, 344.0]}, {"polygon": [[63.0, 344.0], [382.0, 344.0], [382.0, 358.0], [63.0, 358.0]], "confidence": 0.9851715564727783, "text": "struction loss suffers from artifacts (like hedgehogs) even in shown", "bbox": [63.0, 344.0, 382.0, 358.0]}, {"polygon": [[409.0, 346.0], [728.0, 346.0], [728.0, 360.0], [409.0, 360.0]], "confidence": 0.9836428165435791, "text": "isting method and also the fastest among our competitors. All", "bbox": [409.0, 346.0, 728.0, 360.0]}, {"polygon": [[63.0, 359.0], [382.0, 359.0], [382.0, 372.0], [63.0, 372.0]], "confidence": 0.9846879243850708, "text": "regions. (b) shows that textual inversion is essential to generate", "bbox": [63.0, 359.0, 382.0, 372.0]}, {"polygon": [[409.0, 362.0], [728.0, 362.0], [728.0, 376.0], [409.0, 376.0]], "confidence": 0.9828180074691772, "text": "reported times here are measured with a single GeForce RTX", "bbox": [409.0, 362.0, 728.0, 376.0]}, {"polygon": [[64.0, 374.0], [314.0, 374.0], [314.0, 387.0], [64.0, 387.0]], "confidence": 0.9804853200912476, "text": "contextually similar appearances on unseen regions.", "bbox": [64.0, 374.0, 314.0, 387.0]}, {"polygon": [[410.0, 378.0], [729.0, 378.0], [729.0, 392.0], [410.0, 392.0]], "confidence": 0.9789912104606628, "text": "3090 GPU. By taking advantage of 3D-GS representations,", "bbox": [410.0, 378.0, 729.0, 392.0]}, {"polygon": [[409.0, 394.0], [729.0, 394.0], [729.0, 408.0], [409.0, 408.0]], "confidence": 0.9814878106117249, "text": "our method achieves more than real-time rendering speed", "bbox": [409.0, 394.0, 729.0, 408.0]}, {"polygon": [[409.0, 410.0], [728.0, 410.0], [728.0, 425.0], [409.0, 425.0]], "confidence": 0.9696217775344849, "text": "with 300 FPS on 1K × 1K images. As shown in Tab. 5 , our", "bbox": [409.0, 410.0, 728.0, 425.0]}, {"polygon": [[73.0, 423.0], [148.0, 423.0], [148.0, 432.0], [73.0, 432.0]], "confidence": 0.9355041980743408, "text": "w/ Textual Inversion", "bbox": [73.0, 423.0, 148.0, 432.0]}, {"polygon": [[161.0, 423.0], [194.0, 423.0], [194.0, 434.0], [161.0, 434.0]], "confidence": 0.6737322211265564, "text": "w/ L SDS", "bbox": [161.0, 423.0, 194.0, 434.0]}, {"polygon": [[257.0, 423.0], [286.0, 423.0], [286.0, 432.0], [257.0, 432.0]], "confidence": 0.5501247644424438, "text": "PNR*", "bbox": [257.0, 423.0, 286.0, 432.0]}, {"polygon": [[297.0, 423.0], [327.0, 423.0], [327.0, 433.0], [297.0, 433.0]], "confidence": 0.6255491971969604, "text": "SSIM†", "bbox": [297.0, 423.0, 327.0, 433.0]}, {"polygon": [[335.0, 423.0], [372.0, 423.0], [372.0, 432.0], [335.0, 432.0]], "confidence": 0.8183962106704712, "text": "LPIPS* ↓", "bbox": [335.0, 423.0, 372.0, 432.0]}, {"polygon": [[210.0, 424.0], [233.0, 424.0], [233.0, 433.0], [210.0, 433.0]], "confidence": 0.4541366994380951, "text": "w/ Cre", "bbox": [210.0, 424.0, 233.0, 433.0]}, {"polygon": [[409.0, 426.0], [728.0, 426.0], [728.0, 440.0], [409.0, 440.0]], "confidence": 0.983040452003479, "text": "method surpasses InstantAvatar [ 17 ] both in rendering speed", "bbox": [409.0, 426.0, 728.0, 440.0]}, {"polygon": [[262.0, 438.0], [284.0, 438.0], [284.0, 448.0], [262.0, 448.0]], "confidence": 0.8207226395606995, "text": "26.03", "bbox": [262.0, 438.0, 284.0, 448.0]}, {"polygon": [[297.0, 437.0], [325.0, 437.0], [325.0, 448.0], [297.0, 448.0]], "confidence": 0.8498377203941345, "text": "0.9435", "bbox": [297.0, 437.0, 325.0, 448.0]}, {"polygon": [[347.0, 438.0], [365.0, 438.0], [365.0, 448.0], [347.0, 448.0]], "confidence": 0.7020666003227234, "text": "7.00", "bbox": [347.0, 438.0, 365.0, 448.0]}, {"polygon": [[409.0, 442.0], [728.0, 442.0], [728.0, 456.0], [409.0, 456.0]], "confidence": 0.9812507629394531, "text": "and memory consumption while showing better rendering", "bbox": [409.0, 442.0, 728.0, 456.0]}, {"polygon": [[297.0, 448.0], [325.0, 448.0], [325.0, 460.0], [297.0, 460.0]], "confidence": 0.8487818837165833, "text": "0.9342", "bbox": [297.0, 448.0, 325.0, 460.0]}, {"polygon": [[263.0, 449.0], [283.0, 449.0], [283.0, 458.0], [263.0, 458.0]], "confidence": 0.8287792205810547, "text": "23.43", "bbox": [263.0, 449.0, 283.0, 458.0]}, {"polygon": [[346.0, 449.0], [365.0, 449.0], [365.0, 458.0], [346.0, 458.0]], "confidence": 0.7449234127998352, "text": "8.36", "bbox": [346.0, 449.0, 365.0, 458.0]}, {"polygon": [[409.0, 458.0], [554.0, 458.0], [554.0, 472.0], [409.0, 472.0]], "confidence": 0.9644482731819153, "text": "quality as shown in Tab. 3 .", "bbox": [409.0, 458.0, 554.0, 472.0]}, {"polygon": [[297.0, 460.0], [325.0, 460.0], [325.0, 470.0], [297.0, 470.0]], "confidence": 0.8514887690544128, "text": "0.9323", "bbox": [297.0, 460.0, 325.0, 470.0]}, {"polygon": [[262.0, 461.0], [284.0, 461.0], [284.0, 470.0], [262.0, 470.0]], "confidence": 0.82118821144104, "text": "24.51", "bbox": [262.0, 461.0, 284.0, 470.0]}, {"polygon": [[344.0, 461.0], [366.0, 461.0], [366.0, 469.0], [344.0, 469.0]], "confidence": 0.8295848965644836, "text": "10.54", "bbox": [344.0, 461.0, 366.0, 469.0]}, {"polygon": [[63.0, 482.0], [384.0, 482.0], [384.0, 496.0], [63.0, 496.0]], "confidence": 0.9776830077171326, "text": "Table 4. Ablation Studies with lower-body occluded ZJU-", "bbox": [63.0, 482.0, 384.0, 496.0]}, {"polygon": [[63.0, 496.0], [382.0, 496.0], [382.0, 510.0], [63.0, 510.0]], "confidence": 0.9747750759124756, "text": "Mocap [38] 377 subject. LPIPS* = 100×LPIPS. As shown, SDS", "bbox": [63.0, 496.0, 382.0, 510.0]}, {"polygon": [[408.0, 499.0], [500.0, 499.0], [500.0, 515.0], [408.0, 515.0]], "confidence": 0.9268996119499207, "text": "6. Discussion", "bbox": [408.0, 499.0, 500.0, 515.0]}, {"polygon": [[63.0, 511.0], [378.0, 511.0], [378.0, 526.0], [63.0, 526.0]], "confidence": 0.9842392802238464, "text": "loss with textual inversion token shows the biggest improvement.", "bbox": [63.0, 511.0, 378.0, 526.0]}, {"polygon": [[425.0, 530.0], [729.0, 530.0], [729.0, 544.0], [425.0, 544.0]], "confidence": 0.9830217361450195, "text": "In this paper, we present a method to reconstruct the world", "bbox": [425.0, 530.0, 729.0, 544.0]}, {"polygon": [[409.0, 546.0], [728.0, 546.0], [728.0, 560.0], [409.0, 560.0]], "confidence": 0.9808144569396973, "text": "and dynamically moving humans in 3D from a monocular", "bbox": [409.0, 546.0, 728.0, 560.0]}, {"polygon": [[63.0, 555.0], [197.0, 555.0], [197.0, 571.0], [63.0, 571.0]], "confidence": 0.9530597925186157, "text": "5.6. Ablation Studies", "bbox": [63.0, 555.0, 197.0, 571.0]}, {"polygon": [[409.0, 563.0], [729.0, 563.0], [729.0, 576.0], [409.0, 576.0]], "confidence": 0.9839493036270142, "text": "video input, particularly focusing on sparse and limited obser-", "bbox": [409.0, 563.0, 729.0, 576.0]}, {"polygon": [[409.0, 578.0], [728.0, 578.0], [728.0, 593.0], [409.0, 593.0]], "confidence": 0.9828051924705505, "text": "vation scenarios. We represent both the world and multiple", "bbox": [409.0, 578.0, 728.0, 593.0]}, {"polygon": [[80.0, 591.0], [384.0, 591.0], [384.0, 605.0], [80.0, 605.0]], "confidence": 0.9811528325080872, "text": "We conduct an ablation study to demonstrate the impor-", "bbox": [80.0, 591.0, 384.0, 605.0]}, {"polygon": [[408.0, 594.0], [728.0, 594.0], [728.0, 608.0], [408.0, 608.0]], "confidence": 0.9824624061584473, "text": "humans via 3D Gaussian Splatting representation, enabling", "bbox": [408.0, 594.0, 728.0, 608.0]}, {"polygon": [[63.0, 607.0], [383.0, 607.0], [383.0, 622.0], [63.0, 622.0]], "confidence": 0.9822189807891846, "text": "tance of the proposed modules of our framework. As a way", "bbox": [63.0, 607.0, 383.0, 622.0]}, {"polygon": [[408.0, 610.0], [728.0, 610.0], [728.0, 624.0], [408.0, 624.0]], "confidence": 0.9775779247283936, "text": "us to conveniently and efficiently compose and render them", "bbox": [408.0, 610.0, 728.0, 624.0]}, {"polygon": [[63.0, 621.0], [383.0, 624.0], [383.0, 638.0], [63.0, 636.0]], "confidence": 0.9778932929039001, "text": "for the quantitative evaluations, we use ZJU-Mocap [ 38 ]", "bbox": [63.0, 621.0, 383.0, 638.0]}, {"polygon": [[408.0, 626.0], [728.0, 626.0], [728.0, 640.0], [408.0, 640.0]], "confidence": 0.9821817874908447, "text": "together. We also introduce a novel approach to optimize", "bbox": [408.0, 626.0, 728.0, 640.0]}, {"polygon": [[63.0, 639.0], [382.0, 639.0], [382.0, 653.0], [63.0, 653.0]], "confidence": 0.9828230738639832, "text": "dataset and simulate a challenging scenario by (1) completely", "bbox": [63.0, 639.0, 382.0, 653.0]}, {"polygon": [[408.0, 642.0], [728.0, 642.0], [728.0, 656.0], [408.0, 656.0]], "confidence": 0.9821876287460327, "text": "the 3D-GS representation in a canonical space by fusing the", "bbox": [408.0, 642.0, 728.0, 656.0]}, {"polygon": [[63.0, 654.0], [382.0, 654.0], [382.0, 669.0], [63.0, 669.0]], "confidence": 0.9830005168914795, "text": "occluding the lower body, and (2) using a few frames only for", "bbox": [63.0, 654.0, 382.0, 669.0]}, {"polygon": [[409.0, 658.0], [729.0, 658.0], [729.0, 672.0], [409.0, 672.0]], "confidence": 0.9823042154312134, "text": "sparse cues in the common space, where we leverage a pre-", "bbox": [409.0, 658.0, 729.0, 672.0]}, {"polygon": [[63.0, 670.0], [384.0, 670.0], [384.0, 684.0], [63.0, 684.0]], "confidence": 0.981910765171051, "text": "the input (10% of frames from the ZJU-Mocap 377 subject).", "bbox": [63.0, 670.0, 384.0, 684.0]}, {"polygon": [[408.0, 673.0], [728.0, 673.0], [728.0, 688.0], [408.0, 688.0]], "confidence": 0.9817615151405334, "text": "trained 2D diffusion model to synthesize unseen views by", "bbox": [408.0, 673.0, 728.0, 688.0]}, {"polygon": [[63.0, 686.0], [384.0, 686.0], [384.0, 700.0], [63.0, 700.0]], "confidence": 0.9775339365005493, "text": "An example of the artificially occluded images is shown in", "bbox": [63.0, 686.0, 384.0, 700.0]}, {"polygon": [[408.0, 689.0], [729.0, 689.0], [729.0, 703.0], [408.0, 703.0]], "confidence": 0.9823247790336609, "text": "keeping the consistency with the observed 2D appearances.", "bbox": [408.0, 689.0, 729.0, 703.0]}, {"polygon": [[63.0, 702.0], [187.0, 702.0], [187.0, 717.0], [63.0, 717.0]], "confidence": 0.9592760801315308, "text": "Fig. 6 with test results.", "bbox": [63.0, 702.0, 187.0, 717.0]}, {"polygon": [[410.0, 705.0], [729.0, 705.0], [729.0, 719.0], [410.0, 719.0]], "confidence": 0.9823728799819946, "text": "Via thorough experiments, we demonstrate the high perfor-", "bbox": [410.0, 705.0, 729.0, 719.0]}, {"polygon": [[409.0, 721.0], [728.0, 721.0], [728.0, 735.0], [409.0, 735.0]], "confidence": 0.9787678718566895, "text": "mance and efficiency of our method in various challenging", "bbox": [409.0, 721.0, 728.0, 735.0]}, {"polygon": [[79.0, 729.0], [383.0, 727.0], [383.0, 743.0], [79.0, 745.0]], "confidence": 0.9805480241775513, "text": "Our Full method can successfully reconstruct the whole", "bbox": [79.0, 729.0, 383.0, 743.0]}, {"polygon": [[409.0, 737.0], [465.0, 737.0], [465.0, 751.0], [409.0, 751.0]], "confidence": 0.898452877998352, "text": "examples.", "bbox": [409.0, 737.0, 465.0, 751.0]}, {"polygon": [[63.0, 745.0], [384.0, 745.0], [384.0, 759.0], [63.0, 759.0]], "confidence": 0.9838929176330566, "text": "part of the humans. Interestingly, it also synthesizes the com-", "bbox": [63.0, 745.0, 384.0, 759.0]}, {"polygon": [[425.0, 756.0], [730.0, 756.0], [730.0, 772.0], [425.0, 772.0]], "confidence": 0.9822822213172913, "text": "Our approach, however, still has limitations such as: (1)", "bbox": [425.0, 756.0, 730.0, 772.0]}, {"polygon": [[63.0, 761.0], [384.0, 761.0], [384.0, 776.0], [63.0, 776.0]], "confidence": 0.9838740229606628, "text": "pletely unseen short pants and shoes of the target individual,", "bbox": [63.0, 761.0, 384.0, 776.0]}, {"polygon": [[409.0, 773.0], [728.0, 773.0], [728.0, 787.0], [409.0, 787.0]], "confidence": 0.9727361798286438, "text": "SMPL fitting needs to be provided; (2) our method only", "bbox": [409.0, 773.0, 728.0, 787.0]}, {"polygon": [[63.0, 777.0], [384.0, 777.0], [384.0, 791.0], [63.0, 791.0]], "confidence": 0.9825476408004761, "text": "while the appearance and colors are different from the GT.", "bbox": [63.0, 777.0, 384.0, 791.0]}, {"polygon": [[408.0, 789.0], [730.0, 789.0], [730.0, 804.0], [408.0, 804.0]], "confidence": 0.9825097322463989, "text": "considers humans as the dynamic target, ignoring animals,", "bbox": [408.0, 789.0, 730.0, 804.0]}, {"polygon": [[63.0, 792.0], [383.0, 792.0], [383.0, 807.0], [63.0, 807.0]], "confidence": 0.9765353202819824, "text": "As expected, the output without L SDS fails to reconstruct", "bbox": [63.0, 792.0, 383.0, 807.0]}, {"polygon": [[409.0, 806.0], [729.0, 806.0], [729.0, 819.0], [409.0, 819.0]], "confidence": 0.983344554901123, "text": "cars, or other dynamic objects; (3) the quality of the synthe-", "bbox": [409.0, 806.0, 729.0, 819.0]}, {"polygon": [[63.0, 809.0], [382.0, 809.0], [382.0, 822.0], [63.0, 822.0]], "confidence": 0.9817855954170227, "text": "the unseen lower part, and also shows poor quality on the", "bbox": [63.0, 809.0, 382.0, 822.0]}, {"polygon": [[409.0, 821.0], [728.0, 821.0], [728.0, 836.0], [409.0, 836.0]], "confidence": 0.9840208888053894, "text": "sized parts are still limited with visible artifacts. All these", "bbox": [409.0, 821.0, 728.0, 836.0]}, {"polygon": [[63.0, 824.0], [383.0, 824.0], [383.0, 839.0], [63.0, 839.0]], "confidence": 0.9777981638908386, "text": "upper body due to the insufficient image frames. The output", "bbox": [63.0, 824.0, 383.0, 839.0]}, {"polygon": [[409.0, 837.0], [693.0, 837.0], [693.0, 851.0], [409.0, 851.0]], "confidence": 0.9819427728652954, "text": "limitations can be exciting future research directions.", "bbox": [409.0, 837.0, 693.0, 851.0]}, {"polygon": [[63.0, 841.0], [384.0, 841.0], [384.0, 854.0], [63.0, 854.0]], "confidence": 0.9826341271400452, "text": "without Texture Inversion (TI) reconstructs the unseen lower", "bbox": [63.0, 841.0, 384.0, 854.0]}, {"polygon": [[409.0, 855.0], [728.0, 855.0], [728.0, 870.0], [409.0, 870.0]], "confidence": 0.9803661108016968, "text": "Acknowledgements This work was supported by Samsung", "bbox": [409.0, 855.0, 728.0, 870.0]}, {"polygon": [[63.0, 856.0], [383.0, 856.0], [383.0, 871.0], [63.0, 871.0]], "confidence": 0.984260082244873, "text": "parts as well, but, interestingly, the output appearance is very", "bbox": [63.0, 856.0, 383.0, 871.0]}, {"polygon": [[63.0, 872.0], [382.0, 872.0], [382.0, 887.0], [63.0, 887.0]], "confidence": 0.9750023484230042, "text": "different from the GT. This result directly demonstrates the", "bbox": [63.0, 872.0, 382.0, 887.0]}, {"polygon": [[409.0, 872.0], [729.0, 872.0], [729.0, 886.0], [409.0, 886.0]], "confidence": 0.9817298650741577, "text": "Electronics C-Lab, NRF grant funded by the Korea govern-", "bbox": [409.0, 872.0, 729.0, 886.0]}, {"polygon": [[63.0, 888.0], [382.0, 888.0], [382.0, 902.0], [63.0, 902.0]], "confidence": 0.9826956987380981, "text": "importance of our Texture Inversion process in applying SDS", "bbox": [63.0, 888.0, 382.0, 902.0]}, {"polygon": [[409.0, 887.0], [729.0, 887.0], [729.0, 902.0], [409.0, 902.0]], "confidence": 0.9777134656906128, "text": "ment (MSIT) (No. 2022R1A2C2092724 and No. RS-2023-", "bbox": [409.0, 887.0, 729.0, 902.0]}, {"polygon": [[63.0, 904.0], [384.0, 904.0], [384.0, 918.0], [63.0, 918.0]], "confidence": 0.9837909936904907, "text": "loss, helpful in preserving the identity of the target individ-", "bbox": [63.0, 904.0, 384.0, 918.0]}, {"polygon": [[408.0, 904.0], [728.0, 904.0], [728.0, 918.0], [408.0, 918.0]], "confidence": 0.977461040019989, "text": "00218601), and IITP grant funded by the Korean government", "bbox": [408.0, 904.0, 728.0, 918.0]}, {"polygon": [[408.0, 919.0], [729.0, 919.0], [729.0, 934.0], [408.0, 934.0]], "confidence": 0.9776477217674255, "text": "(MSIT) (No.2021-0-01343). H. Joo is the corresponding au-", "bbox": [408.0, 919.0, 729.0, 934.0]}, {"polygon": [[63.0, 920.0], [382.0, 920.0], [382.0, 934.0], [63.0, 934.0]], "confidence": 0.9765011072158813, "text": "ual. We also show the quantification results in Tab. 4 , where", "bbox": [63.0, 920.0, 382.0, 934.0]}, {"polygon": [[63.0, 936.0], [384.0, 936.0], [384.0, 950.0], [63.0, 950.0]], "confidence": 0.9825999736785889, "text": "the importance of each module is also clearly demonstrated.", "bbox": [63.0, 936.0, 384.0, 950.0]}, {"polygon": [[409.0, 937.0], [437.0, 937.0], [437.0, 951.0], [409.0, 951.0]], "confidence": 0.8156669735908508, "text": "thor.", "bbox": [409.0, 937.0, 437.0, 951.0]}], "languages": ["en"], "image_bbox": [0.0, 0.0, 816.0, 1056.0], "page": 8}, {"text_lines": [{"polygon": [[64.0, 95.0], [142.0, 95.0], [142.0, 111.0], [64.0, 111.0]], "confidence": 0.9060128927230835, "text": "References", "bbox": [64.0, 95.0, 142.0, 111.0]}, {"polygon": [[408.0, 97.0], [730.0, 97.0], [730.0, 111.0], [408.0, 111.0]], "confidence": 0.9828944206237793, "text": "[17] Tianjian Jiang, Xu Chen, Jie Song, and Otmar Hilliges. In-", "bbox": [408.0, 97.0, 730.0, 111.0]}, {"polygon": [[436.0, 112.0], [729.0, 112.0], [729.0, 125.0], [436.0, 125.0]], "confidence": 0.9819236993789673, "text": "stantavatar: Learning avatars from monocular video in 60", "bbox": [436.0, 112.0, 729.0, 125.0]}, {"polygon": [[69.0, 122.0], [382.0, 122.0], [382.0, 136.0], [69.0, 136.0]], "confidence": 0.9827255606651306, "text": "[1] Ijaz Akhter, Tomas Simon, Sohaib Khan, Iain Matthews, and", "bbox": [69.0, 122.0, 382.0, 136.0]}, {"polygon": [[436.0, 127.0], [623.0, 127.0], [623.0, 141.0], [436.0, 141.0]], "confidence": 0.940666913986206, "text": "seconds. In CVPR , 2023. 2 , 6, 7, 8, 12 ,", "bbox": [436.0, 127.0, 623.0, 141.0]}, {"polygon": [[91.0, 137.0], [384.0, 137.0], [384.0, 150.0], [91.0, 150.0]], "confidence": 0.9762923121452332, "text": "Yaser Sheikh. Bilinear spatiotemporal basis models. TOG ,", "bbox": [91.0, 137.0, 384.0, 150.0]}, {"polygon": [[408.0, 141.0], [729.0, 141.0], [729.0, 156.0], [408.0, 156.0]], "confidence": 0.9824950695037842, "text": "[18] Wei Jiang, Kwang Moo Yi, Golnoosh Samei, Oncel Tuzel,", "bbox": [408.0, 141.0, 729.0, 156.0]}, {"polygon": [[91.0, 152.0], [131.0, 152.0], [131.0, 165.0], [91.0, 165.0]], "confidence": 0.8591271042823792, "text": "2012. 2", "bbox": [91.0, 152.0, 131.0, 165.0]}, {"polygon": [[436.0, 157.0], [729.0, 157.0], [729.0, 170.0], [436.0, 170.0]], "confidence": 0.977180004119873, "text": "and Anurag Ranjan. Neuman: Neural human radiance field", "bbox": [436.0, 157.0, 729.0, 170.0]}, {"polygon": [[69.0, 168.0], [383.0, 168.0], [383.0, 182.0], [69.0, 182.0]], "confidence": 0.9818059802055359, "text": "[2] Thiemo Alldieck, Marcus Magnor, Weipeng Xu, Christian", "bbox": [69.0, 168.0, 383.0, 182.0]}, {"polygon": [[435.0, 172.0], [636.0, 172.0], [636.0, 185.0], [435.0, 185.0]], "confidence": 0.9721343517303467, "text": "from a single video. In ECCV , 2022. 2 , 3", "bbox": [435.0, 172.0, 636.0, 185.0]}, {"polygon": [[91.0, 183.0], [383.0, 183.0], [383.0, 196.0], [91.0, 196.0]], "confidence": 0.9816215634346008, "text": "Theobalt, and Gerard Pons-Moll. Video based reconstruction", "bbox": [91.0, 183.0, 383.0, 196.0]}, {"polygon": [[408.0, 185.0], [729.0, 187.0], [729.0, 201.0], [408.0, 199.0]], "confidence": 0.9811901450157166, "text": "[19] Hanbyul Joo, Tomas Simon, Xulong Li, Hao Liu, Lei Tan,", "bbox": [408.0, 185.0, 729.0, 201.0]}, {"polygon": [[91.0, 198.0], [302.0, 198.0], [302.0, 211.0], [91.0, 211.0]], "confidence": 0.9556673169136047, "text": "of 3d people models. In CVPR , 2018. 2 , 12 ,", "bbox": [91.0, 198.0, 302.0, 211.0]}, {"polygon": [[435.0, 201.0], [729.0, 201.0], [729.0, 214.0], [435.0, 214.0]], "confidence": 0.9829418063163757, "text": "Lin Gui, Sean Banerjee, Timothy Scott Godisart, Bart Nabbe,", "bbox": [435.0, 201.0, 729.0, 214.0]}, {"polygon": [[69.0, 213.0], [383.0, 213.0], [383.0, 227.0], [69.0, 227.0]], "confidence": 0.9825040698051453, "text": "[3] Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter", "bbox": [69.0, 213.0, 383.0, 227.0]}, {"polygon": [[435.0, 215.0], [728.0, 215.0], [728.0, 230.0], [435.0, 230.0]], "confidence": 0.9813299775123596, "text": "Iain Matthews, Takeo Kanade, Shohei Nobuhara, and Yaser", "bbox": [435.0, 215.0, 728.0, 230.0]}, {"polygon": [[90.0, 228.0], [384.0, 228.0], [384.0, 242.0], [90.0, 242.0]], "confidence": 0.9763643741607666, "text": "Gehler, Javier Romero, and Michael J. Black. Keep it SMPL:", "bbox": [90.0, 228.0, 384.0, 242.0]}, {"polygon": [[435.0, 230.0], [728.0, 230.0], [728.0, 244.0], [435.0, 244.0]], "confidence": 0.981698215007782, "text": "Sheikh. Panoptic studio: A massively multiview system for", "bbox": [435.0, 230.0, 728.0, 244.0]}, {"polygon": [[91.0, 243.0], [384.0, 242.0], [384.0, 256.0], [91.0, 257.0]], "confidence": 0.981512188911438, "text": "Automatic estimation of 3D human pose and shape from a", "bbox": [91.0, 243.0, 384.0, 256.0]}, {"polygon": [[436.0, 246.0], [681.0, 246.0], [681.0, 258.0], [436.0, 258.0]], "confidence": 0.966044545173645, "text": "social interaction capture. TPAMI , 2017. 6, 12 , 13", "bbox": [436.0, 246.0, 681.0, 258.0]}, {"polygon": [[90.0, 257.0], [267.0, 257.0], [267.0, 271.0], [90.0, 271.0]], "confidence": 0.9592745900154114, "text": "single image. In ECCV , 2016. 6, 13", "bbox": [90.0, 257.0, 267.0, 271.0]}, {"polygon": [[409.0, 260.0], [728.0, 260.0], [728.0, 273.0], [409.0, 273.0]], "confidence": 0.9816452264785767, "text": "[20] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and", "bbox": [409.0, 260.0, 728.0, 273.0]}, {"polygon": [[69.0, 273.0], [384.0, 273.0], [384.0, 287.0], [69.0, 287.0]], "confidence": 0.9825231432914734, "text": "[4] Z. Cao, G. Hidalgo Martinez, T. Simon, S. Wei, and Y. A.", "bbox": [69.0, 273.0, 384.0, 287.0]}, {"polygon": [[435.0, 274.0], [728.0, 274.0], [728.0, 287.0], [435.0, 287.0]], "confidence": 0.9829914569854736, "text": "George Drettakis. 3d gaussian splatting for real-time radiance", "bbox": [435.0, 274.0, 728.0, 287.0]}, {"polygon": [[91.0, 288.0], [383.0, 288.0], [383.0, 301.0], [91.0, 301.0]], "confidence": 0.9809946417808533, "text": "Sheikh. Openpose: Realtime multi-person 2d pose estimation", "bbox": [91.0, 288.0, 383.0, 301.0]}, {"polygon": [[435.0, 289.0], [673.0, 289.0], [673.0, 303.0], [435.0, 303.0]], "confidence": 0.9682387113571167, "text": "field rendering. In SIGGRAPH , 2023. 2 , 3 , 4 , 12", "bbox": [435.0, 289.0, 673.0, 303.0]}, {"polygon": [[90.0, 303.0], [292.0, 303.0], [292.0, 316.0], [90.0, 316.0]], "confidence": 0.9703872203826904, "text": "using part affinity fields. TPAMI , 2019. 5", "bbox": [90.0, 303.0, 292.0, 316.0]}, {"polygon": [[408.0, 303.0], [728.0, 303.0], [728.0, 318.0], [408.0, 318.0]], "confidence": 0.9770271182060242, "text": "[21] Diederik P Kingma and Jimmy Ba.  Adam: A method for", "bbox": [408.0, 303.0, 728.0, 318.0]}, {"polygon": [[69.0, 318.0], [384.0, 318.0], [384.0, 332.0], [69.0, 332.0]], "confidence": 0.981661856174469, "text": "[5] Eric R. Chan, Koki Nagano, Matthew A. Chan, Alexander W.", "bbox": [69.0, 318.0, 384.0, 332.0]}, {"polygon": [[436.0, 319.0], [663.0, 319.0], [663.0, 332.0], [436.0, 332.0]], "confidence": 0.9725127816200256, "text": "stochastic optimization. In ICLR , 2015. 12 , 14", "bbox": [436.0, 319.0, 663.0, 332.0]}, {"polygon": [[90.0, 334.0], [382.0, 334.0], [382.0, 347.0], [90.0, 347.0]], "confidence": 0.9827595353126526, "text": "Bergman, Jeong Joon Park, Axel Levy, Miika Aittala, Shalini", "bbox": [90.0, 334.0, 382.0, 347.0]}, {"polygon": [[408.0, 334.0], [729.0, 334.0], [729.0, 347.0], [408.0, 347.0]], "confidence": 0.9828142523765564, "text": "[22] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao,", "bbox": [408.0, 334.0, 729.0, 347.0]}, {"polygon": [[90.0, 348.0], [383.0, 348.0], [383.0, 361.0], [90.0, 361.0]], "confidence": 0.9750657677650452, "text": "De Mello, Tero Karras, and Gordon Wetzstein. Generative", "bbox": [90.0, 348.0, 383.0, 361.0]}, {"polygon": [[435.0, 348.0], [729.0, 348.0], [729.0, 361.0], [435.0, 361.0]], "confidence": 0.9819749593734741, "text": "Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer White-", "bbox": [435.0, 348.0, 729.0, 361.0]}, {"polygon": [[90.0, 363.0], [384.0, 363.0], [384.0, 376.0], [90.0, 376.0]], "confidence": 0.9796952605247498, "text": "novel view synthesis with 3d-aware diffusion models.  In", "bbox": [90.0, 363.0, 384.0, 376.0]}, {"polygon": [[435.0, 363.0], [729.0, 363.0], [729.0, 377.0], [435.0, 377.0]], "confidence": 0.9673100709915161, "text": "head, Alexander C Berg, Wan-Yen Lo, et al.  Segment any-", "bbox": [435.0, 363.0, 729.0, 377.0]}, {"polygon": [[90.0, 377.0], [164.0, 377.0], [164.0, 392.0], [90.0, 392.0]], "confidence": 0.9133464097976685, "text": "ICCV , 2023. 2.", "bbox": [90.0, 377.0, 164.0, 392.0]}, {"polygon": [[435.0, 377.0], [572.0, 377.0], [572.0, 391.0], [435.0, 391.0]], "confidence": 0.9337607622146606, "text": "thing. In ICCV , 2023. 6, 13", "bbox": [435.0, 377.0, 572.0, 391.0]}, {"polygon": [[408.0, 392.0], [730.0, 392.0], [730.0, 406.0], [408.0, 406.0]], "confidence": 0.9829674363136292, "text": "[23] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shecht-", "bbox": [408.0, 392.0, 730.0, 406.0]}, {"polygon": [[69.0, 394.0], [384.0, 394.0], [384.0, 407.0], [69.0, 407.0]], "confidence": 0.9777867794036865, "text": "[6] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fan-", "bbox": [69.0, 394.0, 384.0, 407.0]}, {"polygon": [[90.0, 409.0], [384.0, 409.0], [384.0, 423.0], [90.0, 423.0]], "confidence": 0.9818432927131653, "text": "tasia3d: Disentangling geometry and appearance for high-", "bbox": [90.0, 409.0, 384.0, 423.0]}, {"polygon": [[436.0, 409.0], [730.0, 409.0], [730.0, 421.0], [436.0, 421.0]], "confidence": 0.9788305759429932, "text": "man, and Jun-Yan Zhu. Multi-concept customization of text-", "bbox": [436.0, 409.0, 730.0, 421.0]}, {"polygon": [[91.0, 423.0], [348.0, 423.0], [348.0, 437.0], [91.0, 437.0]], "confidence": 0.9787793755531311, "text": "quality text-to-3d content creation. In ICCV , 2023. 2", "bbox": [91.0, 423.0, 348.0, 437.0]}, {"polygon": [[435.0, 422.0], [650.0, 422.0], [650.0, 435.0], [435.0, 435.0]], "confidence": 0.9616944789886475, "text": "to-image diffusion. In CVPR , 2023. 4, 5, 13", "bbox": [435.0, 422.0, 650.0, 435.0]}, {"polygon": [[409.0, 437.0], [729.0, 437.0], [729.0, 450.0], [409.0, 450.0]], "confidence": 0.9840152859687805, "text": "[24] Abhijit Kundu, Kyle Genova, Xiaoqi Yin, Alireza Fathi, Caro-", "bbox": [409.0, 437.0, 729.0, 450.0]}, {"polygon": [[69.0, 440.0], [382.0, 439.0], [382.0, 453.0], [69.0, 454.0]], "confidence": 0.9821933507919312, "text": "[7] Xu Chen, Tianjian Jiang, Jie Song, Max Rietmann, Andreas", "bbox": [69.0, 440.0, 382.0, 453.0]}, {"polygon": [[435.0, 451.0], [728.0, 451.0], [728.0, 465.0], [435.0, 465.0]], "confidence": 0.9827966690063477, "text": "line Pantofaru, Leonidas Guibas, Andrea Tagliasacchi, Frank", "bbox": [435.0, 451.0, 728.0, 465.0]}, {"polygon": [[90.0, 454.0], [383.0, 454.0], [383.0, 468.0], [90.0, 468.0]], "confidence": 0.9799954295158386, "text": "Geiger, Michael J Black, and Otmar Hilliges. Fast-snarf: A", "bbox": [90.0, 454.0, 383.0, 468.0]}, {"polygon": [[435.0, 466.0], [730.0, 466.0], [730.0, 480.0], [435.0, 480.0]], "confidence": 0.981377124786377, "text": "Dellaert, and Thomas Funkhouser. Panoptic Neural Fields:", "bbox": [435.0, 466.0, 730.0, 480.0]}, {"polygon": [[90.0, 469.0], [376.0, 469.0], [376.0, 484.0], [90.0, 484.0]], "confidence": 0.9775121808052063, "text": "fast deformer for articulated neural fields. TPAMI , 2023. 2", "bbox": [90.0, 469.0, 376.0, 484.0]}, {"polygon": [[435.0, 480.0], [729.0, 480.0], [729.0, 494.0], [435.0, 494.0]], "confidence": 0.9780479073524475, "text": "A Semantic Object-Aware Neural Scene Representation. In", "bbox": [435.0, 480.0, 729.0, 494.0]}, {"polygon": [[70.0, 484.0], [383.0, 484.0], [383.0, 498.0], [70.0, 498.0]], "confidence": 0.9810930490493774, "text": "[8] Xu Chen, Yufeng Zheng, Michael J Black, Otmar Hilliges,", "bbox": [70.0, 484.0, 383.0, 498.0]}, {"polygon": [[436.0, 496.0], [512.0, 496.0], [512.0, 509.0], [436.0, 509.0]], "confidence": 0.9208239912986755, "text": "CVPR , 2022. 3", "bbox": [436.0, 496.0, 512.0, 509.0]}, {"polygon": [[90.0, 499.0], [382.0, 499.0], [382.0, 513.0], [90.0, 513.0]], "confidence": 0.9820279479026794, "text": "and Andreas Geiger. Snarf: Differentiable forward skinning", "bbox": [90.0, 499.0, 382.0, 513.0]}, {"polygon": [[409.0, 510.0], [728.0, 510.0], [728.0, 523.0], [409.0, 523.0]], "confidence": 0.9797364473342896, "text": "[25] Jiabao Lei, Jiapeng Tang, and Kui Jia. Rgbd2: Generative", "bbox": [409.0, 510.0, 728.0, 523.0]}, {"polygon": [[90.0, 513.0], [384.0, 513.0], [384.0, 527.0], [90.0, 527.0]], "confidence": 0.9828572273254395, "text": "for animating non-rigid neural implicit shapes. In ICCV , 2021.", "bbox": [90.0, 513.0, 384.0, 527.0]}, {"polygon": [[436.0, 525.0], [729.0, 525.0], [729.0, 539.0], [436.0, 539.0]], "confidence": 0.982798159122467, "text": "scene synthesis via incremental view inpainting using rgbd", "bbox": [436.0, 525.0, 729.0, 539.0]}, {"polygon": [[435.0, 540.0], [613.0, 540.0], [613.0, 553.0], [435.0, 553.0]], "confidence": 0.9567283987998962, "text": "diffusion models. In CVPR , 2023. 2", "bbox": [435.0, 540.0, 613.0, 553.0]}, {"polygon": [[69.0, 545.0], [383.0, 545.0], [383.0, 558.0], [69.0, 558.0]], "confidence": 0.9825401306152344, "text": "[9] Congyue Deng, Chiyu Jiang, Charles R Qi, Xinchen Yan, Yin", "bbox": [69.0, 545.0, 383.0, 558.0]}, {"polygon": [[409.0, 555.0], [728.0, 555.0], [728.0, 568.0], [409.0, 568.0]], "confidence": 0.9828483462333679, "text": "[26] Tingting Liao, Hongwei Yi, Yuliang Xiu, Jiaxaing Tang,", "bbox": [409.0, 555.0, 728.0, 568.0]}, {"polygon": [[90.0, 559.0], [384.0, 559.0], [384.0, 572.0], [90.0, 572.0]], "confidence": 0.971609354019165, "text": "Zhou, Leonidas Guibas, Dragomir Anguelov, et al.  Nerdi:", "bbox": [90.0, 559.0, 384.0, 572.0]}, {"polygon": [[437.0, 570.0], [728.0, 570.0], [728.0, 583.0], [437.0, 583.0]], "confidence": 0.9803667068481445, "text": "Yangyi Huang, Justus Thies, and Michael J Black.  Tada!", "bbox": [437.0, 570.0, 728.0, 583.0]}, {"polygon": [[90.0, 574.0], [382.0, 574.0], [382.0, 587.0], [90.0, 587.0]], "confidence": 0.9780855178833008, "text": "Single-view nerf synthesis with language-guided diffusion as", "bbox": [90.0, 574.0, 382.0, 587.0]}, {"polygon": [[435.0, 584.0], [680.0, 584.0], [680.0, 597.0], [435.0, 597.0]], "confidence": 0.9753994345664978, "text": "text to animatable digital avatars. In 3DV , 2024. 2", "bbox": [435.0, 584.0, 680.0, 597.0]}, {"polygon": [[90.0, 588.0], [286.0, 588.0], [286.0, 603.0], [90.0, 603.0]], "confidence": 0.9648851156234741, "text": "general image priors. In CVPR , 2023. 2", "bbox": [90.0, 588.0, 286.0, 603.0]}, {"polygon": [[409.0, 599.0], [729.0, 599.0], [729.0, 612.0], [409.0, 612.0]], "confidence": 0.9790294170379639, "text": "[27] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,", "bbox": [409.0, 599.0, 729.0, 612.0]}, {"polygon": [[63.0, 605.0], [383.0, 605.0], [383.0, 618.0], [63.0, 618.0]], "confidence": 0.9823746681213379, "text": "[10] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik,", "bbox": [63.0, 605.0, 383.0, 618.0]}, {"polygon": [[435.0, 613.0], [729.0, 613.0], [729.0, 627.0], [435.0, 627.0]], "confidence": 0.9824628829956055, "text": "Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-", "bbox": [435.0, 613.0, 729.0, 627.0]}, {"polygon": [[91.0, 619.0], [384.0, 619.0], [384.0, 632.0], [91.0, 632.0]], "confidence": 0.9764959812164307, "text": "Amit H. Bermano, Gal Chechik, and Daniel Cohen-Or. An", "bbox": [91.0, 619.0, 384.0, 632.0]}, {"polygon": [[435.0, 627.0], [729.0, 627.0], [729.0, 642.0], [435.0, 642.0]], "confidence": 0.9794237613677979, "text": "Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-", "bbox": [435.0, 627.0, 729.0, 642.0]}, {"polygon": [[90.0, 634.0], [384.0, 634.0], [384.0, 648.0], [90.0, 648.0]], "confidence": 0.9817578792572021, "text": "image is worth one word: Personalizing text-to-image genera-", "bbox": [90.0, 634.0, 384.0, 648.0]}, {"polygon": [[436.0, 643.0], [623.0, 643.0], [623.0, 656.0], [436.0, 656.0]], "confidence": 0.9563225507736206, "text": "3d content creation. In CVPR , 2023. 2 .", "bbox": [436.0, 643.0, 623.0, 656.0]}, {"polygon": [[90.0, 649.0], [325.0, 649.0], [325.0, 663.0], [90.0, 663.0]], "confidence": 0.9781983494758606, "text": "tion using textual inversion. In ICLR , 2023. 2 , 5", "bbox": [90.0, 649.0, 325.0, 663.0]}, {"polygon": [[408.0, 658.0], [729.0, 658.0], [729.0, 671.0], [408.0, 671.0]], "confidence": 0.9835520386695862, "text": "[28] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov,", "bbox": [408.0, 658.0, 729.0, 671.0]}, {"polygon": [[63.0, 665.0], [383.0, 664.0], [383.0, 679.0], [63.0, 680.0]], "confidence": 0.9827243089675903, "text": "[11] Shubham Goel, Georgios Pavlakos, Jathushan Rajasegaran,", "bbox": [63.0, 665.0, 383.0, 679.0]}, {"polygon": [[435.0, 672.0], [728.0, 672.0], [728.0, 685.0], [435.0, 685.0]], "confidence": 0.9788938164710999, "text": "Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot", "bbox": [435.0, 672.0, 728.0, 685.0]}, {"polygon": [[90.0, 680.0], [384.0, 680.0], [384.0, 693.0], [90.0, 693.0]], "confidence": 0.9771668910980225, "text": "Angjoo Kanazawa*, and Jitendra Malik*.  Humans in 4D:", "bbox": [90.0, 680.0, 384.0, 693.0]}, {"polygon": [[435.0, 687.0], [638.0, 687.0], [638.0, 700.0], [435.0, 700.0]], "confidence": 0.9563201665878296, "text": "one image to 3d object. In ICCV , 2023. 2 .", "bbox": [435.0, 687.0, 638.0, 700.0]}, {"polygon": [[90.0, 695.0], [384.0, 695.0], [384.0, 709.0], [90.0, 709.0]], "confidence": 0.9759030938148499, "text": "Reconstructing and tracking humans with transformers.  In", "bbox": [90.0, 695.0, 384.0, 709.0]}, {"polygon": [[408.0, 701.0], [729.0, 701.0], [729.0, 715.0], [408.0, 715.0]], "confidence": 0.9815354943275452, "text": "[29] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard", "bbox": [408.0, 701.0, 729.0, 715.0]}, {"polygon": [[90.0, 709.0], [171.0, 709.0], [171.0, 723.0], [90.0, 723.0]], "confidence": 0.9326903223991394, "text": "ICCV , 2023. 13", "bbox": [90.0, 709.0, 171.0, 723.0]}, {"polygon": [[435.0, 716.0], [729.0, 716.0], [729.0, 730.0], [435.0, 730.0]], "confidence": 0.9758070707321167, "text": "Pons-Moll, and Michael J. Black.  Smpl: A skinned multi-", "bbox": [435.0, 716.0, 729.0, 730.0]}, {"polygon": [[63.0, 726.0], [383.0, 726.0], [383.0, 740.0], [63.0, 740.0]], "confidence": 0.9824289679527283, "text": "[12] Chen Guo, Tianjian Jiang, Xu Chen, Jie Song, and Otmar", "bbox": [63.0, 726.0, 383.0, 740.0]}, {"polygon": [[436.0, 732.0], [632.0, 732.0], [632.0, 744.0], [436.0, 744.0]], "confidence": 0.9745943546295166, "text": "person linear model. TOG , 2015. 1 , 2 , 4", "bbox": [436.0, 732.0, 632.0, 744.0]}, {"polygon": [[90.0, 740.0], [383.0, 740.0], [383.0, 754.0], [90.0, 754.0]], "confidence": 0.9813126921653748, "text": "Hilliges. Vid2avatar: 3d avatar reconstruction from videos in", "bbox": [90.0, 740.0, 383.0, 754.0]}, {"polygon": [[409.0, 746.0], [728.0, 746.0], [728.0, 759.0], [409.0, 759.0]], "confidence": 0.9823905825614929, "text": "[30] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher", "bbox": [409.0, 746.0, 728.0, 759.0]}, {"polygon": [[90.0, 755.0], [384.0, 755.0], [384.0, 769.0], [90.0, 769.0]], "confidence": 0.9790489077568054, "text": "the wild via self-supervised scene decomposition. In CVPR ,", "bbox": [90.0, 755.0, 384.0, 769.0]}, {"polygon": [[437.0, 761.0], [728.0, 761.0], [728.0, 773.0], [437.0, 773.0]], "confidence": 0.9774070382118225, "text": "Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting", "bbox": [437.0, 761.0, 728.0, 773.0]}, {"polygon": [[91.0, 770.0], [156.0, 770.0], [156.0, 783.0], [91.0, 783.0]], "confidence": 0.878085196018219, "text": "2023. 2, 3 , 4", "bbox": [91.0, 770.0, 156.0, 783.0]}, {"polygon": [[435.0, 775.0], [729.0, 775.0], [729.0, 789.0], [435.0, 789.0]], "confidence": 0.9761993885040283, "text": "using denoising diffusion probabilistic models.  In CVPR ,", "bbox": [435.0, 775.0, 729.0, 789.0]}, {"polygon": [[63.0, 785.0], [382.0, 785.0], [382.0, 799.0], [63.0, 799.0]], "confidence": 0.9825960397720337, "text": "[13] R. I. Hartley and A. Zisserman. Multiple View Geometry", "bbox": [63.0, 785.0, 382.0, 799.0]}, {"polygon": [[435.0, 789.0], [476.0, 789.0], [476.0, 803.0], [435.0, 803.0]], "confidence": 0.8181880712509155, "text": "2022. 2", "bbox": [435.0, 789.0, 476.0, 803.0]}, {"polygon": [[90.0, 799.0], [384.0, 799.0], [384.0, 814.0], [90.0, 814.0]], "confidence": 0.9723103642463684, "text": "in Computer Vision .  Cambridge University Press, ISBN:", "bbox": [90.0, 799.0, 384.0, 814.0]}, {"polygon": [[408.0, 805.0], [730.0, 805.0], [730.0, 818.0], [408.0, 818.0]], "confidence": 0.9812848567962646, "text": "[31] Naureen Mahmood, Nima Ghorbani, Nikolaus F. Troje, Ger-", "bbox": [408.0, 805.0, 730.0, 818.0]}, {"polygon": [[90.0, 815.0], [273.0, 815.0], [273.0, 828.0], [90.0, 828.0]], "confidence": 0.9682461619377136, "text": "0521540518, second edition, 2004. 2", "bbox": [90.0, 815.0, 273.0, 828.0]}, {"polygon": [[436.0, 820.0], [728.0, 820.0], [728.0, 833.0], [436.0, 833.0]], "confidence": 0.9759515523910522, "text": "ard Pons-Moll, and Michael J. Black. AMASS: Archive of", "bbox": [436.0, 820.0, 728.0, 833.0]}, {"polygon": [[63.0, 830.0], [384.0, 830.0], [384.0, 844.0], [63.0, 844.0]], "confidence": 0.9830008149147034, "text": "[14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-", "bbox": [63.0, 830.0, 384.0, 844.0]}, {"polygon": [[435.0, 834.0], [687.0, 834.0], [687.0, 847.0], [435.0, 847.0]], "confidence": 0.9796360731124878, "text": "motion capture as surface shapes. In ICCV , 2019. 4", "bbox": [435.0, 834.0, 687.0, 847.0]}, {"polygon": [[91.0, 845.0], [319.0, 845.0], [319.0, 859.0], [91.0, 859.0]], "confidence": 0.9747971892356873, "text": "sion probabilistic models. In NeurIPS , 2020. 2", "bbox": [91.0, 845.0, 319.0, 859.0]}, {"polygon": [[408.0, 849.0], [729.0, 849.0], [729.0, 862.0], [408.0, 862.0]], "confidence": 0.9822784066200256, "text": "[32] Luke Melas-Kyriazi, Iro Laina, Christian Rupprecht, and An-", "bbox": [408.0, 849.0, 729.0, 862.0]}, {"polygon": [[63.0, 861.0], [384.0, 861.0], [384.0, 875.0], [63.0, 875.0]], "confidence": 0.9834403395652771, "text": "[15] Yangyi Huang, Hongwei Yi, Yuliang Xiu, Tingting Liao, Ji-", "bbox": [63.0, 861.0, 384.0, 875.0]}, {"polygon": [[435.0, 863.0], [728.0, 863.0], [728.0, 877.0], [435.0, 877.0]], "confidence": 0.9807043671607971, "text": "drea Vedaldi. Realfusion: 360deg reconstruction of any object", "bbox": [435.0, 863.0, 728.0, 877.0]}, {"polygon": [[91.0, 876.0], [383.0, 876.0], [383.0, 890.0], [91.0, 890.0]], "confidence": 0.9795830845832825, "text": "axiang Tang, Deng Cai, and Justus Thies. TeCH: Text-guided", "bbox": [91.0, 876.0, 383.0, 890.0]}, {"polygon": [[435.0, 878.0], [626.0, 878.0], [626.0, 892.0], [435.0, 892.0]], "confidence": 0.9722926020622253, "text": "from a single image. In CVPR , 2023. 2", "bbox": [435.0, 878.0, 626.0, 892.0]}, {"polygon": [[90.0, 891.0], [383.0, 891.0], [383.0, 905.0], [90.0, 905.0]], "confidence": 0.9810672402381897, "text": "Reconstruction of Lifelike Clothed Humans. In 3DV , 2024. 2", "bbox": [90.0, 891.0, 383.0, 905.0]}, {"polygon": [[409.0, 894.0], [729.0, 894.0], [729.0, 907.0], [409.0, 907.0]], "confidence": 0.9813593626022339, "text": "[33] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,", "bbox": [409.0, 894.0, 729.0, 907.0]}, {"polygon": [[63.0, 908.0], [384.0, 908.0], [384.0, 921.0], [63.0, 921.0]], "confidence": 0.9825553297996521, "text": "[16] Boyi Jiang, Yang Hong, Hujun Bao, and Juyong Zhang. Self-", "bbox": [63.0, 908.0, 384.0, 921.0]}, {"polygon": [[435.0, 908.0], [730.0, 908.0], [730.0, 921.0], [435.0, 921.0]], "confidence": 0.9738876223564148, "text": "Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng.  Nerf:", "bbox": [435.0, 908.0, 730.0, 921.0]}, {"polygon": [[90.0, 922.0], [382.0, 922.0], [382.0, 935.0], [90.0, 935.0]], "confidence": 0.9832958579063416, "text": "recon: Self reconstruction your digital avatar from monocular", "bbox": [90.0, 922.0, 382.0, 935.0]}, {"polygon": [[435.0, 922.0], [728.0, 922.0], [728.0, 935.0], [435.0, 935.0]], "confidence": 0.9814012050628662, "text": "Representing scenes as neural radiance fields for view synthe-", "bbox": [435.0, 922.0, 728.0, 935.0]}, {"polygon": [[90.0, 937.0], [225.0, 937.0], [225.0, 951.0], [90.0, 951.0]], "confidence": 0.9425650238990784, "text": "video. In CVPR , 2022. 2 , 5", "bbox": [90.0, 937.0, 225.0, 951.0]}, {"polygon": [[435.0, 937.0], [588.0, 937.0], [588.0, 951.0], [435.0, 951.0]], "confidence": 0.9528473615646362, "text": "sis. In ECCV , 2020. 2, 3, 6, 12", "bbox": [435.0, 937.0, 588.0, 951.0]}, {"polygon": [[391.0, 976.0], [401.0, 976.0], [401.0, 988.0], [391.0, 988.0]], "confidence": 0.4812418222427368, "text": "9", "bbox": [391.0, 976.0, 401.0, 988.0]}], "languages": ["en"], "image_bbox": [0.0, 0.0, 816.0, 1056.0], "page": 9}, {"text_lines": [{"polygon": [[408.0, 96.0], [729.0, 96.0], [729.0, 111.0], [408.0, 111.0]], "confidence": 0.9813330769538879, "text": "[51] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh,", "bbox": [408.0, 96.0, 729.0, 111.0]}, {"polygon": [[63.0, 97.0], [384.0, 97.0], [384.0, 111.0], [63.0, 111.0]], "confidence": 0.9830514788627625, "text": "[34] Thomas Müller, Alex Evans, Christoph Schied, and Alexander", "bbox": [63.0, 97.0, 384.0, 111.0]}, {"polygon": [[91.0, 112.0], [384.0, 112.0], [384.0, 125.0], [91.0, 125.0]], "confidence": 0.9836279153823853, "text": "Keller. Instant neural graphics primitives with a multiresolu-", "bbox": [91.0, 112.0, 384.0, 125.0]}, {"polygon": [[436.0, 112.0], [728.0, 112.0], [728.0, 125.0], [436.0, 125.0]], "confidence": 0.9733413457870483, "text": "and Greg Shakhnarovich. Score jacobian chaining: Lifting", "bbox": [436.0, 112.0, 728.0, 125.0]}, {"polygon": [[90.0, 127.0], [303.0, 127.0], [303.0, 141.0], [90.0, 141.0]], "confidence": 0.973542332649231, "text": "tion hash encoding. In SIGGRAPH , 2022. 2", "bbox": [90.0, 127.0, 303.0, 141.0]}, {"polygon": [[436.0, 127.0], [729.0, 127.0], [729.0, 141.0], [436.0, 141.0]], "confidence": 0.9777420163154602, "text": "pretrained 2d diffusion models for 3d generation. In CVPR ,", "bbox": [436.0, 127.0, 729.0, 141.0]}, {"polygon": [[63.0, 142.0], [382.0, 141.0], [382.0, 156.0], [63.0, 157.0]], "confidence": 0.9824318885803223, "text": "[35] Natalia Neverova, David Novotny, Vasil Khalidov, Marc", "bbox": [63.0, 142.0, 382.0, 156.0]}, {"polygon": [[437.0, 142.0], [476.0, 142.0], [476.0, 155.0], [437.0, 155.0]], "confidence": 0.8580737709999084, "text": "2023. 2", "bbox": [437.0, 142.0, 476.0, 155.0]}, {"polygon": [[90.0, 157.0], [383.0, 157.0], [383.0, 170.0], [90.0, 170.0]], "confidence": 0.982973039150238, "text": "Szafraniec, Patrick Labatut, and Andrea Vedaldi. Continuous", "bbox": [90.0, 157.0, 383.0, 170.0]}, {"polygon": [[408.0, 158.0], [728.0, 158.0], [728.0, 171.0], [408.0, 171.0]], "confidence": 0.9831146001815796, "text": "[52] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku", "bbox": [408.0, 158.0, 728.0, 171.0]}, {"polygon": [[91.0, 172.0], [294.0, 172.0], [294.0, 185.0], [91.0, 185.0]], "confidence": 0.9658913016319275, "text": "surface embeddings. In NeurIPS , 2020. 2", "bbox": [91.0, 172.0, 294.0, 185.0]}, {"polygon": [[435.0, 172.0], [728.0, 172.0], [728.0, 185.0], [435.0, 185.0]], "confidence": 0.9820274710655212, "text": "Komura, and Wenping Wang. Neus: Learning neural implicit", "bbox": [435.0, 172.0, 728.0, 185.0]}, {"polygon": [[63.0, 186.0], [383.0, 185.0], [383.0, 199.0], [63.0, 200.0]], "confidence": 0.9834859371185303, "text": "[36] Julian Ost, Fahim Mannan, Nils Thuerey, Julian Knodt, and", "bbox": [63.0, 186.0, 383.0, 199.0]}, {"polygon": [[435.0, 187.0], [728.0, 187.0], [728.0, 200.0], [435.0, 200.0]], "confidence": 0.9816661477088928, "text": "surfaces by volume rendering for multi-view reconstruction.", "bbox": [435.0, 187.0, 728.0, 200.0]}, {"polygon": [[90.0, 201.0], [383.0, 201.0], [383.0, 214.0], [90.0, 214.0]], "confidence": 0.9770755171775818, "text": "Felix Heide.  Neural scene graphs for dynamic scenes.  In", "bbox": [90.0, 201.0, 383.0, 214.0]}, {"polygon": [[435.0, 201.0], [537.0, 201.0], [537.0, 216.0], [435.0, 216.0]], "confidence": 0.9448140263557434, "text": "In NeurIPS , 2021. 2", "bbox": [435.0, 201.0, 537.0, 216.0]}, {"polygon": [[90.0, 215.0], [179.0, 215.0], [179.0, 229.0], [90.0, 229.0]], "confidence": 0.9351382255554199, "text": "CVPR , 2021. 2 , 3", "bbox": [90.0, 215.0, 179.0, 229.0]}, {"polygon": [[409.0, 218.0], [728.0, 218.0], [728.0, 231.0], [409.0, 231.0]], "confidence": 0.982391357421875, "text": "[53] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan", "bbox": [409.0, 218.0, 728.0, 231.0]}, {"polygon": [[63.0, 230.0], [383.0, 230.0], [383.0, 244.0], [63.0, 244.0]], "confidence": 0.9818506240844727, "text": "[37] Hyun Soo Park, Takaaki Shiratori, Iain Matthews, and Yaser", "bbox": [63.0, 230.0, 383.0, 244.0]}, {"polygon": [[436.0, 233.0], [728.0, 233.0], [728.0, 246.0], [436.0, 246.0]], "confidence": 0.9760345816612244, "text": "Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and", "bbox": [436.0, 233.0, 728.0, 246.0]}, {"polygon": [[91.0, 246.0], [383.0, 246.0], [383.0, 258.0], [91.0, 258.0]], "confidence": 0.9833137392997742, "text": "Sheikh. 3d reconstruction of a moving point from a series of", "bbox": [91.0, 246.0, 383.0, 258.0]}, {"polygon": [[435.0, 247.0], [729.0, 247.0], [729.0, 261.0], [435.0, 261.0]], "confidence": 0.9817146062850952, "text": "diverse text-to-3d generation with variational score distilla-", "bbox": [435.0, 247.0, 729.0, 261.0]}, {"polygon": [[91.0, 260.0], [257.0, 260.0], [257.0, 273.0], [91.0, 273.0]], "confidence": 0.9619467258453369, "text": "2d projections. In ECCV , 2010. 2", "bbox": [91.0, 260.0, 257.0, 273.0]}, {"polygon": [[435.0, 262.0], [562.0, 262.0], [562.0, 276.0], [435.0, 276.0]], "confidence": 0.9255318641662598, "text": "tion. In NeurIPS , 2023. 2 .", "bbox": [435.0, 262.0, 562.0, 276.0]}, {"polygon": [[64.0, 275.0], [384.0, 275.0], [384.0, 288.0], [64.0, 288.0]], "confidence": 0.981221079826355, "text": "[38] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang,", "bbox": [64.0, 275.0, 384.0, 288.0]}, {"polygon": [[408.0, 278.0], [729.0, 278.0], [729.0, 291.0], [408.0, 291.0]], "confidence": 0.9747201204299927, "text": "[54] Chung-Yi Weng, Brian Curless, Pratul P. Srinivasan,", "bbox": [408.0, 278.0, 729.0, 291.0]}, {"polygon": [[90.0, 289.0], [384.0, 289.0], [384.0, 303.0], [90.0, 303.0]], "confidence": 0.9808201789855957, "text": "Qing Shuai, Hujun Bao, and Xiaowei Zhou.  Neural body:", "bbox": [90.0, 289.0, 384.0, 303.0]}, {"polygon": [[435.0, 292.0], [729.0, 292.0], [729.0, 305.0], [435.0, 305.0]], "confidence": 0.9746972918510437, "text": "Jonathan T. Barron, and Ira Kemelmacher-Shlizerman. Hu-", "bbox": [435.0, 292.0, 729.0, 305.0]}, {"polygon": [[90.0, 304.0], [383.0, 304.0], [383.0, 318.0], [90.0, 318.0]], "confidence": 0.9833692908287048, "text": "Implicit neural representations with structured latent codes", "bbox": [90.0, 304.0, 383.0, 318.0]}, {"polygon": [[435.0, 307.0], [728.0, 307.0], [728.0, 321.0], [435.0, 321.0]], "confidence": 0.9764747023582458, "text": "mannerf: Free-viewpoint rendering of moving people from", "bbox": [435.0, 307.0, 728.0, 321.0]}, {"polygon": [[90.0, 319.0], [383.0, 319.0], [383.0, 332.0], [90.0, 332.0]], "confidence": 0.982103705406189, "text": "for novel view synthesis of dynamic humans. In CVPR , 2021.", "bbox": [90.0, 319.0, 383.0, 332.0]}, {"polygon": [[435.0, 322.0], [667.0, 322.0], [667.0, 336.0], [435.0, 336.0]], "confidence": 0.9693772792816162, "text": "monocular video. In CVPR , 2022. 2 , 4 , 6 , 7 , 12", "bbox": [435.0, 322.0, 667.0, 336.0]}, {"polygon": [[91.0, 334.0], [166.0, 334.0], [166.0, 347.0], [91.0, 347.0]], "confidence": 0.9368192553520203, "text": "2 , 3 , 6 , 7 , 8 , 12", "bbox": [91.0, 334.0, 166.0, 347.0]}, {"polygon": [[408.0, 337.0], [729.0, 337.0], [729.0, 351.0], [408.0, 351.0]], "confidence": 0.9824373722076416, "text": "[55] Tianhao Wu, Fangcheng Zhong, Andrea Tagliasacchi, For-", "bbox": [408.0, 337.0, 729.0, 351.0]}, {"polygon": [[63.0, 348.0], [384.0, 347.0], [384.0, 361.0], [63.0, 362.0]], "confidence": 0.9838730692863464, "text": "[39] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall.", "bbox": [63.0, 348.0, 384.0, 361.0]}, {"polygon": [[436.0, 352.0], [728.0, 352.0], [728.0, 365.0], [436.0, 365.0]], "confidence": 0.9761810898780823, "text": "rester Cole, and Cengiz Oztireli. D^ 2nerf: Self-supervised", "bbox": [436.0, 352.0, 728.0, 365.0]}, {"polygon": [[90.0, 363.0], [384.0, 363.0], [384.0, 376.0], [90.0, 376.0]], "confidence": 0.9742723107337952, "text": "Dreamfusion: Text-to-3d using 2d diffusion. In ICLR , 2023.", "bbox": [90.0, 363.0, 384.0, 376.0]}, {"polygon": [[435.0, 366.0], [728.0, 366.0], [728.0, 381.0], [435.0, 381.0]], "confidence": 0.9824835658073425, "text": "decoupling of dynamic and static objects from a monocular", "bbox": [435.0, 366.0, 728.0, 381.0]}, {"polygon": [[90.0, 378.0], [153.0, 378.0], [153.0, 391.0], [90.0, 391.0]], "confidence": 0.9145846962928772, "text": "1, 2, 3, 5, 12", "bbox": [90.0, 378.0, 153.0, 391.0]}, {"polygon": [[436.0, 381.0], [569.0, 381.0], [569.0, 395.0], [436.0, 395.0]], "confidence": 0.9591883420944214, "text": "video. In NeurIPS , 2022. 3", "bbox": [436.0, 381.0, 569.0, 395.0]}, {"polygon": [[64.0, 393.0], [342.0, 393.0], [342.0, 406.0], [64.0, 406.0]], "confidence": 0.9765247702598572, "text": "[40] Jathushan Rajasegaran, Georgios Pavlakos,", "bbox": [64.0, 393.0, 342.0, 406.0]}, {"polygon": [[340.0, 395.0], [384.0, 395.0], [384.0, 406.0], [340.0, 406.0]], "confidence": 0.8536167740821838, "text": "Angjoo", "bbox": [340.0, 395.0, 384.0, 406.0]}, {"polygon": [[408.0, 396.0], [728.0, 397.0], [728.0, 411.0], [408.0, 410.0]], "confidence": 0.9826220273971558, "text": "[56] Tiange Xiang, Adam Sun, Jiajun Wu, Ehsan Adeli, and Li", "bbox": [408.0, 396.0, 728.0, 411.0]}, {"polygon": [[91.0, 406.0], [382.0, 406.0], [382.0, 421.0], [91.0, 421.0]], "confidence": 0.9830450415611267, "text": "Kanazawa, and Jitendra Malik. Tracking people by predicting", "bbox": [91.0, 406.0, 382.0, 421.0]}, {"polygon": [[435.0, 411.0], [728.0, 411.0], [728.0, 425.0], [435.0, 425.0]], "confidence": 0.9792630672454834, "text": "Fei-Fei. Rendering humans from object-occluded monocular", "bbox": [435.0, 411.0, 728.0, 425.0]}, {"polygon": [[91.0, 422.0], [341.0, 422.0], [341.0, 435.0], [91.0, 435.0]], "confidence": 0.9698302745819092, "text": "3D appearance, location & pose. In CVPR , 2022. 4", "bbox": [91.0, 422.0, 341.0, 435.0]}, {"polygon": [[435.0, 426.0], [560.0, 426.0], [560.0, 440.0], [435.0, 440.0]], "confidence": 0.9385905265808105, "text": "videos. In ICCV , 2023. 2 .", "bbox": [435.0, 426.0, 560.0, 440.0]}, {"polygon": [[63.0, 437.0], [384.0, 437.0], [384.0, 450.0], [63.0, 450.0]], "confidence": 0.9818453192710876, "text": "[41] Daniel Rebain, Mark Matthews, Kwang Moo Yi, Dmitry La-", "bbox": [63.0, 437.0, 384.0, 450.0]}, {"polygon": [[409.0, 442.0], [730.0, 442.0], [730.0, 456.0], [409.0, 456.0]], "confidence": 0.9786163568496704, "text": "[57] Hongyi Xu, Eduard Gabriel Bazavan, Andrei Zanfir,", "bbox": [409.0, 442.0, 730.0, 456.0]}, {"polygon": [[91.0, 451.0], [383.0, 451.0], [383.0, 466.0], [91.0, 466.0]], "confidence": 0.9827845096588135, "text": "gun, and Andrea Tagliasacchi. Lolnerf: Learn from one look.", "bbox": [91.0, 451.0, 383.0, 466.0]}, {"polygon": [[436.0, 457.0], [729.0, 457.0], [729.0, 470.0], [436.0, 470.0]], "confidence": 0.9778250455856323, "text": "William T. Freeman, Rahul Sukthankar, and Cristian Smin-", "bbox": [436.0, 457.0, 729.0, 470.0]}, {"polygon": [[90.0, 465.0], [180.0, 465.0], [180.0, 480.0], [90.0, 480.0]], "confidence": 0.9346529841423035, "text": "In CVPR , 2022. 5", "bbox": [90.0, 465.0, 180.0, 480.0]}, {"polygon": [[436.0, 473.0], [729.0, 473.0], [729.0, 484.0], [436.0, 484.0]], "confidence": 0.980506181716919, "text": "chisescu. Ghum & ghuml: Generative 3d human shape and", "bbox": [436.0, 473.0, 729.0, 484.0]}, {"polygon": [[63.0, 481.0], [384.0, 481.0], [384.0, 494.0], [63.0, 494.0]], "confidence": 0.9814686179161072, "text": "[42] Robin Rombach, Andreas Blattmann, Dominik Lorenz,", "bbox": [63.0, 481.0, 384.0, 494.0]}, {"polygon": [[435.0, 486.0], [646.0, 486.0], [646.0, 499.0], [435.0, 499.0]], "confidence": 0.9637821316719055, "text": "articulated pose models. In CVPR , 2020. 2", "bbox": [435.0, 486.0, 646.0, 499.0]}, {"polygon": [[90.0, 496.0], [382.0, 496.0], [382.0, 509.0], [90.0, 509.0]], "confidence": 0.977877140045166, "text": "Patrick Esser, and Björn Ommer.  High-resolution image", "bbox": [90.0, 496.0, 382.0, 509.0]}, {"polygon": [[408.0, 501.0], [728.0, 501.0], [728.0, 515.0], [408.0, 515.0]], "confidence": 0.9830387830734253, "text": "[58] Weipeng Xu, Avishek Chatterjee, Michael Zollhöfer, Helge", "bbox": [408.0, 501.0, 728.0, 515.0]}, {"polygon": [[90.0, 510.0], [384.0, 510.0], [384.0, 523.0], [90.0, 523.0]], "confidence": 0.9739400744438171, "text": "synthesis with latent diffusion models. In CVPR , 2022. 1 , 2 ,", "bbox": [90.0, 510.0, 384.0, 523.0]}, {"polygon": [[435.0, 517.0], [728.0, 517.0], [728.0, 530.0], [435.0, 530.0]], "confidence": 0.9786149859428406, "text": "Rhodin, Dushyant Mehta, Hans-Peter Seidel, and Christian", "bbox": [435.0, 517.0, 728.0, 530.0]}, {"polygon": [[90.0, 525.0], [148.0, 525.0], [148.0, 539.0], [90.0, 539.0]], "confidence": 0.8784576654434204, "text": "5, 7, 12, 14", "bbox": [90.0, 525.0, 148.0, 539.0]}, {"polygon": [[437.0, 532.0], [728.0, 532.0], [728.0, 544.0], [437.0, 544.0]], "confidence": 0.980777382850647, "text": "Theobalt. Monoperfcap: Human performance capture from", "bbox": [437.0, 532.0, 728.0, 544.0]}, {"polygon": [[63.0, 538.0], [383.0, 539.0], [383.0, 554.0], [63.0, 553.0]], "confidence": 0.98157799243927, "text": "[43] Robin Rombach, Andreas Blattmann, Dominik Lorenz,", "bbox": [63.0, 538.0, 383.0, 554.0]}, {"polygon": [[435.0, 546.0], [641.0, 546.0], [641.0, 560.0], [435.0, 560.0]], "confidence": 0.9705154299736023, "text": "monocular video. In SIGGRAPH , 2018. 2", "bbox": [435.0, 546.0, 641.0, 560.0]}, {"polygon": [[90.0, 555.0], [382.0, 555.0], [382.0, 568.0], [90.0, 568.0]], "confidence": 0.9775379300117493, "text": "Patrick Esser, and Björn Ommer.  High-resolution image", "bbox": [90.0, 555.0, 382.0, 568.0]}, {"polygon": [[408.0, 561.0], [729.0, 561.0], [729.0, 575.0], [408.0, 575.0]], "confidence": 0.9821261763572693, "text": "[59] Yufei Xu, Jing Zhang, Qiming Zhang, and Dacheng Tao.", "bbox": [408.0, 561.0, 729.0, 575.0]}, {"polygon": [[91.0, 570.0], [370.0, 570.0], [370.0, 583.0], [91.0, 583.0]], "confidence": 0.974551796913147, "text": "synthesis with latent diffusion models. In CVPR , 2022. 3", "bbox": [91.0, 570.0, 370.0, 583.0]}, {"polygon": [[436.0, 577.0], [728.0, 577.0], [728.0, 590.0], [436.0, 590.0]], "confidence": 0.9825760126113892, "text": "ViTPose: Simple vision transformer baselines for human pose", "bbox": [436.0, 577.0, 728.0, 590.0]}, {"polygon": [[63.0, 584.0], [384.0, 584.0], [384.0, 597.0], [63.0, 597.0]], "confidence": 0.9824945330619812, "text": "[44] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,", "bbox": [63.0, 584.0, 384.0, 597.0]}, {"polygon": [[436.0, 592.0], [599.0, 592.0], [599.0, 604.0], [436.0, 604.0]], "confidence": 0.962093710899353, "text": "estimation. In NeurIPS , 2022. 13", "bbox": [436.0, 592.0, 599.0, 604.0]}, {"polygon": [[90.0, 598.0], [383.0, 598.0], [383.0, 611.0], [90.0, 611.0]], "confidence": 0.9734458923339844, "text": "Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine", "bbox": [90.0, 598.0, 383.0, 611.0]}, {"polygon": [[408.0, 606.0], [729.0, 606.0], [729.0, 621.0], [408.0, 621.0]], "confidence": 0.9827929735183716, "text": "[60] Gengshan Yang, Minh Vo, Natalia Neverova, Deva Ramanan,", "bbox": [408.0, 606.0, 729.0, 621.0]}, {"polygon": [[90.0, 613.0], [384.0, 613.0], [384.0, 626.0], [90.0, 626.0]], "confidence": 0.9818898439407349, "text": "tuning text-to-image diffusion models for subject-driven gen-", "bbox": [90.0, 613.0, 384.0, 626.0]}, {"polygon": [[436.0, 622.0], [729.0, 622.0], [729.0, 635.0], [436.0, 635.0]], "confidence": 0.9814088940620422, "text": "Andrea Vedaldi, and Hanbyul Joo. Banmo: Building animat-", "bbox": [436.0, 622.0, 729.0, 635.0]}, {"polygon": [[90.0, 626.0], [221.0, 628.0], [221.0, 643.0], [90.0, 641.0]], "confidence": 0.9455439448356628, "text": "eration. In CVPR , 2023. 5", "bbox": [90.0, 626.0, 221.0, 643.0]}, {"polygon": [[435.0, 637.0], [729.0, 637.0], [729.0, 650.0], [435.0, 650.0]], "confidence": 0.9732639193534851, "text": "able 3d neural models from many casual videos.  In CVPR ,", "bbox": [435.0, 637.0, 729.0, 650.0]}, {"polygon": [[63.0, 643.0], [384.0, 643.0], [384.0, 656.0], [63.0, 656.0]], "confidence": 0.9827526807785034, "text": "[45] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li,", "bbox": [63.0, 643.0, 384.0, 656.0]}, {"polygon": [[436.0, 652.0], [477.0, 652.0], [477.0, 664.0], [436.0, 664.0]], "confidence": 0.8419981002807617, "text": "2022. 2", "bbox": [436.0, 652.0, 477.0, 664.0]}, {"polygon": [[90.0, 658.0], [383.0, 658.0], [383.0, 671.0], [90.0, 671.0]], "confidence": 0.9804858565330505, "text": "Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael", "bbox": [90.0, 658.0, 383.0, 671.0]}, {"polygon": [[408.0, 666.0], [728.0, 666.0], [728.0, 681.0], [408.0, 681.0]], "confidence": 0.982805073261261, "text": "[61] Yifei Yin, Chen Guo, Manuel Kaufmann, Juan Zarate, Jie", "bbox": [408.0, 666.0, 728.0, 681.0]}, {"polygon": [[90.0, 672.0], [383.0, 672.0], [383.0, 685.0], [90.0, 685.0]], "confidence": 0.9774579405784607, "text": "Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Pho-", "bbox": [90.0, 672.0, 383.0, 685.0]}, {"polygon": [[436.0, 682.0], [728.0, 682.0], [728.0, 696.0], [436.0, 696.0]], "confidence": 0.9826748967170715, "text": "Song, and Otmar Hilliges. Hi4d: 4d instance segmentation of", "bbox": [436.0, 682.0, 728.0, 696.0]}, {"polygon": [[91.0, 688.0], [382.0, 688.0], [382.0, 700.0], [91.0, 700.0]], "confidence": 0.9821955561637878, "text": "torealistic text-to-image diffusion models with deep language", "bbox": [91.0, 688.0, 382.0, 700.0]}, {"polygon": [[435.0, 696.0], [695.0, 696.0], [695.0, 710.0], [435.0, 710.0]], "confidence": 0.9530171751976013, "text": "close human interaction. In CVPR , 2023. 6, 7, 12, 13", "bbox": [435.0, 696.0, 695.0, 710.0]}, {"polygon": [[90.0, 701.0], [267.0, 701.0], [267.0, 714.0], [90.0, 714.0]], "confidence": 0.9578480124473572, "text": "understanding. In NeurIPS , 2022. 2", "bbox": [90.0, 701.0, 267.0, 714.0]}, {"polygon": [[408.0, 711.0], [729.0, 711.0], [729.0, 726.0], [408.0, 726.0]], "confidence": 0.9822273254394531, "text": "[62] Zhengming Yu, Wei Cheng, xian Liu, Wayne Wu, and Kwan-", "bbox": [408.0, 711.0, 729.0, 726.0]}, {"polygon": [[63.0, 716.0], [384.0, 716.0], [384.0, 730.0], [63.0, 730.0]], "confidence": 0.9779869914054871, "text": "[46] Johannes Lutz Schönberger and Jan-Michael Frahm.", "bbox": [63.0, 716.0, 384.0, 730.0]}, {"polygon": [[435.0, 726.0], [729.0, 726.0], [729.0, 740.0], [435.0, 740.0]], "confidence": 0.9759049415588379, "text": "Yee Lin. MonoHuman: Animatable human neural field from", "bbox": [435.0, 726.0, 729.0, 740.0]}, {"polygon": [[90.0, 731.0], [358.0, 731.0], [358.0, 745.0], [90.0, 745.0]], "confidence": 0.9548194408416748, "text": "Structure-from-motion revisited. In CVPR , 2016. 4 , 12 ,", "bbox": [90.0, 731.0, 358.0, 745.0]}, {"polygon": [[437.0, 743.0], [612.0, 743.0], [612.0, 755.0], [437.0, 755.0]], "confidence": 0.9682523608207703, "text": "monocular video. In CVPR , 2023. 2", "bbox": [437.0, 743.0, 612.0, 755.0]}, {"polygon": [[63.0, 746.0], [382.0, 746.0], [382.0, 759.0], [63.0, 759.0]], "confidence": 0.9820722341537476, "text": "[47] Ruizhi Shao, Zerong Zheng, Hongwen Zhang, Jingxiang", "bbox": [63.0, 746.0, 382.0, 759.0]}, {"polygon": [[408.0, 757.0], [728.0, 757.0], [728.0, 771.0], [408.0, 771.0]], "confidence": 0.983677089214325, "text": "[63] Jiakai Zhang, Xinhang Liu, Xinyi Ye, Fuqiang Zhao, Yanshun", "bbox": [408.0, 757.0, 728.0, 771.0]}, {"polygon": [[91.0, 761.0], [384.0, 761.0], [384.0, 774.0], [91.0, 774.0]], "confidence": 0.9793764352798462, "text": "Sun, and Yebin Liu.  Diffustereo: High quality human re-", "bbox": [91.0, 761.0, 384.0, 774.0]}, {"polygon": [[435.0, 771.0], [729.0, 771.0], [729.0, 786.0], [435.0, 786.0]], "confidence": 0.9806247353553772, "text": "Zhang, Minye Wu, Yingliang Zhang, Lan Xu, and Jingyi", "bbox": [435.0, 771.0, 729.0, 786.0]}, {"polygon": [[90.0, 775.0], [384.0, 775.0], [384.0, 789.0], [90.0, 789.0]], "confidence": 0.9833856225013733, "text": "construction via diffusion-based stereo using sparse cameras.", "bbox": [90.0, 775.0, 384.0, 789.0]}, {"polygon": [[436.0, 787.0], [728.0, 787.0], [728.0, 800.0], [436.0, 800.0]], "confidence": 0.9734569191932678, "text": "Yu. Editable free-viewpoint video using a layered neural", "bbox": [436.0, 787.0, 728.0, 800.0]}, {"polygon": [[90.0, 789.0], [180.0, 789.0], [180.0, 804.0], [90.0, 804.0]], "confidence": 0.9403586983680725, "text": "In ECCV , 2022. 2", "bbox": [90.0, 789.0, 180.0, 804.0]}, {"polygon": [[436.0, 802.0], [652.0, 802.0], [652.0, 815.0], [436.0, 815.0]], "confidence": 0.9718638062477112, "text": "representation. In SIGGRAPH , 2021. 2 , 3 , 6", "bbox": [436.0, 802.0, 652.0, 815.0]}, {"polygon": [[63.0, 805.0], [383.0, 805.0], [383.0, 819.0], [63.0, 819.0]], "confidence": 0.9829617738723755, "text": "[48] Qing Shuai, Chen Geng, Qi Fang, Sida Peng, Wenhao Shen,", "bbox": [63.0, 805.0, 383.0, 819.0]}, {"polygon": [[409.0, 818.0], [728.0, 816.0], [728.0, 829.0], [409.0, 831.0]], "confidence": 0.975946843624115, "text": "[64] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding", "bbox": [409.0, 818.0, 728.0, 829.0]}, {"polygon": [[90.0, 820.0], [383.0, 820.0], [383.0, 833.0], [90.0, 833.0]], "confidence": 0.9802905917167664, "text": "Xiaowei Zhou, and Hujun Bao.  Novel view synthesis of", "bbox": [90.0, 820.0, 383.0, 833.0]}, {"polygon": [[435.0, 831.0], [729.0, 831.0], [729.0, 845.0], [435.0, 845.0]], "confidence": 0.9801636934280396, "text": "conditional control to text-to-image diffusion models.  In", "bbox": [435.0, 831.0, 729.0, 845.0]}, {"polygon": [[90.0, 834.0], [384.0, 834.0], [384.0, 847.0], [90.0, 847.0]], "confidence": 0.9784834384918213, "text": "human interactions from sparse multi-view videos. In SIG-", "bbox": [90.0, 834.0, 384.0, 847.0]}, {"polygon": [[435.0, 847.0], [563.0, 847.0], [563.0, 860.0], [435.0, 860.0]], "confidence": 0.9452052712440491, "text": "ICCV , 2023. 5, 12 , 13 , 14.", "bbox": [435.0, 847.0, 563.0, 860.0]}, {"polygon": [[91.0, 850.0], [218.0, 850.0], [218.0, 862.0], [91.0, 862.0]], "confidence": 0.9480055570602417, "text": "GRAPH , 2022. 3, 6, 7, 12", "bbox": [91.0, 850.0, 218.0, 862.0]}, {"polygon": [[63.0, 863.0], [383.0, 863.0], [383.0, 878.0], [63.0, 878.0]], "confidence": 0.9841792583465576, "text": "[49] Yu Sun, Qian Bao, Wu Liu, Yili Fu, Michael J Black, and Tao", "bbox": [63.0, 863.0, 383.0, 878.0]}, {"polygon": [[409.0, 863.0], [729.0, 863.0], [729.0, 876.0], [409.0, 876.0]], "confidence": 0.9841965436935425, "text": "[65] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,", "bbox": [409.0, 863.0, 729.0, 876.0]}, {"polygon": [[436.0, 877.0], [728.0, 877.0], [728.0, 891.0], [436.0, 891.0]], "confidence": 0.979489266872406, "text": "and Oliver Wang.  The unreasonable effectiveness of deep", "bbox": [436.0, 877.0, 728.0, 891.0]}, {"polygon": [[90.0, 879.0], [383.0, 877.0], [383.0, 891.0], [90.0, 893.0]], "confidence": 0.9811323881149292, "text": "Mei. Monocular, one-stage, regression of multiple 3d people.", "bbox": [90.0, 879.0, 383.0, 891.0]}, {"polygon": [[435.0, 892.0], [678.0, 892.0], [678.0, 906.0], [435.0, 906.0]], "confidence": 0.9713397026062012, "text": "features as a perceptual metric. In CVPR , 2018. 7", "bbox": [435.0, 892.0, 678.0, 906.0]}, {"polygon": [[91.0, 894.0], [178.0, 894.0], [178.0, 906.0], [91.0, 906.0]], "confidence": 0.939245879650116, "text": "In ICCV , 2021. 4", "bbox": [91.0, 894.0, 178.0, 906.0]}, {"polygon": [[63.0, 907.0], [382.0, 907.0], [382.0, 921.0], [63.0, 921.0]], "confidence": 0.9714182615280151, "text": "[50] Zachary Teed and Jia Deng.  DROID-SLAM: Deep Visual", "bbox": [63.0, 907.0, 382.0, 921.0]}, {"polygon": [[409.0, 908.0], [727.0, 908.0], [727.0, 921.0], [409.0, 921.0]], "confidence": 0.9834395051002502, "text": "[66] Zhizhuo Zhou and Shubham Tulsiani. Sparsefusion: Distilling", "bbox": [409.0, 908.0, 727.0, 921.0]}, {"polygon": [[435.0, 922.0], [729.0, 922.0], [729.0, 935.0], [435.0, 935.0]], "confidence": 0.9765558242797852, "text": "view-conditioned diffusion for 3d reconstruction. In CVPR ,", "bbox": [435.0, 922.0, 729.0, 935.0]}, {"polygon": [[91.0, 924.0], [383.0, 922.0], [383.0, 935.0], [91.0, 937.0]], "confidence": 0.9715701937675476, "text": "SLAM for Monocular, Stereo, and RGB-D Cameras. In", "bbox": [91.0, 924.0, 383.0, 935.0]}, {"polygon": [[90.0, 937.0], [202.0, 937.0], [202.0, 951.0], [90.0, 951.0]], "confidence": 0.9470476508140564, "text": "NeurIPS , 2021. 12 , 13", "bbox": [90.0, 937.0, 202.0, 951.0]}, {"polygon": [[436.0, 937.0], [478.0, 937.0], [478.0, 950.0], [436.0, 950.0]], "confidence": 0.8540416359901428, "text": "2023. 2", "bbox": [436.0, 937.0, 478.0, 950.0]}, {"polygon": [[388.0, 975.0], [405.0, 975.0], [405.0, 990.0], [388.0, 990.0]], "confidence": 0.5796814560890198, "text": "10", "bbox": [388.0, 975.0, 405.0, 990.0]}], "languages": ["en"], "image_bbox": [0.0, 0.0, 816.0, 1056.0], "page": 10}, {"text_lines": [{"polygon": [[62.0, 95.0], [386.0, 93.0], [386.0, 113.0], [62.0, 115.0]], "confidence": 0.9787933230400085, "text": "[67] Joseph Zhu and Peiye Zhuang. Hifa: High-fidelity text-to-3d", "bbox": [62.0, 95.0, 386.0, 113.0]}, {"polygon": [[89.0, 111.0], [354.0, 109.0], [354.0, 129.0], [89.0, 131.0]], "confidence": 0.9790090918540955, "text": "with advanced diffusion guidance. In ICLR , 2024. 12", "bbox": [89.0, 111.0, 354.0, 129.0]}, {"polygon": [[62.0, 126.0], [385.0, 126.0], [385.0, 145.0], [62.0, 145.0]], "confidence": 0.9777588248252869, "text": "[68] M. Zwicker, H. Pfister, J. van Baar, and M. Gross.  Ewa", "bbox": [62.0, 126.0, 385.0, 145.0]}, {"polygon": [[89.0, 140.0], [386.0, 140.0], [386.0, 159.0], [89.0, 159.0]], "confidence": 0.9759381413459778, "text": "volume splatting. In Proceedings Visualization, 2001. VIS", "bbox": [89.0, 140.0, 386.0, 159.0]}, {"polygon": [[88.0, 154.0], [157.0, 154.0], [157.0, 174.0], [88.0, 174.0]], "confidence": 0.8080056309700012, "text": "201. , 2001.", "bbox": [88.0, 154.0, 157.0, 174.0]}, {"polygon": [[386.0, 974.0], [406.0, 974.0], [406.0, 993.0], [386.0, 993.0]], "confidence": 0.4960804879665375, "text": "111", "bbox": [386.0, 974.0, 406.0, 993.0]}], "languages": ["en"], "image_bbox": [0.0, 0.0, 816.0, 1056.0], "page": 11}, {"text_lines": [{"polygon": [[63.0, 95.0], [248.0, 95.0], [248.0, 112.0], [63.0, 112.0]], "confidence": 0.9603639841079712, "text": "A. Implementation Details", "bbox": [63.0, 95.0, 248.0, 112.0]}, {"polygon": [[408.0, 96.0], [729.0, 96.0], [729.0, 111.0], [408.0, 111.0]], "confidence": 0.9824219942092896, "text": "body being transparent. We densify the human Gaussians in", "bbox": [408.0, 96.0, 729.0, 111.0]}, {"polygon": [[409.0, 113.0], [729.0, 113.0], [729.0, 127.0], [409.0, 127.0]], "confidence": 0.9834965467453003, "text": "[2000, 2500, 3000] iterations for detailed reconstruction and", "bbox": [409.0, 113.0, 729.0, 127.0]}, {"polygon": [[63.0, 122.0], [299.0, 122.0], [299.0, 138.0], [63.0, 138.0]], "confidence": 0.9717969298362732, "text": "A.1. Baseline Implementation Details", "bbox": [63.0, 122.0, 299.0, 138.0]}, {"polygon": [[408.0, 129.0], [728.0, 129.0], [728.0, 143.0], [408.0, 143.0]], "confidence": 0.9833190441131592, "text": "prune Gaussians which are exceptionally large or transparent", "bbox": [408.0, 129.0, 728.0, 143.0]}, {"polygon": [[408.0, 145.0], [728.0, 145.0], [728.0, 160.0], [408.0, 160.0]], "confidence": 0.9832078814506531, "text": "every 500 iterations until the end of optimizations to reduce", "bbox": [408.0, 145.0, 728.0, 160.0]}, {"polygon": [[63.0, 146.0], [384.0, 147.0], [384.0, 162.0], [63.0, 161.0]], "confidence": 0.9777419567108154, "text": "HumanNeRF [54] does not support the simultaneous op-", "bbox": [63.0, 146.0, 384.0, 162.0]}, {"polygon": [[409.0, 161.0], [729.0, 161.0], [729.0, 175.0], [409.0, 175.0]], "confidence": 0.9794662594795227, "text": "artifacts. The background Gaussians are densified only dur-", "bbox": [409.0, 161.0, 729.0, 175.0]}, {"polygon": [[63.0, 162.0], [384.0, 162.0], [384.0, 177.0], [63.0, 177.0]], "confidence": 0.9825776815414429, "text": "timization of multiple people, so we optimize each person", "bbox": [63.0, 162.0, 384.0, 177.0]}, {"polygon": [[409.0, 177.0], [728.0, 177.0], [728.0, 191.0], [409.0, 191.0]], "confidence": 0.9810896515846252, "text": "ing pre-optimization stage and keep the same number of", "bbox": [409.0, 177.0, 728.0, 191.0]}, {"polygon": [[63.0, 179.0], [382.0, 179.0], [382.0, 192.0], [63.0, 192.0]], "confidence": 0.983238935470581, "text": "separately and merge them in the evaluation stage. Following", "bbox": [63.0, 179.0, 382.0, 192.0]}, {"polygon": [[408.0, 193.0], [631.0, 193.0], [631.0, 208.0], [408.0, 208.0]], "confidence": 0.9764363765716553, "text": "Gaussians in the joint optimization stage.", "bbox": [408.0, 193.0, 631.0, 208.0]}, {"polygon": [[63.0, 194.0], [383.0, 194.0], [383.0, 209.0], [63.0, 209.0]], "confidence": 0.9799029231071472, "text": "the default HumanNeRF experiment settings, each person", "bbox": [63.0, 194.0, 383.0, 209.0]}, {"polygon": [[63.0, 211.0], [383.0, 211.0], [383.0, 224.0], [63.0, 224.0]], "confidence": 0.9815378785133362, "text": "is optimized for 400k iterations using 4 NVIDIA RTX4090", "bbox": [63.0, 211.0, 383.0, 224.0]}, {"polygon": [[409.0, 211.0], [729.0, 211.0], [729.0, 226.0], [409.0, 226.0]], "confidence": 0.9811864495277405, "text": "Optimization Details. We use Adam [ 21 ] optimizer with", "bbox": [409.0, 211.0, 729.0, 226.0]}, {"polygon": [[63.0, 226.0], [383.0, 226.0], [383.0, 241.0], [63.0, 241.0]], "confidence": 0.9801969528198242, "text": "GPUs which takes approximately 40 hours per person. For", "bbox": [63.0, 226.0, 383.0, 241.0]}, {"polygon": [[409.0, 227.0], [729.0, 227.0], [729.0, 241.0], [409.0, 241.0]], "confidence": 0.9827573895454407, "text": "different learning rates for each component of 3D Gaussians.", "bbox": [409.0, 227.0, 729.0, 241.0]}, {"polygon": [[63.0, 242.0], [383.0, 242.0], [383.0, 256.0], [63.0, 256.0]], "confidence": 0.9819220304489136, "text": "the ZJU-Mocap [ 38 ] dataset, we utilize the publicly available", "bbox": [63.0, 242.0, 383.0, 256.0]}, {"polygon": [[408.0, 242.0], [728.0, 242.0], [728.0, 257.0], [408.0, 257.0]], "confidence": 0.9736722111701965, "text": "For the center of Gaussian µ , we set an initial learning rate", "bbox": [408.0, 242.0, 728.0, 257.0]}, {"polygon": [[63.0, 258.0], [251.0, 258.0], [251.0, 272.0], [63.0, 272.0]], "confidence": 0.9710240960121155, "text": "checkpoints shared by the authors.", "bbox": [63.0, 258.0, 251.0, 272.0]}, {"polygon": [[409.0, 257.0], [729.0, 257.0], [729.0, 275.0], [409.0, 275.0]], "confidence": 0.9731994271278381, "text": "as 1 e − 3 and decay it until 2 e − 6 during training. We use a", "bbox": [409.0, 257.0, 729.0, 275.0]}, {"polygon": [[63.0, 274.0], [384.0, 274.0], [384.0, 288.0], [63.0, 288.0]], "confidence": 0.979844868183136, "text": "Shuai et al. [48] represents the scene as a composition of a", "bbox": [63.0, 274.0, 384.0, 288.0]}, {"polygon": [[409.0, 274.0], [730.0, 274.0], [730.0, 290.0], [409.0, 290.0]], "confidence": 0.9492335915565491, "text": "fixed learning rate 2.5e − 3 for color c, 5e − 2 for opacity o ,", "bbox": [409.0, 274.0, 730.0, 290.0]}, {"polygon": [[409.0, 289.0], [728.0, 289.0], [728.0, 305.0], [409.0, 305.0]], "confidence": 0.9602822661399841, "text": "5e − 3 for scale s, and 1 e − 3 for quaternion q . We set the loss", "bbox": [409.0, 289.0, 728.0, 305.0]}, {"polygon": [[63.0, 290.0], [384.0, 290.0], [384.0, 305.0], [63.0, 305.0]], "confidence": 0.9813823699951172, "text": "background model and human model, both represented by a", "bbox": [63.0, 290.0, 384.0, 305.0]}, {"polygon": [[63.0, 306.0], [384.0, 306.0], [384.0, 321.0], [63.0, 321.0]], "confidence": 0.9790936708450317, "text": "variant of NeRF [ 33 , 38 ]. For the Panoptic dataset [ 19 ] and", "bbox": [63.0, 306.0, 384.0, 321.0]}, {"polygon": [[409.0, 306.0], [729.0, 306.0], [729.0, 322.0], [409.0, 322.0]], "confidence": 0.9737244844436646, "text": "weight of SSIM loss λ ssim = 0.2, MSE loss λ rgb = 0.8,", "bbox": [409.0, 306.0, 729.0, 322.0]}, {"polygon": [[63.0, 322.0], [384.0, 322.0], [384.0, 337.0], [63.0, 337.0]], "confidence": 0.9793727397918701, "text": "Hi4D dataset [ 61 ], we model the background using a time-", "bbox": [63.0, 322.0, 384.0, 337.0]}, {"polygon": [[408.0, 322.0], [729.0, 322.0], [729.0, 337.0], [408.0, 337.0]], "confidence": 0.9731889367103577, "text": "LPIPS loss λ lpips = 0.1, and SDS loss λ sds = 1.0. For hard", "bbox": [408.0, 322.0, 729.0, 337.0]}, {"polygon": [[63.0, 338.0], [383.0, 338.0], [383.0, 352.0], [63.0, 352.0]], "confidence": 0.9773721694946289, "text": "conditioned NeRF defined on the surface of the cylinder", "bbox": [63.0, 338.0, 383.0, 352.0]}, {"polygon": [[409.0, 337.0], [728.0, 338.0], [728.0, 353.0], [409.0, 352.0]], "confidence": 0.9754045009613037, "text": "surface regularization loss, we set the weight of loss λ hard", "bbox": [409.0, 337.0, 728.0, 353.0]}, {"polygon": [[63.0, 354.0], [384.0, 354.0], [384.0, 368.0], [63.0, 368.0]], "confidence": 0.9813991189002991, "text": "fully covering the scene and the human model with Neu-", "bbox": [63.0, 354.0, 384.0, 368.0]}, {"polygon": [[408.0, 354.0], [727.0, 354.0], [727.0, 368.0], [408.0, 368.0]], "confidence": 0.9774279594421387, "text": "relative to reconstruction loss weight λ recon = 0.1 × λ recon", "bbox": [408.0, 354.0, 727.0, 368.0]}, {"polygon": [[63.0, 370.0], [383.0, 370.0], [383.0, 385.0], [63.0, 385.0]], "confidence": 0.9779908657073975, "text": "ralBody [ 38 ]. We jointly optimize these models for 400k", "bbox": [63.0, 370.0, 383.0, 385.0]}, {"polygon": [[408.0, 370.0], [728.0, 370.0], [728.0, 385.0], [408.0, 385.0]], "confidence": 0.9816861152648926, "text": "to keep a balance of losses. We use a fixed reconstruction loss", "bbox": [408.0, 370.0, 728.0, 385.0]}, {"polygon": [[63.0, 386.0], [382.0, 386.0], [382.0, 400.0], [63.0, 400.0]], "confidence": 0.9797656536102295, "text": "iterations using 2 NVIDIA RTX4090 GPUs which takes", "bbox": [63.0, 386.0, 382.0, 400.0]}, {"polygon": [[409.0, 386.0], [728.0, 386.0], [728.0, 400.0], [409.0, 400.0]], "confidence": 0.9775953888893127, "text": "weight λ recon = 1.0 before 1 k iterations and then schedule", "bbox": [409.0, 386.0, 728.0, 400.0]}, {"polygon": [[63.0, 402.0], [383.0, 402.0], [383.0, 417.0], [63.0, 417.0]], "confidence": 0.9813096523284912, "text": "approximately 70 hours per scene. The remaining settings", "bbox": [63.0, 402.0, 383.0, 417.0]}, {"polygon": [[409.0, 402.0], [729.0, 402.0], [729.0, 417.0], [409.0, 417.0]], "confidence": 0.981753408908844, "text": "the weight after 1 k iterations to balance the reconstruction", "bbox": [409.0, 402.0, 729.0, 417.0]}, {"polygon": [[63.0, 418.0], [383.0, 418.0], [383.0, 432.0], [63.0, 432.0]], "confidence": 0.9805567860603333, "text": "are the same as the original paper [ 48 ]. When we render the", "bbox": [63.0, 418.0, 383.0, 432.0]}, {"polygon": [[409.0, 418.0], [509.0, 418.0], [509.0, 432.0], [409.0, 432.0]], "confidence": 0.9465907216072083, "text": "loss and SDS loss.", "bbox": [409.0, 418.0, 509.0, 432.0]}, {"polygon": [[63.0, 433.0], [382.0, 433.0], [382.0, 448.0], [63.0, 448.0]], "confidence": 0.9819629192352295, "text": "scene for evaluation, we discard the background and only", "bbox": [63.0, 433.0, 382.0, 448.0]}, {"polygon": [[409.0, 435.0], [728.0, 436.0], [728.0, 451.0], [409.0, 450.0]], "confidence": 0.9800671339035034, "text": "SDS loss details. We use a publicly available SD1.5 [ 42 ]", "bbox": [409.0, 435.0, 728.0, 451.0]}, {"polygon": [[63.0, 450.0], [199.0, 450.0], [199.0, 464.0], [63.0, 464.0]], "confidence": 0.95792156457901, "text": "render the human model.", "bbox": [63.0, 450.0, 199.0, 464.0]}, {"polygon": [[409.0, 452.0], [729.0, 452.0], [729.0, 467.0], [409.0, 467.0]], "confidence": 0.9751855134963989, "text": "and OpenPose ControlNet [ 64 ] checkpoint for the SDS loss.", "bbox": [409.0, 452.0, 729.0, 467.0]}, {"polygon": [[63.0, 466.0], [384.0, 466.0], [384.0, 481.0], [63.0, 481.0]], "confidence": 0.9741746187210083, "text": "InstantAvatar [ 17 ] reconstructs a single person from monoc-", "bbox": [63.0, 466.0, 384.0, 481.0]}, {"polygon": [[409.0, 468.0], [729.0, 468.0], [729.0, 483.0], [409.0, 483.0]], "confidence": 0.9810722470283508, "text": "Similar to other methods using SDS [ 39 ], we use a high CFG", "bbox": [409.0, 468.0, 729.0, 483.0]}, {"polygon": [[63.0, 482.0], [384.0, 482.0], [384.0, 496.0], [63.0, 496.0]], "confidence": 0.9830895662307739, "text": "ular video input. Hence, we optimize it on each person sepa-", "bbox": [63.0, 482.0, 384.0, 496.0]}, {"polygon": [[409.0, 484.0], [728.0, 484.0], [728.0, 498.0], [409.0, 498.0]], "confidence": 0.9831910133361816, "text": "scale of 50 to generate detailed texture on unseen parts. We", "bbox": [409.0, 484.0, 728.0, 498.0]}, {"polygon": [[63.0, 498.0], [384.0, 498.0], [384.0, 513.0], [63.0, 513.0]], "confidence": 0.9813389778137207, "text": "rately and merge them in the evaluation stage same as Hu-", "bbox": [63.0, 498.0, 384.0, 513.0]}, {"polygon": [[409.0, 499.0], [727.0, 499.0], [727.0, 514.0], [409.0, 514.0]], "confidence": 0.9710625410079956, "text": "sample the noise time step τ of SDS loss from U [0.5, 0.98]", "bbox": [409.0, 499.0, 727.0, 514.0]}, {"polygon": [[63.0, 513.0], [382.0, 513.0], [382.0, 528.0], [63.0, 528.0]], "confidence": 0.971432626247406, "text": "manNeRF [ 54 ]. We train the InstantAvatar for 50 epochs", "bbox": [63.0, 513.0, 382.0, 528.0]}, {"polygon": [[409.0, 516.0], [729.0, 516.0], [729.0, 530.0], [409.0, 530.0]], "confidence": 0.9768956303596497, "text": "for the first 2 k iterations and then smoothly anneal it into", "bbox": [409.0, 516.0, 729.0, 530.0]}, {"polygon": [[63.0, 528.0], [383.0, 528.0], [383.0, 543.0], [63.0, 543.0]], "confidence": 0.9826390147209167, "text": "using a single RTX3090, following the default options used", "bbox": [63.0, 528.0, 383.0, 543.0]}, {"polygon": [[408.0, 531.0], [728.0, 531.0], [728.0, 546.0], [408.0, 546.0]], "confidence": 0.9705806970596313, "text": "U [0.02, 0.3] over following 2 k iterations similar to the prior", "bbox": [408.0, 531.0, 728.0, 546.0]}, {"polygon": [[63.0, 545.0], [350.0, 545.0], [350.0, 559.0], [63.0, 559.0]], "confidence": 0.9801860451698303, "text": "to optimize PeopleSnapShot [ 2 ] in the original paper.", "bbox": [63.0, 545.0, 350.0, 559.0]}, {"polygon": [[409.0, 547.0], [728.0, 547.0], [728.0, 562.0], [409.0, 562.0]], "confidence": 0.9798113703727722, "text": "work [ 67 ]. We also schedule the weight of reconstruction loss", "bbox": [409.0, 547.0, 728.0, 562.0]}, {"polygon": [[410.0, 563.0], [728.0, 563.0], [728.0, 579.0], [410.0, 579.0]], "confidence": 0.9544152617454529, "text": "λ recon with a maximum time step τ max on each iteration to", "bbox": [410.0, 563.0, 728.0, 579.0]}, {"polygon": [[63.0, 571.0], [279.0, 571.0], [279.0, 586.0], [63.0, 586.0]], "confidence": 0.9630733132362366, "text": "A.2. Ours Implementation Details", "bbox": [63.0, 571.0, 279.0, 586.0]}, {"polygon": [[408.0, 579.0], [711.0, 579.0], [711.0, 594.0], [408.0, 594.0]], "confidence": 0.9820055365562439, "text": "balance the reconstruction loss and SDS loss as follows:", "bbox": [408.0, 579.0, 711.0, 594.0]}, {"polygon": [[63.0, 595.0], [384.0, 595.0], [384.0, 610.0], [63.0, 610.0]], "confidence": 0.9747229814529419, "text": "Background pre-optimization. We first optimize back-", "bbox": [63.0, 595.0, 384.0, 610.0]}, {"polygon": [[63.0, 610.0], [382.0, 610.0], [382.0, 626.0], [63.0, 626.0]], "confidence": 0.9684063196182251, "text": "ground Gaussians G BG with images that humans are masked", "bbox": [63.0, 610.0, 382.0, 626.0]}, {"polygon": [[507.0, 612.0], [625.0, 612.0], [625.0, 629.0], [507.0, 629.0]], "confidence": 0.8703015446662903, "text": "Arecon = 10 6 × T 2", "bbox": [507.0, 612.0, 625.0, 629.0]}, {"polygon": [[702.0, 612.0], [730.0, 612.0], [730.0, 627.0], [702.0, 627.0]], "confidence": 0.7886564135551453, "text": "(15)", "bbox": [702.0, 612.0, 730.0, 627.0]}, {"polygon": [[63.0, 627.0], [383.0, 627.0], [383.0, 643.0], [63.0, 643.0]], "confidence": 0.9783396124839783, "text": "out. The background Gaussians GBG are initialized with", "bbox": [63.0, 627.0, 383.0, 643.0]}, {"polygon": [[63.0, 644.0], [382.0, 644.0], [382.0, 659.0], [63.0, 659.0]], "confidence": 0.9793892502784729, "text": "point cloud obtained by SfM [ 46 ] or SLAM [ 50 ]. In the", "bbox": [63.0, 644.0, 382.0, 659.0]}, {"polygon": [[409.0, 645.0], [729.0, 645.0], [729.0, 660.0], [409.0, 660.0]], "confidence": 0.981437623500824, "text": "We apply SDS loss from 1 k iteration of the joint optimization.", "bbox": [409.0, 645.0, 729.0, 660.0]}, {"polygon": [[63.0, 661.0], [383.0, 658.0], [383.0, 674.0], [63.0, 677.0]], "confidence": 0.9783205389976501, "text": "case of a fixed camera, we initialize Gaussians GBG with a", "bbox": [63.0, 661.0, 383.0, 674.0]}, {"polygon": [[409.0, 661.0], [728.0, 661.0], [728.0, 675.0], [409.0, 675.0]], "confidence": 0.9831483960151672, "text": "For every single iteration of reconstruction loss, we apply", "bbox": [409.0, 661.0, 728.0, 675.0]}, {"polygon": [[63.0, 676.0], [383.0, 676.0], [383.0, 690.0], [63.0, 690.0]], "confidence": 0.9816948771476746, "text": "3D sphere whose radius is 30m, together with background", "bbox": [63.0, 676.0, 383.0, 690.0]}, {"polygon": [[409.0, 677.0], [685.0, 677.0], [685.0, 692.0], [409.0, 692.0]], "confidence": 0.9795506000518799, "text": "SDS loss on all humans who appeared in the scene.", "bbox": [409.0, 677.0, 685.0, 692.0]}, {"polygon": [[63.0, 692.0], [382.0, 692.0], [382.0, 706.0], [63.0, 706.0]], "confidence": 0.9830693006515503, "text": "regularization loss to prevent it from occluding the people", "bbox": [63.0, 692.0, 382.0, 706.0]}, {"polygon": [[425.0, 695.0], [728.0, 695.0], [728.0, 710.0], [425.0, 710.0]], "confidence": 0.9790371656417847, "text": "We sample random unseen cameras for SDS loss from", "bbox": [425.0, 695.0, 728.0, 710.0]}, {"polygon": [[63.0, 708.0], [126.0, 708.0], [126.0, 723.0], [63.0, 723.0]], "confidence": 0.9144583344459534, "text": "as follows:", "bbox": [63.0, 708.0, 126.0, 723.0]}, {"polygon": [[409.0, 711.0], [728.0, 711.0], [728.0, 726.0], [409.0, 726.0]], "confidence": 0.9831423163414001, "text": "the surface of a sphere with a radius of 2.2, centered on the", "bbox": [409.0, 711.0, 728.0, 726.0]}, {"polygon": [[408.0, 727.0], [728.0, 727.0], [728.0, 741.0], [408.0, 741.0]], "confidence": 0.9659890532493591, "text": "human pelvis. The azimuth ϕ and elevation θ of cameras", "bbox": [408.0, 727.0, 728.0, 741.0]}, {"polygon": [[409.0, 743.0], [729.0, 743.0], [729.0, 758.0], [409.0, 758.0]], "confidence": 0.9398185610771179, "text": "are drawn from ϕ ∼ U [ − π, π ] and θ ∼ U [ − 0.3 π, 0.3 π ].", "bbox": [409.0, 743.0, 729.0, 758.0]}, {"polygon": [[234.0, 747.0], [310.0, 747.0], [310.0, 760.0], [234.0, 760.0]], "confidence": 0.683285117149353, "text": "| µ BG  – 30|| 2", "bbox": [234.0, 747.0, 310.0, 760.0]}, {"polygon": [[357.0, 747.0], [385.0, 747.0], [385.0, 762.0], [357.0, 762.0]], "confidence": 0.7889192700386047, "text": "(14)", "bbox": [357.0, 747.0, 385.0, 762.0]}, {"polygon": [[409.0, 759.0], [730.0, 759.0], [730.0, 773.0], [409.0, 773.0]], "confidence": 0.9752333164215088, "text": "Additionally, we choose a view-augmented prompt [ side ,", "bbox": [409.0, 759.0, 730.0, 773.0]}, {"polygon": [[408.0, 775.0], [729.0, 775.0], [729.0, 790.0], [408.0, 790.0]], "confidence": 0.9757464528083801, "text": "front, back ] based on the sampled azimuth ϕ and SMPL", "bbox": [408.0, 775.0, 729.0, 790.0]}, {"polygon": [[63.0, 788.0], [384.0, 789.0], [384.0, 805.0], [63.0, 804.0]], "confidence": 0.9299757480621338, "text": ", where pBG is the center of i-th background Gaussian.", "bbox": [63.0, 788.0, 384.0, 805.0]}, {"polygon": [[408.0, 792.0], [728.0, 792.0], [728.0, 805.0], [408.0, 805.0]], "confidence": 0.9781437516212463, "text": "global rotation. For the initial 3 k iterations of optimization", "bbox": [408.0, 792.0, 728.0, 805.0]}, {"polygon": [[63.0, 806.0], [383.0, 806.0], [383.0, 821.0], [63.0, 821.0]], "confidence": 0.9816527962684631, "text": "We scale the world's unit distance to be 1m before starting", "bbox": [63.0, 806.0, 383.0, 821.0]}, {"polygon": [[409.0, 806.0], [729.0, 806.0], [729.0, 821.0], [409.0, 821.0]], "confidence": 0.9815533757209778, "text": "with SDS loss, we mainly render the full body of posed", "bbox": [409.0, 806.0, 729.0, 821.0]}, {"polygon": [[63.0, 822.0], [383.0, 822.0], [383.0, 836.0], [63.0, 836.0]], "confidence": 0.9831897616386414, "text": "optimization. The background is optimized for 30k iterations", "bbox": [63.0, 822.0, 383.0, 836.0]}, {"polygon": [[409.0, 822.0], [728.0, 822.0], [728.0, 839.0], [409.0, 839.0]], "confidence": 0.9490852952003479, "text": "human Gaussians G h ( θ j,t ) and canonical human Gaussians", "bbox": [409.0, 822.0, 728.0, 839.0]}, {"polygon": [[63.0, 838.0], [355.0, 838.0], [355.0, 853.0], [63.0, 853.0]], "confidence": 0.9799351692199707, "text": "following the default 3D-GS [ 20 ] experiment settings.", "bbox": [63.0, 838.0, 355.0, 853.0]}, {"polygon": [[409.0, 840.0], [729.0, 840.0], [729.0, 854.0], [409.0, 854.0]], "confidence": 0.9514124989509583, "text": "G h (θ c ) for SDS loss. In the subsequent iterations, we also", "bbox": [409.0, 840.0, 729.0, 854.0]}, {"polygon": [[63.0, 854.0], [384.0, 854.0], [384.0, 868.0], [63.0, 868.0]], "confidence": 0.9801158905029297, "text": "Human background joint optimization. After the pre-", "bbox": [63.0, 854.0, 384.0, 868.0]}, {"polygon": [[408.0, 856.0], [728.0, 856.0], [728.0, 871.0], [408.0, 871.0]], "confidence": 0.9808042645454407, "text": "randomly sample from zoomed-in views of the head, upper", "bbox": [408.0, 856.0, 728.0, 871.0]}, {"polygon": [[63.0, 870.0], [384.0, 870.0], [384.0, 885.0], [63.0, 885.0]], "confidence": 0.9814255833625793, "text": "optimization of the background, we optimize human Gaus-", "bbox": [63.0, 870.0, 384.0, 885.0]}, {"polygon": [[409.0, 872.0], [728.0, 872.0], [728.0, 886.0], [409.0, 886.0]], "confidence": 0.9836198687553406, "text": "body, and lower body together with the full body of the posed", "bbox": [409.0, 872.0, 728.0, 886.0]}, {"polygon": [[64.0, 883.0], [141.0, 890.0], [140.0, 906.0], [63.0, 899.0]], "confidence": 0.7735188007354736, "text": "sians G j = 1 ...", "bbox": [64.0, 883.0, 141.0, 906.0]}, {"polygon": [[143.0, 887.0], [384.0, 884.0], [384.0, 900.0], [143.0, 903.0]], "confidence": 0.9615939855575562, "text": "N and background Gaussians GBG together.", "bbox": [143.0, 887.0, 384.0, 900.0]}, {"polygon": [[409.0, 887.0], [728.0, 887.0], [728.0, 902.0], [409.0, 902.0]], "confidence": 0.9820701479911804, "text": "human, and the full body of the canonical with a uniform", "bbox": [409.0, 887.0, 728.0, 902.0]}, {"polygon": [[63.0, 903.0], [384.0, 903.0], [384.0, 918.0], [63.0, 918.0]], "confidence": 0.9741880297660828, "text": "For the first 1.5 k iteration of joint optimization, we fix", "bbox": [63.0, 903.0, 384.0, 918.0]}, {"polygon": [[409.0, 904.0], [728.0, 904.0], [728.0, 918.0], [409.0, 918.0]], "confidence": 0.979923665523529, "text": "probability of 0.2. This two-stage random camera sampling", "bbox": [409.0, 904.0, 728.0, 918.0]}, {"polygon": [[63.0, 920.0], [382.0, 920.0], [382.0, 935.0], [63.0, 935.0]], "confidence": 0.9714593291282654, "text": "the center of human Gaussians µ i on the initial points", "bbox": [63.0, 920.0, 382.0, 935.0]}, {"polygon": [[409.0, 920.0], [728.0, 920.0], [728.0, 934.0], [409.0, 934.0]], "confidence": 0.9831495881080627, "text": "facilitates the detailed reconstruction of unseen parts and", "bbox": [409.0, 920.0, 728.0, 934.0]}, {"polygon": [[63.0, 936.0], [382.0, 936.0], [382.0, 951.0], [63.0, 951.0]], "confidence": 0.9320383071899414, "text": "Xi,init and clamp the opacity oi below 0.9 to avoid the", "bbox": [63.0, 936.0, 382.0, 951.0]}, {"polygon": [[409.0, 937.0], [441.0, 937.0], [441.0, 951.0], [409.0, 951.0]], "confidence": 0.8295519351959229, "text": "head.", "bbox": [409.0, 937.0, 441.0, 951.0]}, {"polygon": [[388.0, 975.0], [404.0, 975.0], [404.0, 990.0], [388.0, 990.0]], "confidence": 0.6371406316757202, "text": "1212", "bbox": [388.0, 975.0, 404.0, 990.0]}], "languages": ["en"], "image_bbox": [0.0, 0.0, 816.0, 1056.0], "page": 12}, {"text_lines": [{"polygon": [[663.0, 117.0], [704.0, 117.0], [704.0, 124.0], [663.0, 124.0]], "confidence": 0.8990601301193237, "text": "Trainable", "bbox": [663.0, 117.0, 704.0, 124.0]}, {"polygon": [[662.0, 129.0], [714.0, 129.0], [714.0, 139.0], [662.0, 139.0]], "confidence": 0.8823575377464294, "text": "w/ gradient", "bbox": [662.0, 129.0, 714.0, 139.0]}, {"polygon": [[662.0, 142.0], [716.0, 142.0], [716.0, 151.0], [662.0, 151.0]], "confidence": 0.8974913358688354, "text": "wo gradient.", "bbox": [662.0, 142.0, 716.0, 151.0]}, {"polygon": [[666.0, 153.0], [709.0, 153.0], [709.0, 163.0], [666.0, 163.0]], "confidence": 0.903815507888794, "text": "(detached)", "bbox": [666.0, 153.0, 709.0, 163.0]}, {"polygon": [[641.0, 236.0], [722.0, 236.0], [722.0, 248.0], [641.0, 248.0]], "confidence": 0.9311970472335815, "text": "Denoised Image", "bbox": [641.0, 236.0, 722.0, 248.0]}, {"polygon": [[643.0, 263.0], [707.0, 263.0], [707.0, 272.0], [643.0, 272.0]], "confidence": 0.9362937808036804, "text": "Text Transforme", "bbox": [643.0, 263.0, 707.0, 272.0]}, {"polygon": [[412.0, 301.0], [498.0, 301.0], [498.0, 313.0], [412.0, 313.0]], "confidence": 0.935937225818634, "text": "OpenPose Joints", "bbox": [412.0, 301.0, 498.0, 313.0]}, {"polygon": [[408.0, 326.0], [728.0, 326.0], [728.0, 340.0], [408.0, 340.0]], "confidence": 0.9809357523918152, "text": "Figure 8. Overall Pipeline of Textual Inversion in our method", "bbox": [408.0, 326.0, 728.0, 340.0]}, {"polygon": [[409.0, 341.0], [728.0, 341.0], [728.0, 354.0], [409.0, 354.0]], "confidence": 0.9839792847633362, "text": "The orange part is what we optimize during textual inversion. V*", "bbox": [409.0, 341.0, 728.0, 354.0]}, {"polygon": [[409.0, 357.0], [729.0, 357.0], [729.0, 368.0], [409.0, 368.0]], "confidence": 0.975508451461792, "text": "indicates textual inversion token <person-j> which is train-", "bbox": [409.0, 357.0, 729.0, 368.0]}, {"polygon": [[409.0, 371.0], [729.0, 370.0], [729.0, 382.0], [409.0, 384.0]], "confidence": 0.980562150478363, "text": "ing target. As shown here, we use CustomDiffusion [ 23 ] to-", "bbox": [409.0, 371.0, 729.0, 382.0]}, {"polygon": [[409.0, 384.0], [729.0, 384.0], [729.0, 398.0], [409.0, 398.0]], "confidence": 0.9771684408187866, "text": "gether with ControlNet [ 64 ] to obtain individuals' inversion token", "bbox": [409.0, 384.0, 729.0, 398.0]}, {"polygon": [[410.0, 400.0], [652.0, 400.0], [652.0, 413.0], [410.0, 413.0]], "confidence": 0.9554985165596008, "text": "<person-j> and fine-tuned diffusion model φ j .", "bbox": [410.0, 400.0, 652.0, 413.0]}, {"polygon": [[82.0, 420.0], [140.0, 420.0], [140.0, 432.0], [82.0, 432.0]], "confidence": 0.9133188724517822, "text": "Unseen view", "bbox": [82.0, 420.0, 140.0, 432.0]}, {"polygon": [[154.0, 422.0], [197.0, 422.0], [197.0, 430.0], [154.0, 430.0]], "confidence": 0.8979357481002808, "text": "Seen view", "bbox": [154.0, 422.0, 197.0, 430.0]}, {"polygon": [[312.0, 422.0], [355.0, 422.0], [355.0, 430.0], [312.0, 430.0]], "confidence": 0.8964223861694336, "text": "Seen view", "bbox": [312.0, 422.0, 355.0, 430.0]}, {"polygon": [[409.0, 435.0], [722.0, 436.0], [722.0, 452.0], [409.0, 451.0]], "confidence": 0.9833554029464722, "text": "is occluded if it's projected on the masks of nearer people.", "bbox": [409.0, 435.0, 722.0, 452.0]}, {"polygon": [[108.0, 440.0], [162.0, 440.0], [162.0, 452.0], [108.0, 452.0]], "confidence": 0.8647684454917908, "text": "(a) CFG 20", "bbox": [108.0, 440.0, 162.0, 452.0]}, {"polygon": [[270.0, 440.0], [329.0, 439.0], [329.0, 451.0], [270.0, 452.0]], "confidence": 0.9071670174598694, "text": "(b) CFG 100", "bbox": [270.0, 440.0, 329.0, 451.0]}, {"polygon": [[63.0, 458.0], [384.0, 458.0], [384.0, 472.0], [63.0, 472.0]], "confidence": 0.978263258934021, "text": "Figure 7. Ablation study for the classifier-free guidance scale.", "bbox": [63.0, 458.0, 384.0, 472.0]}, {"polygon": [[409.0, 461.0], [556.0, 461.0], [556.0, 477.0], [409.0, 477.0]], "confidence": 0.9534924030303955, "text": "B.2. In-the-wild Videos", "bbox": [409.0, 461.0, 556.0, 477.0]}, {"polygon": [[63.0, 474.0], [383.0, 474.0], [383.0, 487.0], [63.0, 487.0]], "confidence": 0.9851261377334595, "text": "We cropped out the black blurry artifacts near the feet due to lack of", "bbox": [63.0, 474.0, 383.0, 487.0]}, {"polygon": [[425.0, 486.0], [729.0, 486.0], [729.0, 501.0], [425.0, 501.0]], "confidence": 0.9814004302024841, "text": "In handling in-the-wild videos, we categorize them into", "bbox": [425.0, 486.0, 729.0, 501.0]}, {"polygon": [[64.0, 489.0], [382.0, 489.0], [382.0, 501.0], [64.0, 501.0]], "confidence": 0.9841215014457703, "text": "space. We can check that a low CFG scale (a) generates a smooth", "bbox": [64.0, 489.0, 382.0, 501.0]}, {"polygon": [[63.0, 503.0], [384.0, 503.0], [384.0, 516.0], [63.0, 516.0]], "confidence": 0.9831204414367676, "text": "monotonic texture in unseen parts while a high CFG scale (b)", "bbox": [63.0, 503.0, 384.0, 516.0]}, {"polygon": [[409.0, 503.0], [728.0, 503.0], [728.0, 517.0], [409.0, 517.0]], "confidence": 0.9817516207695007, "text": "two scenarios: static camera and moving camera. For the", "bbox": [409.0, 503.0, 728.0, 517.0]}, {"polygon": [[63.0, 517.0], [383.0, 517.0], [383.0, 530.0], [63.0, 530.0]], "confidence": 0.983832061290741, "text": "synthesizes and enhances wrinkles of clothing on both seen and", "bbox": [63.0, 517.0, 383.0, 530.0]}, {"polygon": [[408.0, 519.0], [729.0, 519.0], [729.0, 534.0], [408.0, 534.0]], "confidence": 0.9768626093864441, "text": "camera moving cases, we employ DROID-SLAM [ 50 ] to", "bbox": [408.0, 519.0, 729.0, 534.0]}, {"polygon": [[63.0, 532.0], [383.0, 532.0], [383.0, 545.0], [63.0, 545.0]], "confidence": 0.9847432971000671, "text": "unseen parts (lower row), but also introduces more artifacts. (upper", "bbox": [63.0, 532.0, 383.0, 545.0]}, {"polygon": [[408.0, 535.0], [728.0, 535.0], [728.0, 550.0], [408.0, 550.0]], "confidence": 0.9746975898742676, "text": "estimate the initial camera pose and Goel et al. [ 11 ] to track", "bbox": [408.0, 535.0, 728.0, 550.0]}, {"polygon": [[63.0, 547.0], [91.0, 547.0], [91.0, 560.0], [63.0, 560.0]], "confidence": 0.6079667806625366, "text": "(for row)", "bbox": [63.0, 547.0, 91.0, 560.0]}, {"polygon": [[408.0, 550.0], [728.0, 550.0], [728.0, 565.0], [408.0, 565.0]], "confidence": 0.9822283983230591, "text": "people with regressing SMPL parameters. Subsequently, we", "bbox": [408.0, 550.0, 728.0, 565.0]}, {"polygon": [[408.0, 567.0], [729.0, 565.0], [729.0, 580.0], [408.0, 582.0]], "confidence": 0.9812851548194885, "text": "refine the estimated parameters by minimizing the reprojec-", "bbox": [408.0, 567.0, 729.0, 580.0]}, {"polygon": [[63.0, 581.0], [238.0, 581.0], [238.0, 600.0], [63.0, 600.0]], "confidence": 0.9587821364402771, "text": "B. Dataset Preprocessing", "bbox": [63.0, 581.0, 238.0, 600.0]}, {"polygon": [[408.0, 582.0], [728.0, 582.0], [728.0, 596.0], [408.0, 596.0]], "confidence": 0.9830314517021179, "text": "tion error between estimated 2D body joints [ 59 ]. In cases", "bbox": [408.0, 582.0, 728.0, 596.0]}, {"polygon": [[409.0, 598.0], [729.0, 598.0], [729.0, 612.0], [409.0, 612.0]], "confidence": 0.9821880459785461, "text": "with a static camera, we skip the camera pose estimation", "bbox": [409.0, 598.0, 729.0, 612.0]}, {"polygon": [[63.0, 608.0], [230.0, 608.0], [230.0, 623.0], [63.0, 623.0]], "confidence": 0.9397994875907898, "text": "B.1. Panoptic Dataset [ 19 ]", "bbox": [63.0, 608.0, 230.0, 623.0]}, {"polygon": [[409.0, 615.0], [437.0, 615.0], [437.0, 629.0], [409.0, 629.0]], "confidence": 0.780279815196991, "text": "step.", "bbox": [409.0, 615.0, 437.0, 629.0]}, {"polygon": [[80.0, 632.0], [384.0, 632.0], [384.0, 647.0], [80.0, 647.0]], "confidence": 0.9817975759506226, "text": "We trim the last round of the ultimatum 160422 sequence,", "bbox": [80.0, 632.0, 384.0, 647.0]}, {"polygon": [[409.0, 643.0], [703.0, 643.0], [703.0, 661.0], [409.0, 661.0]], "confidence": 0.9722785949707031, "text": "C. Effect of Classifier-Free Guidance Scale", "bbox": [409.0, 643.0, 703.0, 661.0]}, {"polygon": [[63.0, 649.0], [384.0, 649.0], [384.0, 663.0], [63.0, 663.0]], "confidence": 0.980523407459259, "text": "extracting 540 multi-view images of 6 individuals by sub-", "bbox": [63.0, 649.0, 384.0, 663.0]}, {"polygon": [[63.0, 665.0], [383.0, 665.0], [383.0, 679.0], [63.0, 679.0]], "confidence": 0.9802417159080505, "text": "sampling every 4 frames. Among the 31 HD cameras in the", "bbox": [63.0, 665.0, 383.0, 679.0]}, {"polygon": [[425.0, 672.0], [728.0, 672.0], [728.0, 686.0], [425.0, 686.0]], "confidence": 0.9740338325500488, "text": "To explore the impact of changing the classifier-free", "bbox": [425.0, 672.0, 728.0, 686.0]}, {"polygon": [[63.0, 681.0], [384.0, 681.0], [384.0, 696.0], [63.0, 696.0]], "confidence": 0.9784271717071533, "text": "Panoptic Dome, we specifically choose cameras 0, 3, 5, 8,", "bbox": [63.0, 681.0, 384.0, 696.0]}, {"polygon": [[410.0, 688.0], [728.0, 688.0], [728.0, 703.0], [410.0, 703.0]], "confidence": 0.9819820523262024, "text": "guidance (CFG) scale, we conduct an ablation study using", "bbox": [410.0, 688.0, 728.0, 703.0]}, {"polygon": [[64.0, 696.0], [382.0, 696.0], [382.0, 711.0], [64.0, 711.0]], "confidence": 0.9819586277008057, "text": "2, 24, and 25 for evaluation, while camera 16 serves as the", "bbox": [64.0, 696.0, 382.0, 711.0]}, {"polygon": [[409.0, 704.0], [728.0, 704.0], [728.0, 718.0], [409.0, 718.0]], "confidence": 0.961094856262207, "text": "Hi4D [61] pair00–dance sequence. As illustrated in the", "bbox": [409.0, 704.0, 728.0, 718.0]}, {"polygon": [[63.0, 713.0], [383.0, 713.0], [383.0, 727.0], [63.0, 727.0]], "confidence": 0.9831039309501648, "text": "input. To simulate a challenging scenario, we intentionally", "bbox": [63.0, 713.0, 383.0, 727.0]}, {"polygon": [[409.0, 718.0], [729.0, 718.0], [729.0, 733.0], [409.0, 733.0]], "confidence": 0.9765026569366455, "text": "lower row of Fig. 7 , a high CFG scale synthesizes detailed", "bbox": [409.0, 718.0, 729.0, 733.0]}, {"polygon": [[63.0, 729.0], [383.0, 729.0], [383.0, 744.0], [63.0, 744.0]], "confidence": 0.9832531213760376, "text": "pick the input view camera that excludes the entrance of the", "bbox": [63.0, 729.0, 383.0, 744.0]}, {"polygon": [[409.0, 735.0], [729.0, 735.0], [729.0, 749.0], [409.0, 749.0]], "confidence": 0.9813269972801208, "text": "unseen parts such as cloth wrinkles and uniform numbers,", "bbox": [409.0, 735.0, 729.0, 749.0]}, {"polygon": [[63.0, 745.0], [367.0, 745.0], [367.0, 759.0], [63.0, 759.0]], "confidence": 0.9771934747695923, "text": "Panoptic Dome [ 19 ] where individuals enter one by one.", "bbox": [63.0, 745.0, 367.0, 759.0]}, {"polygon": [[409.0, 751.0], [728.0, 751.0], [728.0, 765.0], [409.0, 765.0]], "confidence": 0.9827136397361755, "text": "while a low CFG scale produces a smooth, monotonic texture", "bbox": [409.0, 751.0, 728.0, 765.0]}, {"polygon": [[80.0, 761.0], [384.0, 761.0], [384.0, 776.0], [80.0, 776.0]], "confidence": 0.9757212996482849, "text": "To acquire the SMPL parameters θ t,j and β j of individu-", "bbox": [80.0, 761.0, 384.0, 776.0]}, {"polygon": [[409.0, 767.0], [728.0, 767.0], [728.0, 781.0], [409.0, 781.0]], "confidence": 0.9827780723571777, "text": "without any wrinkles. Notably, a high CFG scale introduces", "bbox": [409.0, 767.0, 728.0, 781.0]}, {"polygon": [[63.0, 777.0], [383.0, 777.0], [383.0, 791.0], [63.0, 791.0]], "confidence": 0.9820118546485901, "text": "als, we optimize them by minimizing the distance between", "bbox": [63.0, 777.0, 383.0, 791.0]}, {"polygon": [[409.0, 783.0], [728.0, 783.0], [728.0, 798.0], [409.0, 798.0]], "confidence": 0.9780680537223816, "text": "more artifacts such as green stains which are amplified by", "bbox": [409.0, 783.0, 728.0, 798.0]}, {"polygon": [[64.0, 792.0], [384.0, 792.0], [384.0, 807.0], [64.0, 807.0]], "confidence": 0.9805627465248108, "text": "3D SMPL joints and provided pseudo ground truth COCO", "bbox": [64.0, 792.0, 384.0, 807.0]}, {"polygon": [[408.0, 799.0], [728.0, 799.0], [728.0, 814.0], [408.0, 814.0]], "confidence": 0.9763286113739014, "text": "the light reflected from the floor shown in the upper row", "bbox": [408.0, 799.0, 728.0, 814.0]}, {"polygon": [[64.0, 808.0], [384.0, 808.0], [384.0, 822.0], [64.0, 822.0]], "confidence": 0.982968270778656, "text": "3D joints. Our optimization process incorporates pose prior,", "bbox": [64.0, 808.0, 384.0, 822.0]}, {"polygon": [[408.0, 815.0], [729.0, 815.0], [729.0, 829.0], [408.0, 829.0]], "confidence": 0.9741507172584534, "text": "of Fig. 7 . This study shows the importance of selecting a", "bbox": [408.0, 815.0, 729.0, 829.0]}, {"polygon": [[63.0, 824.0], [384.0, 824.0], [384.0, 839.0], [63.0, 839.0]], "confidence": 0.9829642176628113, "text": "angle shape regularization, and 3D joint error, as outlined", "bbox": [63.0, 824.0, 384.0, 839.0]}, {"polygon": [[409.0, 830.0], [728.0, 830.0], [728.0, 845.0], [409.0, 845.0]], "confidence": 0.9819259643554688, "text": "proper CFG scale to reconstruct a detailed human avatar", "bbox": [409.0, 830.0, 728.0, 845.0]}, {"polygon": [[63.0, 840.0], [383.0, 840.0], [383.0, 854.0], [63.0, 854.0]], "confidence": 0.981174886226654, "text": "in [ 3 ]. We leverage SMPL joints and SAM [ 22 ] to obtain", "bbox": [63.0, 840.0, 383.0, 854.0]}, {"polygon": [[409.0, 847.0], [531.0, 847.0], [531.0, 861.0], [409.0, 861.0]], "confidence": 0.9578152298927307, "text": "with minimal artifacts.", "bbox": [409.0, 847.0, 531.0, 861.0]}, {"polygon": [[63.0, 856.0], [384.0, 856.0], [384.0, 871.0], [63.0, 871.0]], "confidence": 0.9834269881248474, "text": "each individual's mask in the input frames. Initially, we ar-", "bbox": [63.0, 856.0, 384.0, 871.0]}, {"polygon": [[63.0, 872.0], [383.0, 872.0], [383.0, 886.0], [63.0, 886.0]], "confidence": 0.982676088809967, "text": "range individuals based on their depth which is calculated", "bbox": [63.0, 872.0, 383.0, 886.0]}, {"polygon": [[409.0, 877.0], [619.0, 877.0], [619.0, 894.0], [409.0, 894.0]], "confidence": 0.9670476913452148, "text": "D. Details of Textual Inversion", "bbox": [409.0, 877.0, 619.0, 894.0]}, {"polygon": [[63.0, 887.0], [383.0, 887.0], [383.0, 902.0], [63.0, 902.0]], "confidence": 0.982427716255188, "text": "as the distance between the pelvis of SMPL and the camera", "bbox": [63.0, 887.0, 383.0, 902.0]}, {"polygon": [[63.0, 904.0], [384.0, 904.0], [384.0, 918.0], [63.0, 918.0]], "confidence": 0.9824559688568115, "text": "center. Starting with the individual closest to the camera,", "bbox": [63.0, 904.0, 384.0, 918.0]}, {"polygon": [[426.0, 904.0], [728.0, 904.0], [728.0, 918.0], [426.0, 918.0]], "confidence": 0.9758643507957458, "text": "To obtain an individual's text-token <person-j> and", "bbox": [426.0, 904.0, 728.0, 918.0]}, {"polygon": [[409.0, 919.0], [729.0, 919.0], [729.0, 934.0], [409.0, 934.0]], "confidence": 0.9757699370384216, "text": "specified fine-tuned diffusion, we run CustomDiffusion on", "bbox": [409.0, 919.0, 729.0, 934.0]}, {"polygon": [[63.0, 920.0], [383.0, 920.0], [383.0, 934.0], [63.0, 934.0]], "confidence": 0.9812671542167664, "text": "we obtain a mask by querying the projected SMPL joints", "bbox": [63.0, 920.0, 383.0, 934.0]}, {"polygon": [[63.0, 936.0], [383.0, 936.0], [383.0, 951.0], [63.0, 951.0]], "confidence": 0.9820718169212341, "text": "which is not occluded into SAM [ 22 ]. We assume the joints", "bbox": [63.0, 936.0, 383.0, 951.0]}, {"polygon": [[409.0, 936.0], [728.0, 936.0], [728.0, 950.0], [409.0, 950.0]], "confidence": 0.9743883609771729, "text": "each individual's observations with modifications as shown", "bbox": [409.0, 936.0, 728.0, 950.0]}, {"polygon": [[388.0, 975.0], [405.0, 975.0], [405.0, 990.0], [388.0, 990.0]], "confidence": 0.5337509512901306, "text": "13", "bbox": [388.0, 975.0, 405.0, 990.0]}], "languages": ["en"], "image_bbox": [0.0, 0.0, 816.0, 1056.0], "page": 13}, {"text_lines": [{"polygon": [[409.0, 96.0], [728.0, 96.0], [728.0, 111.0], [409.0, 111.0]], "confidence": 0.9818841218948364, "text": "an extreme lack of frontal train views. We further show", "bbox": [409.0, 96.0, 728.0, 111.0]}, {"polygon": [[408.0, 113.0], [728.0, 113.0], [728.0, 127.0], [408.0, 127.0]], "confidence": 0.9811727404594421, "text": "such scenario in Fig. 9 (b), where training the TI with just", "bbox": [408.0, 113.0, 728.0, 127.0]}, {"polygon": [[408.0, 129.0], [728.0, 129.0], [728.0, 143.0], [408.0, 143.0]], "confidence": 0.983252763748169, "text": "a single additional frontal image substantially improves the", "bbox": [408.0, 129.0, 728.0, 143.0]}, {"polygon": [[408.0, 145.0], [728.0, 145.0], [728.0, 160.0], [408.0, 160.0]], "confidence": 0.9786555171012878, "text": "resemblance of the outputs, compared to Fig. 9 (a). This", "bbox": [408.0, 145.0, 728.0, 160.0]}, {"polygon": [[409.0, 161.0], [728.0, 161.0], [728.0, 175.0], [409.0, 175.0]], "confidence": 0.9832724332809448, "text": "demonstrates the unique advantage of using textual inversion", "bbox": [409.0, 161.0, 728.0, 175.0]}, {"polygon": [[409.0, 176.0], [728.0, 176.0], [728.0, 191.0], [409.0, 191.0]], "confidence": 0.9811465740203857, "text": "for reconstruction, a method that is difficult to leverage using", "bbox": [409.0, 176.0, 728.0, 191.0]}, {"polygon": [[74.0, 177.0], [131.0, 177.0], [131.0, 189.0], [74.0, 189.0]], "confidence": 0.915194034576416, "text": "Train views", "bbox": [74.0, 177.0, 131.0, 189.0]}, {"polygon": [[409.0, 193.0], [542.0, 193.0], [542.0, 206.0], [409.0, 206.0]], "confidence": 0.9609515070915222, "text": "only reconstruction loss.", "bbox": [409.0, 193.0, 542.0, 206.0]}, {"polygon": [[68.0, 271.0], [146.0, 271.0], [146.0, 283.0], [68.0, 283.0]], "confidence": 0.9133164882659912, "text": "Additional data", "bbox": [68.0, 271.0, 146.0, 283.0]}, {"polygon": [[154.0, 271.0], [202.0, 271.0], [202.0, 284.0], [154.0, 284.0]], "confidence": 0.8335840702056885, "text": "(a) Ours", "bbox": [154.0, 271.0, 202.0, 284.0]}, {"polygon": [[223.0, 271.0], [272.0, 271.0], [272.0, 284.0], [223.0, 284.0]], "confidence": 0.8816860318183899, "text": "(b) Ours", "bbox": [223.0, 271.0, 272.0, 284.0]}, {"polygon": [[286.0, 271.0], [381.0, 271.0], [381.0, 284.0], [286.0, 284.0]], "confidence": 0.9378312826156616, "text": "(c) Ground Truth", "bbox": [286.0, 271.0, 381.0, 284.0]}, {"polygon": [[68.0, 286.0], [142.0, 286.0], [142.0, 299.0], [68.0, 299.0]], "confidence": 0.9270952939987183, "text": "(Only for TI*)", "bbox": [68.0, 286.0, 142.0, 299.0]}, {"polygon": [[172.0, 286.0], [204.0, 286.0], [204.0, 300.0], [172.0, 300.0]], "confidence": 0.7822425365447998, "text": "w/ TI", "bbox": [172.0, 286.0, 204.0, 300.0]}, {"polygon": [[240.0, 286.0], [276.0, 286.0], [276.0, 300.0], [240.0, 300.0]], "confidence": 0.7696161866188049, "text": "w/ TI*", "bbox": [240.0, 286.0, 276.0, 300.0]}, {"polygon": [[63.0, 313.0], [384.0, 313.0], [384.0, 326.0], [63.0, 326.0]], "confidence": 0.9795930981636047, "text": "Figure 9. Ablation study of adding additional data during Tex-", "bbox": [63.0, 313.0, 384.0, 326.0]}, {"polygon": [[63.0, 329.0], [382.0, 329.0], [382.0, 341.0], [63.0, 341.0]], "confidence": 0.9844299554824829, "text": "tual Inversion. TI* means the textual inversion used in SDS loss is", "bbox": [63.0, 329.0, 382.0, 341.0]}, {"polygon": [[63.0, 342.0], [384.0, 342.0], [384.0, 356.0], [63.0, 356.0]], "confidence": 0.9852069616317749, "text": "trained with a single additional image of the frontal view. Both (a)", "bbox": [63.0, 342.0, 384.0, 356.0]}, {"polygon": [[63.0, 357.0], [383.0, 357.0], [383.0, 371.0], [63.0, 371.0]], "confidence": 0.985030472278595, "text": "and (b) are optimized with train views and the only difference is in", "bbox": [63.0, 357.0, 383.0, 371.0]}, {"polygon": [[63.0, 373.0], [170.0, 373.0], [170.0, 385.0], [63.0, 385.0]], "confidence": 0.9551016688346863, "text": "the Textual Inversion.", "bbox": [63.0, 373.0, 170.0, 385.0]}, {"polygon": [[63.0, 408.0], [382.0, 408.0], [382.0, 423.0], [63.0, 423.0]], "confidence": 0.9714149832725525, "text": "in Fig. 8 . We use OpenPose ControlNet [ 64 ] during Textual", "bbox": [63.0, 408.0, 382.0, 423.0]}, {"polygon": [[63.0, 424.0], [382.0, 424.0], [382.0, 439.0], [63.0, 439.0]], "confidence": 0.9778622984886169, "text": "Inversion to avoid possible overfitting on observed body", "bbox": [63.0, 424.0, 382.0, 439.0]}, {"polygon": [[63.0, 440.0], [383.0, 440.0], [383.0, 454.0], [63.0, 454.0]], "confidence": 0.9819234013557434, "text": "pose and camera pose. To obtain an individual's text-token", "bbox": [63.0, 440.0, 383.0, 454.0]}, {"polygon": [[65.0, 456.0], [383.0, 456.0], [383.0, 470.0], [65.0, 470.0]], "confidence": 0.9626666307449341, "text": "<person-j> and specified fine-tuned diffusion, we first", "bbox": [65.0, 456.0, 383.0, 470.0]}, {"polygon": [[63.0, 472.0], [382.0, 472.0], [382.0, 486.0], [63.0, 486.0]], "confidence": 0.9823199510574341, "text": "randomly perturb the observed image and then estimate the", "bbox": [63.0, 472.0, 382.0, 486.0]}, {"polygon": [[63.0, 488.0], [382.0, 488.0], [382.0, 502.0], [63.0, 502.0]], "confidence": 0.9824377298355103, "text": "added noise of the perturbed image. By minimizing the MSE", "bbox": [63.0, 488.0, 382.0, 502.0]}, {"polygon": [[63.0, 504.0], [382.0, 504.0], [382.0, 518.0], [63.0, 518.0]], "confidence": 0.981735110282898, "text": "loss between the added noise and the estimated noise, we", "bbox": [63.0, 504.0, 382.0, 518.0]}, {"polygon": [[63.0, 520.0], [383.0, 520.0], [383.0, 534.0], [63.0, 534.0]], "confidence": 0.9775635600090027, "text": "optimize the text-token and fine-tune the diffusion model.", "bbox": [63.0, 520.0, 383.0, 534.0]}, {"polygon": [[64.0, 535.0], [382.0, 535.0], [382.0, 550.0], [64.0, 550.0]], "confidence": 0.9833434820175171, "text": "As we use the latent diffusion model [ 42 ] here, the training", "bbox": [64.0, 535.0, 382.0, 550.0]}, {"polygon": [[63.0, 551.0], [189.0, 551.0], [189.0, 565.0], [63.0, 565.0]], "confidence": 0.9593055844306946, "text": "objective is as follows:", "bbox": [63.0, 551.0, 189.0, 565.0]}, {"polygon": [[357.0, 580.0], [384.0, 580.0], [384.0, 594.0], [357.0, 594.0]], "confidence": 0.7889362573623657, "text": "(16)", "bbox": [357.0, 580.0, 384.0, 594.0]}, {"polygon": [[129.0, 581.0], [318.0, 579.0], [318.0, 595.0], [129.0, 596.0]], "confidence": 0.758357584476471, "text": "Ltextual = MSE(cp(z,,y,T) - c)", "bbox": [129.0, 581.0, 318.0, 595.0]}, {"polygon": [[64.0, 608.0], [382.0, 608.0], [382.0, 622.0], [64.0, 622.0]], "confidence": 0.9580488801002502, "text": ", where z r is a perturbed latent corresponding to perturbed", "bbox": [64.0, 608.0, 382.0, 622.0]}, {"polygon": [[63.0, 623.0], [384.0, 623.0], [384.0, 638.0], [63.0, 638.0]], "confidence": 0.972185492515564, "text": "image in Fig. 8 and ε is the added noise. During optimization,", "bbox": [63.0, 623.0, 384.0, 638.0]}, {"polygon": [[63.0, 640.0], [285.0, 640.0], [285.0, 654.0], [63.0, 654.0]], "confidence": 0.9464176893234253, "text": "we randomly sample τ from τ ∼ U [0, 1].", "bbox": [63.0, 640.0, 285.0, 654.0]}, {"polygon": [[80.0, 656.0], [382.0, 656.0], [382.0, 670.0], [80.0, 670.0]], "confidence": 0.9742228388786316, "text": "We optimize textual token and fine-tune diffusion using", "bbox": [80.0, 656.0, 382.0, 670.0]}, {"polygon": [[63.0, 672.0], [382.0, 672.0], [382.0, 686.0], [63.0, 686.0]], "confidence": 0.9689621329307556, "text": "Adam [ 21 ] optimizer with learning rate 5e − 6 and batch size", "bbox": [63.0, 672.0, 382.0, 686.0]}, {"polygon": [[63.0, 688.0], [383.0, 688.0], [383.0, 703.0], [63.0, 703.0]], "confidence": 0.9837766289710999, "text": "4 for 1000 iterations. To mitigate the situation where the text", "bbox": [63.0, 688.0, 383.0, 703.0]}, {"polygon": [[63.0, 704.0], [383.0, 704.0], [383.0, 718.0], [63.0, 718.0]], "confidence": 0.9818953275680542, "text": "token learns the background, we mask out the background", "bbox": [63.0, 704.0, 383.0, 718.0]}, {"polygon": [[63.0, 720.0], [383.0, 720.0], [383.0, 734.0], [63.0, 734.0]], "confidence": 0.9783512949943542, "text": "and randomly fill it with random color. We do not use prior", "bbox": [63.0, 720.0, 383.0, 734.0]}, {"polygon": [[63.0, 736.0], [383.0, 736.0], [383.0, 750.0], [63.0, 750.0]], "confidence": 0.9762796759605408, "text": "preservation loss here to overfit the text token on observed", "bbox": [63.0, 736.0, 383.0, 750.0]}, {"polygon": [[63.0, 752.0], [383.0, 752.0], [383.0, 766.0], [63.0, 766.0]], "confidence": 0.9754436612129211, "text": "images. The text-token <person-j> is queried only in", "bbox": [63.0, 752.0, 383.0, 766.0]}, {"polygon": [[63.0, 768.0], [382.0, 768.0], [382.0, 783.0], [63.0, 783.0]], "confidence": 0.9816113710403442, "text": "Diffusion U-Net and not queried in the ControlNet module", "bbox": [63.0, 768.0, 382.0, 783.0]}, {"polygon": [[63.0, 784.0], [167.0, 784.0], [167.0, 799.0], [63.0, 799.0]], "confidence": 0.9488093852996826, "text": "as shown in Fig. 8 .", "bbox": [63.0, 784.0, 167.0, 799.0]}, {"polygon": [[63.0, 813.0], [382.0, 813.0], [382.0, 830.0], [63.0, 830.0]], "confidence": 0.9771792888641357, "text": "E. Enhancing Identity with Additional Images", "bbox": [63.0, 813.0, 382.0, 830.0]}, {"polygon": [[79.0, 840.0], [383.0, 840.0], [383.0, 854.0], [79.0, 854.0]], "confidence": 0.980809211730957, "text": "By employing additional image sources for the target", "bbox": [79.0, 840.0, 383.0, 854.0]}, {"polygon": [[63.0, 856.0], [382.0, 856.0], [382.0, 871.0], [63.0, 871.0]], "confidence": 0.9826525449752808, "text": "identity, if they are known in advance, we can enhance the", "bbox": [63.0, 856.0, 382.0, 871.0]}, {"polygon": [[63.0, 872.0], [384.0, 872.0], [384.0, 886.0], [63.0, 886.0]], "confidence": 0.9794227480888367, "text": "identity of the person with sparse observations. Specifically,", "bbox": [63.0, 872.0, 384.0, 886.0]}, {"polygon": [[63.0, 887.0], [382.0, 887.0], [382.0, 902.0], [63.0, 902.0]], "confidence": 0.9832842350006104, "text": "training the Textual Inversion (TI) with an extra face image", "bbox": [63.0, 887.0, 382.0, 902.0]}, {"polygon": [[63.0, 904.0], [382.0, 904.0], [382.0, 918.0], [63.0, 918.0]], "confidence": 0.9830700159072876, "text": "of the target person, assuming this information is available", "bbox": [63.0, 904.0, 382.0, 918.0]}, {"polygon": [[63.0, 920.0], [382.0, 920.0], [382.0, 934.0], [63.0, 934.0]], "confidence": 0.982998251914978, "text": "beforehand, enables our method to produce results that more", "bbox": [63.0, 920.0, 382.0, 934.0]}, {"polygon": [[63.0, 936.0], [383.0, 936.0], [383.0, 950.0], [63.0, 950.0]], "confidence": 0.9824649095535278, "text": "closely resemble the target human, even in scenarios with", "bbox": [63.0, 936.0, 383.0, 950.0]}, {"polygon": [[388.0, 975.0], [405.0, 975.0], [405.0, 990.0], [388.0, 990.0]], "confidence": 0.6367051005363464, "text": "14", "bbox": [388.0, 975.0, 405.0, 990.0]}], "languages": ["en"], "image_bbox": [0.0, 0.0, 816.0, 1056.0], "page": 14}, {"text_lines": [{"polygon": [[329.0, 262.0], [461.0, 262.0], [461.0, 277.0], [329.0, 277.0]], "confidence": 0.9569759368896484, "text": "Table 6. Table of notations.", "bbox": [329.0, 262.0, 461.0, 277.0]}, {"polygon": [[250.0, 295.0], [316.0, 295.0], [316.0, 311.0], [250.0, 311.0]], "confidence": 0.9156114459037781, "text": "Description", "bbox": [250.0, 295.0, 316.0, 311.0]}, {"polygon": [[169.0, 296.0], [216.0, 294.0], [217.0, 310.0], [169.0, 312.0]], "confidence": 0.8536238074302673, "text": "Symbol", "bbox": [169.0, 296.0, 216.0, 310.0]}, {"polygon": [[378.0, 318.0], [414.0, 318.0], [414.0, 332.0], [378.0, 332.0]], "confidence": 0.829102098941803, "text": "Index", "bbox": [378.0, 318.0, 414.0, 332.0]}, {"polygon": [[250.0, 334.0], [562.0, 332.0], [562.0, 349.0], [250.0, 351.0]], "confidence": 0.9595486521720886, "text": "Gaussian index, i ∈ { 1 , . . , N } in 3D Gaussian attributes", "bbox": [250.0, 334.0, 562.0, 349.0]}, {"polygon": [[170.0, 336.0], [180.0, 336.0], [180.0, 348.0], [170.0, 348.0]], "confidence": 0.4727568030357361, "text": "i", "bbox": [170.0, 336.0, 180.0, 348.0]}, {"polygon": [[250.0, 349.0], [620.0, 349.0], [620.0, 367.0], [250.0, 367.0]], "confidence": 0.9406765699386597, "text": "Human index, in human Gaussians G it and SMPL parameters θ j,t, β j", "bbox": [250.0, 349.0, 620.0, 367.0]}, {"polygon": [[170.0, 352.0], [180.0, 352.0], [180.0, 367.0], [170.0, 367.0]], "confidence": 0.30162519216537476, "text": "i", "bbox": [170.0, 352.0, 180.0, 367.0]}, {"polygon": [[250.0, 366.0], [618.0, 366.0], [618.0, 382.0], [250.0, 382.0]], "confidence": 0.9692481160163879, "text": "Time index, t ∈ { 1 , . . . , T } in SMPL pose parameters, input images", "bbox": [250.0, 366.0, 618.0, 382.0]}, {"polygon": [[170.0, 367.0], [180.0, 367.0], [180.0, 381.0], [170.0, 381.0]], "confidence": 0.4983445703983307, "text": "t", "bbox": [170.0, 367.0, 180.0, 381.0]}, {"polygon": [[170.0, 383.0], [182.0, 383.0], [182.0, 397.0], [170.0, 397.0]], "confidence": 0.4891315996646881, "text": "k", "bbox": [170.0, 383.0, 182.0, 397.0]}, {"polygon": [[250.0, 383.0], [519.0, 383.0], [519.0, 400.0], [250.0, 400.0]], "confidence": 0.9550159573554993, "text": "Joint index, k ∈ { 1 , . . . , N joint } in LBS skinning", "bbox": [250.0, 383.0, 519.0, 400.0]}, {"polygon": [[286.0, 403.0], [505.0, 403.0], [505.0, 419.0], [286.0, 419.0]], "confidence": 0.9721773266792297, "text": "Learnable Attributes of 3D Gaussians", "bbox": [286.0, 403.0, 505.0, 419.0]}, {"polygon": [[169.0, 420.0], [220.0, 419.0], [220.0, 435.0], [170.0, 436.0]], "confidence": 0.6356064081192017, "text": "µ µ i E R 3", "bbox": [169.0, 420.0, 220.0, 435.0]}, {"polygon": [[250.0, 421.0], [379.0, 421.0], [379.0, 435.0], [250.0, 435.0]], "confidence": 0.8956601023674011, "text": "Center of --th Gaussian", "bbox": [250.0, 421.0, 379.0, 435.0]}, {"polygon": [[248.0, 436.0], [580.0, 436.0], [580.0, 452.0], [248.0, 452.0]], "confidence": 0.9741998910903931, "text": "Covariance Matrix's Quaternion Component of i -th Gaussian", "bbox": [248.0, 436.0, 580.0, 452.0]}, {"polygon": [[170.0, 437.0], [240.0, 437.0], [240.0, 452.0], [170.0, 452.0]], "confidence": 0.7176531553268433, "text": "q i  E SO(3)", "bbox": [170.0, 437.0, 240.0, 452.0]}, {"polygon": [[250.0, 453.0], [550.0, 453.0], [550.0, 467.0], [250.0, 467.0]], "confidence": 0.9729517698287964, "text": "Covariance Matrix's Scale Component of i -th Gaussian", "bbox": [250.0, 453.0, 550.0, 467.0]}, {"polygon": [[169.0, 454.0], [218.0, 452.0], [219.0, 467.0], [170.0, 469.0]], "confidence": 0.6330736875534058, "text": "s E R 3", "bbox": [169.0, 454.0, 218.0, 467.0]}, {"polygon": [[169.0, 468.0], [217.0, 468.0], [217.0, 484.0], [169.0, 484.0]], "confidence": 0.5197731256484985, "text": "c C R 3", "bbox": [169.0, 468.0, 217.0, 484.0]}, {"polygon": [[250.0, 469.0], [375.0, 469.0], [375.0, 484.0], [250.0, 484.0]], "confidence": 0.9310839176177979, "text": "Color of i -th Gaussian", "bbox": [250.0, 469.0, 375.0, 484.0]}, {"polygon": [[169.0, 485.0], [211.0, 485.0], [211.0, 500.0], [169.0, 500.0]], "confidence": 0.43239477276802063, "text": "0 E R", "bbox": [169.0, 485.0, 211.0, 500.0]}, {"polygon": [[250.0, 485.0], [385.0, 485.0], [385.0, 498.0], [250.0, 498.0]], "confidence": 0.902387797832489, "text": "Opacity of -th Gaussian", "bbox": [250.0, 485.0, 385.0, 498.0]}, {"polygon": [[250.0, 499.0], [483.0, 502.0], [483.0, 518.0], [250.0, 515.0]], "confidence": 0.9366708397865295, "text": "i -th Gaussian consists of { μ i , q i , s i , c i , o i }", "bbox": [250.0, 499.0, 483.0, 518.0]}, {"polygon": [[169.0, 501.0], [188.0, 501.0], [188.0, 516.0], [169.0, 516.0]], "confidence": 0.27753978967666626, "text": "G 2", "bbox": [169.0, 501.0, 188.0, 516.0]}, {"polygon": [[306.0, 521.0], [486.0, 521.0], [486.0, 537.0], [306.0, 537.0]], "confidence": 0.9613208770751953, "text": "Parameters of Diffusion Model", "bbox": [306.0, 521.0, 486.0, 537.0]}, {"polygon": [[250.0, 537.0], [576.0, 538.0], [576.0, 554.0], [250.0, 553.0]], "confidence": 0.9758849143981934, "text": "Diffusion model / Diffusion model fine-tuned on j -th person", "bbox": [250.0, 537.0, 576.0, 554.0]}, {"polygon": [[169.0, 540.0], [201.0, 540.0], [201.0, 555.0], [169.0, 555.0]], "confidence": 0.6402724385261536, "text": "φ/φ j", "bbox": [169.0, 540.0, 201.0, 555.0]}, {"polygon": [[250.0, 555.0], [491.0, 555.0], [491.0, 569.0], [250.0, 569.0]], "confidence": 0.9600639939308167, "text": "noise time-step of diffusion model τ ∈ [0, 1]", "bbox": [250.0, 555.0, 491.0, 569.0]}, {"polygon": [[171.0, 557.0], [181.0, 557.0], [181.0, 569.0], [171.0, 569.0]], "confidence": 0.46111932396888733, "text": "T", "bbox": [171.0, 557.0, 181.0, 569.0]}, {"polygon": [[250.0, 571.0], [586.0, 571.0], [586.0, 586.0], [250.0, 586.0]], "confidence": 0.9831281304359436, "text": "Encoded latent of the queried RGB images on diffusion model", "bbox": [250.0, 571.0, 586.0, 586.0]}, {"polygon": [[171.0, 572.0], [187.0, 572.0], [187.0, 586.0], [171.0, 586.0]], "confidence": 0.2781091332435608, "text": "2020", "bbox": [171.0, 572.0, 187.0, 586.0]}, {"polygon": [[250.0, 587.0], [502.0, 587.0], [502.0, 601.0], [250.0, 601.0]], "confidence": 0.9582818746566772, "text": "Perturbed latent with noise time-step τ ∈ [0, 1]", "bbox": [250.0, 587.0, 502.0, 601.0]}, {"polygon": [[171.0, 589.0], [187.0, 589.0], [187.0, 602.0], [171.0, 602.0]], "confidence": 0.15716373920440674, "text": "z ,", "bbox": [171.0, 589.0, 187.0, 602.0]}, {"polygon": [[250.0, 602.0], [386.0, 602.0], [386.0, 617.0], [250.0, 617.0]], "confidence": 0.9604580402374268, "text": "Noise added to the latent", "bbox": [250.0, 602.0, 386.0, 617.0]}, {"polygon": [[170.0, 606.0], [180.0, 606.0], [180.0, 617.0], [170.0, 617.0]], "confidence": 0.3831683099269867, "text": "e", "bbox": [170.0, 606.0, 180.0, 617.0]}, {"polygon": [[250.0, 619.0], [455.0, 619.0], [455.0, 634.0], [250.0, 634.0]], "confidence": 0.969303548336029, "text": "Noise estimated by diffusion model φ", "bbox": [250.0, 619.0, 455.0, 634.0]}, {"polygon": [[169.0, 620.0], [186.0, 620.0], [186.0, 635.0], [169.0, 635.0]], "confidence": 0.08142735809087753, "text": "EE", "bbox": [169.0, 620.0, 186.0, 635.0]}, {"polygon": [[293.0, 641.0], [499.0, 641.0], [499.0, 656.0], [293.0, 656.0]], "confidence": 0.9648215174674988, "text": "Parameters of Human Deformation", "bbox": [293.0, 641.0, 499.0, 656.0]}, {"polygon": [[250.0, 656.0], [577.0, 658.0], [577.0, 674.0], [250.0, 672.0]], "confidence": 0.9609823226928711, "text": "SMPL pose parameter of j -th Human in time t ∈ { 1 , . . . , T }", "bbox": [250.0, 656.0, 577.0, 674.0]}, {"polygon": [[168.0, 659.0], [231.0, 655.0], [231.0, 672.0], [169.0, 675.0]], "confidence": 0.7652168273925781, "text": "θ j,t  ∈  R 72", "bbox": [168.0, 659.0, 231.0, 672.0]}, {"polygon": [[250.0, 674.0], [461.0, 674.0], [461.0, 689.0], [250.0, 689.0]], "confidence": 0.9644028544425964, "text": "SMPL shape parameter of j -th Human", "bbox": [250.0, 674.0, 461.0, 689.0]}, {"polygon": [[168.0, 675.0], [226.0, 671.0], [226.0, 687.0], [169.0, 691.0]], "confidence": 0.8760932683944702, "text": "β ", "bbox": [168.0, 675.0, 226.0, 687.0]}, {"polygon": [[249.0, 689.0], [510.0, 689.0], [510.0, 705.0], [249.0, 705.0]], "confidence": 0.9782662987709045, "text": "Canonical pose parameter shared for all humans", "bbox": [249.0, 689.0, 510.0, 705.0]}, {"polygon": [[168.0, 691.0], [224.0, 686.0], [224.0, 703.0], [169.0, 707.0]], "confidence": 0.6600868105888367, "text": "0 C E R 72", "bbox": [168.0, 691.0, 224.0, 703.0]}, {"polygon": [[302.0, 711.0], [489.0, 711.0], [489.0, 726.0], [302.0, 726.0]], "confidence": 0.9626705646514893, "text": "Rendered and Observed Images", "bbox": [302.0, 711.0, 489.0, 726.0]}, {"polygon": [[250.0, 726.0], [555.0, 728.0], [555.0, 743.0], [250.0, 742.0]], "confidence": 0.9493451714515686, "text": "Rendered / Observed RGB image in time t E {1,...,T}", "bbox": [250.0, 726.0, 555.0, 743.0]}, {"polygon": [[171.0, 728.0], [206.0, 728.0], [206.0, 744.0], [171.0, 744.0]], "confidence": 0.7702105045318604, "text": "R t / I t", "bbox": [171.0, 728.0, 206.0, 744.0]}, {"polygon": [[171.0, 743.0], [189.0, 743.0], [189.0, 759.0], [171.0, 759.0]], "confidence": 0.3452851176261902, "text": "R h", "bbox": [171.0, 743.0, 189.0, 759.0]}, {"polygon": [[250.0, 743.0], [513.0, 743.0], [513.0, 759.0], [250.0, 759.0]], "confidence": 0.975782573223114, "text": "Rendered RGB image of a human with camera v", "bbox": [250.0, 743.0, 513.0, 759.0]}, {"polygon": [[388.0, 975.0], [404.0, 975.0], [404.0, 990.0], [388.0, 990.0]], "confidence": 0.4990405738353729, "text": "15", "bbox": [388.0, 975.0, 404.0, 990.0]}], "languages": ["en"], "image_bbox": [0.0, 0.0, 816.0, 1056.0], "page": 15}]}