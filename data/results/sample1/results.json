{"sample1": [{"text_lines": [{"polygon": [[193.0, 131.0], [620.0, 131.0], [620.0, 155.0], [193.0, 155.0]], "confidence": 0.9692570567131042, "text": "SpaceByte: Towards Deleting Tokenization", "bbox": [193.0, 131.0, 620.0, 155.0]}, {"polygon": [[247.0, 158.0], [567.0, 158.0], [567.0, 182.0], [247.0, 182.0]], "confidence": 0.9652006030082703, "text": "from Large Language Modeling", "bbox": [247.0, 158.0, 567.0, 182.0]}, {"polygon": [[369.0, 240.0], [444.0, 240.0], [444.0, 255.0], [369.0, 255.0]], "confidence": 0.9210072755813599, "text": "Kevin Slagle", "bbox": [369.0, 240.0, 444.0, 255.0]}, {"polygon": [[364.0, 255.0], [449.0, 255.0], [449.0, 269.0], [364.0, 269.0]], "confidence": 0.936122715473175, "text": "Rice University", "bbox": [364.0, 255.0, 449.0, 269.0]}, {"polygon": [[332.0, 270.0], [482.0, 270.0], [482.0, 284.0], [332.0, 284.0]], "confidence": 0.9529482126235962, "text": "kevin.slagle@rice.edu", "bbox": [332.0, 270.0, 482.0, 284.0]}, {"polygon": [[10.0, 303.0], [55.0, 303.0], [52.0, 733.0], [8.0, 732.0]], "confidence": 0.9641523361206055, "text": "rXiv:2404.14408v1 [cs.CL] 22 Apr 202", "bbox": [10.0, 303.0, 55.0, 733.0]}, {"polygon": [[376.0, 322.0], [438.0, 322.0], [438.0, 338.0], [376.0, 338.0]], "confidence": 0.885407030582428, "text": "Abstract", "bbox": [376.0, 322.0, 438.0, 338.0]}, {"polygon": [[189.0, 355.0], [624.0, 355.0], [624.0, 369.0], [189.0, 369.0]], "confidence": 0.9848861694335938, "text": "Tokenization is widely used in large language models because it significantly", "bbox": [189.0, 355.0, 624.0, 369.0]}, {"polygon": [[188.0, 370.0], [625.0, 370.0], [625.0, 384.0], [188.0, 384.0]], "confidence": 0.987159252166748, "text": "improves performance. However, tokenization imposes several disadvantages, such", "bbox": [188.0, 370.0, 625.0, 384.0]}, {"polygon": [[188.0, 384.0], [625.0, 384.0], [625.0, 398.0], [188.0, 398.0]], "confidence": 0.9873313307762146, "text": "as performance biases, increased adversarial vulnerability, decreased character-", "bbox": [188.0, 384.0, 625.0, 398.0]}, {"polygon": [[188.0, 399.0], [624.0, 399.0], [624.0, 413.0], [188.0, 413.0]], "confidence": 0.9872328042984009, "text": "level modeling performance, and increased modeling complexity. To address these", "bbox": [188.0, 399.0, 624.0, 413.0]}, {"polygon": [[188.0, 414.0], [624.0, 414.0], [624.0, 427.0], [188.0, 427.0]], "confidence": 0.9854700565338135, "text": "disadvantages without sacrificing performance, we propose SpaceByte, a novel", "bbox": [188.0, 414.0, 624.0, 427.0]}, {"polygon": [[188.0, 429.0], [624.0, 429.0], [624.0, 442.0], [188.0, 442.0]], "confidence": 0.9849538207054138, "text": "byte-level decoder architecture that closes the performance gap between byte-level", "bbox": [188.0, 429.0, 624.0, 442.0]}, {"polygon": [[188.0, 443.0], [625.0, 443.0], [625.0, 457.0], [188.0, 457.0]], "confidence": 0.9857083559036255, "text": "and subword autoregressive language modeling. SpaceByte consists of a byte-level", "bbox": [188.0, 443.0, 625.0, 457.0]}, {"polygon": [[189.0, 458.0], [626.0, 458.0], [626.0, 471.0], [189.0, 471.0]], "confidence": 0.9877316355705261, "text": "Transformer model, but with extra larger transformer blocks inserted in the middle", "bbox": [189.0, 458.0, 626.0, 471.0]}, {"polygon": [[188.0, 472.0], [625.0, 472.0], [625.0, 486.0], [188.0, 486.0]], "confidence": 0.9853821992874146, "text": "of the layers. We find that performance is significantly improved by applying these", "bbox": [188.0, 472.0, 625.0, 486.0]}, {"polygon": [[189.0, 486.0], [625.0, 486.0], [625.0, 501.0], [189.0, 501.0]], "confidence": 0.9875025153160095, "text": "larger blocks only after certain bytes, such as space characters, which typically", "bbox": [189.0, 486.0, 625.0, 501.0]}, {"polygon": [[188.0, 501.0], [625.0, 501.0], [625.0, 515.0], [188.0, 515.0]], "confidence": 0.9839563965797424, "text": "denote word boundaries.  Our experiments show that for a fixed training and", "bbox": [188.0, 501.0, 625.0, 515.0]}, {"polygon": [[188.0, 516.0], [624.0, 516.0], [624.0, 530.0], [188.0, 530.0]], "confidence": 0.9852660894393921, "text": "inference compute budget, SpaceByte outperforms other byte-level architectures", "bbox": [188.0, 516.0, 624.0, 530.0]}, {"polygon": [[189.0, 531.0], [605.0, 531.0], [605.0, 544.0], [189.0, 544.0]], "confidence": 0.9865506887435913, "text": "and roughly matches the performance of tokenized Transformer architectures.", "bbox": [189.0, 531.0, 605.0, 544.0]}, {"polygon": [[142.0, 573.0], [153.0, 573.0], [153.0, 588.0], [142.0, 588.0]], "confidence": 0.40550556778907776, "text": "1", "bbox": [142.0, 573.0, 153.0, 588.0]}, {"polygon": [[165.0, 573.0], [255.0, 573.0], [255.0, 589.0], [165.0, 589.0]], "confidence": 0.9222244620323181, "text": "Introduction", "bbox": [165.0, 573.0, 255.0, 589.0]}, {"polygon": [[141.0, 606.0], [673.0, 606.0], [673.0, 621.0], [141.0, 621.0]], "confidence": 0.990071713924408, "text": "Most language models are trained using tokenization, which partitions text into tokens that typically", "bbox": [141.0, 606.0, 673.0, 621.0]}, {"polygon": [[141.0, 620.0], [673.0, 620.0], [673.0, 635.0], [141.0, 635.0]], "confidence": 0.9888371229171753, "text": "consist of words or subwords. Tokenization is useful because it significantly decreases the inference", "bbox": [141.0, 620.0, 673.0, 635.0]}, {"polygon": [[142.0, 635.0], [673.0, 635.0], [673.0, 649.0], [142.0, 649.0]], "confidence": 0.9891467690467834, "text": "and training computational costs of large language models. However, tokenization also imposes", "bbox": [142.0, 635.0, 673.0, 649.0]}, {"polygon": [[142.0, 650.0], [673.0, 650.0], [673.0, 664.0], [142.0, 664.0]], "confidence": 0.9897804856300354, "text": "several disadvantages, including a performance penalty on text distributions different from what the", "bbox": [142.0, 650.0, 673.0, 664.0]}, {"polygon": [[142.0, 665.0], [673.0, 665.0], [673.0, 678.0], [142.0, 678.0]], "confidence": 0.9895065426826477, "text": "tokenizer was trained on [ 1 ]; increased vulnerability to adversarial attacks [ 2 ]; worse character-level", "bbox": [142.0, 665.0, 673.0, 678.0]}, {"polygon": [[142.0, 679.0], [469.0, 679.0], [469.0, 692.0], [142.0, 692.0]], "confidence": 0.9765956401824951, "text": "modeling performance [ 3 ], and additional model complexity. 1", "bbox": [142.0, 679.0, 469.0, 692.0]}, {"polygon": [[142.0, 700.0], [673.0, 700.0], [673.0, 715.0], [142.0, 715.0]], "confidence": 0.9812783002853394, "text": "Recently, MegaByte [ 4 ], MambaByte [ 5 ], and more [ 6 , 7 ] have been proposed as new byte-level", "bbox": [142.0, 700.0, 673.0, 715.0]}, {"polygon": [[142.0, 715.0], [673.0, 715.0], [673.0, 729.0], [142.0, 729.0]], "confidence": 0.9850634932518005, "text": "autoregressive language models that model bytes instead of tokens.  (See [ 8 – 14 ] for encoder", "bbox": [142.0, 715.0, 673.0, 729.0]}, {"polygon": [[141.0, 729.0], [674.0, 729.0], [674.0, 744.0], [141.0, 744.0]], "confidence": 0.9834652543067932, "text": "and encoder-decoder byte-level modeling.) To address the longer context size resulting from", "bbox": [141.0, 729.0, 674.0, 744.0]}, {"polygon": [[141.0, 744.0], [673.0, 744.0], [673.0, 758.0], [141.0, 758.0]], "confidence": 0.9897156357765198, "text": "modeling bytes instead of tokens, MegaByte uses multiscale modeling (which is more challenging for", "bbox": [141.0, 744.0, 673.0, 758.0]}, {"polygon": [[142.0, 758.0], [673.0, 758.0], [673.0, 773.0], [142.0, 773.0]], "confidence": 0.9880436062812805, "text": "autoregressive models than Transformer encoders [ 15 , 16 ]), while MambaByte uses Mamba blocks", "bbox": [142.0, 758.0, 673.0, 773.0]}, {"polygon": [[141.0, 774.0], [673.0, 774.0], [673.0, 787.0], [141.0, 787.0]], "confidence": 0.9876987338066101, "text": "[ 17 ] instead of Transformer blocks. But although MegaByte and MambaByte have been shown to", "bbox": [141.0, 774.0, 673.0, 787.0]}, {"polygon": [[142.0, 788.0], [673.0, 788.0], [673.0, 802.0], [142.0, 802.0]], "confidence": 0.9882813096046448, "text": "perform better than a standard byte-level Transformer, to our knowledge, no byte-level autoregressive", "bbox": [142.0, 788.0, 673.0, 802.0]}, {"polygon": [[142.0, 802.0], [673.0, 802.0], [673.0, 816.0], [142.0, 816.0]], "confidence": 0.989233136177063, "text": "large language model architecture has been shown to match the performance of tokenized models", "bbox": [142.0, 802.0, 673.0, 816.0]}, {"polygon": [[141.0, 817.0], [335.0, 817.0], [335.0, 832.0], [141.0, 832.0]], "confidence": 0.9708129167556763, "text": "when controlling for compute costs.", "bbox": [141.0, 817.0, 335.0, 832.0]}, {"polygon": [[142.0, 838.0], [673.0, 838.0], [673.0, 853.0], [142.0, 853.0]], "confidence": 0.9879453182220459, "text": "In this work, we study the performance of byte-level and subword-level autoregressive models", "bbox": [142.0, 838.0, 673.0, 853.0]}, {"polygon": [[141.0, 853.0], [673.0, 853.0], [673.0, 867.0], [141.0, 867.0]], "confidence": 0.9865814447402954, "text": "when trained using a fixed compute budget. We measure the performance in terms of the cross", "bbox": [141.0, 853.0, 673.0, 867.0]}, {"polygon": [[141.0, 868.0], [673.0, 868.0], [673.0, 882.0], [141.0, 882.0]], "confidence": 0.9885828495025635, "text": "entropy (measured in bits-per-byte), which has been shown to be a strong predictor of down-stream", "bbox": [141.0, 868.0, 673.0, 882.0]}, {"polygon": [[142.0, 883.0], [673.0, 883.0], [673.0, 896.0], [142.0, 896.0]], "confidence": 0.9850029945373535, "text": "performance [ 18 ]. In addition to controlling for training compute, we also control for inference", "bbox": [142.0, 883.0, 673.0, 896.0]}, {"polygon": [[141.0, 897.0], [673.0, 897.0], [673.0, 911.0], [141.0, 911.0]], "confidence": 0.9850264191627502, "text": "compute costs (measured in FLOPs). We find that byte-level Transformer and MegaByte models", "bbox": [141.0, 897.0, 673.0, 911.0]}, {"polygon": [[163.0, 924.0], [673.0, 922.0], [673.0, 936.0], [163.0, 937.0]], "confidence": 0.982814610004425, "text": "'See also Andrej Karpathy's tweet twitter.com/karpathy/status/1657949234535211009 and video", "bbox": [163.0, 924.0, 673.0, 936.0]}, {"polygon": [[142.0, 937.0], [550.0, 937.0], [550.0, 951.0], [142.0, 951.0]], "confidence": 0.9812173843383789, "text": "youtube.com/watch?v=zduSFxRajkE&t=6725 s on the disadvantages of tokenization.", "bbox": [142.0, 937.0, 550.0, 951.0]}, {"polygon": [[142.0, 975.0], [256.0, 975.0], [256.0, 989.0], [142.0, 989.0]], "confidence": 0.9575119614601135, "text": "Preprint. Under review.", "bbox": [142.0, 975.0, 256.0, 989.0]}], "languages": ["en"], "image_bbox": [0.0, 0.0, 816.0, 1056.0], "page": 1}, {"text_lines": [{"polygon": [[494.0, 121.0], [563.0, 121.0], [563.0, 132.0], [494.0, 132.0]], "confidence": 0.8972260355949402, "text": "de-embedding", "bbox": [494.0, 121.0, 563.0, 132.0]}, {"polygon": [[514.0, 145.0], [639.0, 145.0], [639.0, 156.0], [514.0, 156.0]], "confidence": 0.9567185044288635, "text": "× local transformer blocks", "bbox": [514.0, 145.0, 639.0, 156.0]}, {"polygon": [[511.0, 168.0], [645.0, 168.0], [645.0, 180.0], [511.0, 180.0]], "confidence": 0.9315738677978516, "text": "α × global transformer blocks", "bbox": [511.0, 168.0, 645.0, 180.0]}, {"polygon": [[515.0, 190.0], [639.0, 190.0], [639.0, 203.0], [515.0, 203.0]], "confidence": 0.9574234485626221, "text": "× local transformer blocks", "bbox": [515.0, 190.0, 639.0, 203.0]}, {"polygon": [[493.0, 215.0], [548.0, 215.0], [548.0, 226.0], [493.0, 226.0]], "confidence": 0.8980180621147156, "text": "embedding", "bbox": [493.0, 215.0, 548.0, 226.0]}, {"polygon": [[141.0, 264.0], [674.0, 264.0], [674.0, 278.0], [141.0, 278.0]], "confidence": 0.9873188734054565, "text": "Figure 1: An overview of the SpaceByte architecture. The embedding, local transformer blocks, and", "bbox": [141.0, 264.0, 674.0, 278.0]}, {"polygon": [[141.0, 278.0], [673.0, 278.0], [673.0, 293.0], [141.0, 293.0]], "confidence": 0.988490641117096, "text": "de-embedding (i.e. a layer norm and linear) are the standard Transformer decoder layers. SpaceByte", "bbox": [141.0, 278.0, 673.0, 293.0]}, {"polygon": [[141.0, 293.0], [675.0, 293.0], [675.0, 308.0], [141.0, 308.0]], "confidence": 0.9884281754493713, "text": "modifies the standard transformer by applying \"global\" transformer blocks only after certain bytes,", "bbox": [141.0, 293.0, 675.0, 308.0]}, {"polygon": [[142.0, 307.0], [674.0, 307.0], [674.0, 322.0], [142.0, 322.0]], "confidence": 0.9891901612281799, "text": "such as space characters. The intuition is that the first character of a word is typically the hardest", "bbox": [142.0, 307.0, 674.0, 322.0]}, {"polygon": [[141.0, 322.0], [674.0, 322.0], [674.0, 337.0], [141.0, 337.0]], "confidence": 0.9899630546569824, "text": "to predict; thus this positioning of the global blocks should make the best use of the global blocks", "bbox": [141.0, 322.0, 674.0, 337.0]}, {"polygon": [[140.0, 337.0], [349.0, 337.0], [349.0, 352.0], [140.0, 352.0]], "confidence": 0.973125159740448, "text": "(which use a larger model dimension).", "bbox": [140.0, 337.0, 349.0, 352.0]}, {"polygon": [[142.0, 377.0], [675.0, 377.0], [675.0, 392.0], [142.0, 392.0]], "confidence": 0.9880476593971252, "text": "can require roughly 10 times more training FLOPs to achieve the same performance as a subword-", "bbox": [142.0, 377.0, 675.0, 392.0]}, {"polygon": [[142.0, 392.0], [673.0, 392.0], [673.0, 407.0], [142.0, 407.0]], "confidence": 0.9888841509819031, "text": "level Transformer. To close this substantial performance gap, we propose a new byte-level decoder", "bbox": [142.0, 392.0, 673.0, 407.0]}, {"polygon": [[142.0, 408.0], [274.0, 408.0], [274.0, 421.0], [142.0, 421.0]], "confidence": 0.9590107798576355, "text": "architecture: SpaceByte.", "bbox": [142.0, 408.0, 274.0, 421.0]}, {"polygon": [[142.0, 429.0], [674.0, 429.0], [674.0, 443.0], [142.0, 443.0]], "confidence": 0.9896182417869568, "text": "SpaceByte also utilizes multiscale modeling to improve efficiency by grouping bytes into patches. But", "bbox": [142.0, 429.0, 674.0, 443.0]}, {"polygon": [[141.0, 443.0], [674.0, 443.0], [674.0, 458.0], [141.0, 458.0]], "confidence": 0.9854521751403809, "text": "ulike MegaByte, which uses a fixed patch size, SpaceByte uses a simple rule to dynamically partition", "bbox": [141.0, 443.0, 674.0, 458.0]}, {"polygon": [[141.0, 458.0], [673.0, 458.0], [673.0, 473.0], [141.0, 473.0]], "confidence": 0.9895225763320923, "text": "the bytes into patches that are aligned with word and other language boundaries. (A similar technique", "bbox": [141.0, 458.0, 673.0, 473.0]}, {"polygon": [[141.0, 472.0], [675.0, 472.0], [675.0, 487.0], [141.0, 487.0]], "confidence": 0.9805341362953186, "text": "was also explored by Thawani et al. [6].) Our experiments show that this simple modification is", "bbox": [141.0, 472.0, 675.0, 487.0]}, {"polygon": [[141.0, 487.0], [673.0, 487.0], [673.0, 502.0], [141.0, 502.0]], "confidence": 0.9885556697845459, "text": "crucial for performance, allowing SpaceByte to outperform other byte-level architectures and roughly", "bbox": [141.0, 487.0, 673.0, 502.0]}, {"polygon": [[142.0, 502.0], [589.0, 502.0], [589.0, 516.0], [142.0, 516.0]], "confidence": 0.9877330660820007, "text": "match the performance of subword Transformers across a variety of text modalities.", "bbox": [142.0, 502.0, 589.0, 516.0]}, {"polygon": [[141.0, 523.0], [673.0, 523.0], [673.0, 537.0], [141.0, 537.0]], "confidence": 0.9890452027320862, "text": "Our experiments are performed on datasets consisting of English books, LaTeX formatted arXiv", "bbox": [141.0, 523.0, 673.0, 537.0]}, {"polygon": [[142.0, 538.0], [673.0, 538.0], [673.0, 552.0], [142.0, 552.0]], "confidence": 0.9883511066436768, "text": "pers, and open-source code. For other data modalities, SpaceByte with our simple patching rule", "bbox": [142.0, 538.0, 673.0, 552.0]}, {"polygon": [[141.0, 553.0], [278.0, 553.0], [278.0, 567.0], [141.0, 567.0]], "confidence": 0.9617839455604553, "text": "might not be as effective.", "bbox": [141.0, 553.0, 278.0, 567.0]}, {"polygon": [[142.0, 588.0], [154.0, 588.0], [154.0, 604.0], [142.0, 604.0]], "confidence": 0.48844102025032043, "text": "2", "bbox": [142.0, 588.0, 154.0, 604.0]}, {"polygon": [[163.0, 588.0], [240.0, 588.0], [240.0, 604.0], [163.0, 604.0]], "confidence": 0.8972557783126831, "text": "SpaceByte", "bbox": [163.0, 588.0, 240.0, 604.0]}, {"polygon": [[141.0, 621.0], [674.0, 621.0], [674.0, 636.0], [141.0, 636.0]], "confidence": 0.9834123849868774, "text": "The SpaceByte architecture is summarized in Figure 1 . In a nutshell, SpaceByte can be thought", "bbox": [141.0, 621.0, 674.0, 636.0]}, {"polygon": [[141.0, 635.0], [673.0, 635.0], [673.0, 650.0], [141.0, 650.0]], "confidence": 0.9886699318885803, "text": "of as a byte-level Transformer model, but with extra \"global\" transformer blocks (with a larger", "bbox": [141.0, 635.0, 673.0, 650.0]}, {"polygon": [[142.0, 650.0], [673.0, 650.0], [673.0, 664.0], [142.0, 664.0]], "confidence": 0.989601731300354, "text": "model dimension) inserted in the middle, which are only applied a fraction of the time. While the", "bbox": [142.0, 650.0, 673.0, 664.0]}, {"polygon": [[142.0, 665.0], [673.0, 665.0], [673.0, 678.0], [142.0, 678.0]], "confidence": 0.9832167029380798, "text": "MegaByte architecture applies the global transformer blocks every P ~ 8 bytes, we hypothesize that", "bbox": [142.0, 665.0, 673.0, 678.0]}, {"polygon": [[141.0, 679.0], [673.0, 679.0], [673.0, 693.0], [141.0, 693.0]], "confidence": 0.9874237179756165, "text": "this fixed spacing hinders performance. Our intuition is that the first character of a word is typically", "bbox": [141.0, 679.0, 673.0, 693.0]}, {"polygon": [[141.0, 693.0], [673.0, 693.0], [673.0, 707.0], [141.0, 707.0]], "confidence": 0.9868919253349304, "text": "significantly harder to predict than the following characters. We therefore expect that performance", "bbox": [141.0, 693.0, 673.0, 707.0]}, {"polygon": [[141.0, 708.0], [554.0, 708.0], [554.0, 723.0], [141.0, 723.0]], "confidence": 0.9866076111793518, "text": "can be improved by applying the global blocks primarily at word boundaries.", "bbox": [141.0, 708.0, 554.0, 723.0]}, {"polygon": [[142.0, 728.0], [674.0, 730.0], [674.0, 746.0], [142.0, 744.0]], "confidence": 0.9891278743743896, "text": "Global Block Insertion Rule In this work, we consider a very simple rule to dynamically decide", "bbox": [142.0, 728.0, 674.0, 746.0]}, {"polygon": [[140.0, 744.0], [674.0, 744.0], [674.0, 758.0], [140.0, 758.0]], "confidence": 0.9893272519111633, "text": "when to apply the global blocks. We assume that the text bytes are encoded using the UTF-8 encoding.", "bbox": [140.0, 744.0, 674.0, 758.0]}, {"polygon": [[142.0, 759.0], [674.0, 759.0], [674.0, 773.0], [142.0, 773.0]], "confidence": 0.9886921048164368, "text": "We define a byte to be spacelike if the byte does not encode a letter, number, or UTF-8 continuation", "bbox": [142.0, 759.0, 674.0, 773.0]}, {"polygon": [[141.0, 774.0], [674.0, 774.0], [674.0, 787.0], [141.0, 787.0]], "confidence": 0.9887934327125549, "text": "byte 2 . We apply the global blocks after any spacelike byte that is not preceded by another spacelike", "bbox": [141.0, 774.0, 674.0, 787.0]}, {"polygon": [[141.0, 788.0], [461.0, 788.0], [461.0, 802.0], [141.0, 802.0]], "confidence": 0.9793958067893982, "text": "byte (and after any BOS token). See Figure 2 for examples.", "bbox": [141.0, 788.0, 461.0, 802.0]}, {"polygon": [[142.0, 809.0], [674.0, 809.0], [674.0, 825.0], [142.0, 825.0]], "confidence": 0.9893700480461121, "text": "The most common spacelike byte is the space character. Thus, the global blocks are applied most", "bbox": [142.0, 809.0, 674.0, 825.0]}, {"polygon": [[142.0, 824.0], [674.0, 824.0], [674.0, 839.0], [142.0, 839.0]], "confidence": 0.9881097674369812, "text": "frequently to predict the first character of a word, which we expect is the hardest character to predict", "bbox": [142.0, 824.0, 674.0, 839.0]}, {"polygon": [[142.0, 839.0], [675.0, 839.0], [675.0, 853.0], [142.0, 853.0]], "confidence": 0.9886379241943359, "text": "in a given word. With fixed patch size (e.g. as in MegaByte), the global blocks are typically inserted", "bbox": [142.0, 839.0, 675.0, 853.0]}, {"polygon": [[142.0, 853.0], [674.0, 853.0], [674.0, 868.0], [142.0, 868.0]], "confidence": 0.9894416332244873, "text": "in the middle a word, which we expect is inefficient because predicting the rest of the word could", "bbox": [142.0, 853.0, 674.0, 868.0]}, {"polygon": [[142.0, 868.0], [673.0, 868.0], [673.0, 882.0], [142.0, 882.0]], "confidence": 0.9886113405227661, "text": "likely be more efficiently accomplished using the local blocks. We define continuation bytes to be", "bbox": [142.0, 868.0, 673.0, 882.0]}, {"polygon": [[142.0, 883.0], [673.0, 883.0], [673.0, 896.0], [142.0, 896.0]], "confidence": 0.9880850911140442, "text": "spacelike so that languages that do not use spaces between words can still benefit from the global", "bbox": [142.0, 883.0, 673.0, 896.0]}, {"polygon": [[142.0, 897.0], [656.0, 897.0], [656.0, 910.0], [142.0, 910.0]], "confidence": 0.986707866191864, "text": "blocks between multi-byte characters (e.g. Chinese characters consists of three bytes in UTF-8).", "bbox": [142.0, 897.0, 656.0, 910.0]}, {"polygon": [[163.0, 921.0], [673.0, 921.0], [673.0, 935.0], [163.0, 935.0]], "confidence": 0.9892938733100891, "text": "2 UTF-8 uses a variable number of bytes to encode a character. English letters or numbers consist of a single", "bbox": [163.0, 921.0, 673.0, 935.0]}, {"polygon": [[142.0, 936.0], [673.0, 936.0], [673.0, 949.0], [142.0, 949.0]], "confidence": 0.9905767440795898, "text": "byte. Characters that are encoded using multiple bytes are encoded using a leading byte (which is spacelike by", "bbox": [142.0, 936.0, 673.0, 949.0]}, {"polygon": [[142.0, 951.0], [550.0, 951.0], [550.0, 963.0], [142.0, 963.0]], "confidence": 0.9876174330711365, "text": "our definition) followed by one or more continuation bytes (which are not spacelike).", "bbox": [142.0, 951.0, 550.0, 963.0]}], "languages": ["en"], "image_bbox": [0.0, 0.0, 816.0, 1056.0], "page": 2}, {"text_lines": [{"polygon": [[141.0, 92.0], [183.0, 92.0], [183.0, 107.0], [141.0, 107.0]], "confidence": 0.8302968740463257, "text": "PG-19:", "bbox": [141.0, 92.0, 183.0, 107.0]}, {"polygon": [[142.0, 106.0], [660.0, 106.0], [660.0, 122.0], [142.0, 122.0]], "confidence": 0.0, "text": "", "bbox": [142.0, 106.0, 660.0, 122.0]}, {"polygon": [[142.0, 122.0], [658.0, 122.0], [658.0, 137.0], [142.0, 137.0]], "confidence": 0.6725046038627625, "text": "thelenemy!", "bbox": [142.0, 122.0, 658.0, 137.0]}, {"polygon": [[142.0, 152.0], [180.0, 152.0], [180.0, 166.0], [142.0, 166.0]], "confidence": 0.8224871754646301, "text": "arXiv:", "bbox": [142.0, 152.0, 180.0, 166.0]}, {"polygon": [[142.0, 166.0], [653.0, 166.0], [653.0, 182.0], [142.0, 182.0]], "confidence": 0.0, "text": "", "bbox": [142.0, 166.0, 653.0, 182.0]}, {"polygon": [[140.0, 183.0], [654.0, 183.0], [654.0, 198.0], [140.0, 198.0]], "confidence": 0.9727672934532166, "text": "where $q_1=q_2=\\dots=q_\\kappa$_and $V_1=V_2=\\dots_V_\\kappa$. In_this_way,", "bbox": [140.0, 183.0, 654.0, 198.0]}, {"polygon": [[141.0, 212.0], [185.0, 212.0], [185.0, 226.0], [141.0, 226.0]], "confidence": 0.8691551685333252, "text": "Github:", "bbox": [141.0, 212.0, 185.0, 226.0]}, {"polygon": [[142.0, 226.0], [669.0, 226.0], [669.0, 241.0], [142.0, 241.0]], "confidence": 0.0, "text": "", "bbox": [142.0, 226.0, 669.0, 241.0]}, {"polygon": [[168.0, 242.0], [253.0, 242.0], [253.0, 256.0], [168.0, 256.0]], "confidence": 0.8100667595863342, "text": "exp_+=_2;++", "bbox": [168.0, 242.0, 253.0, 256.0]}, {"polygon": [[272.0, 242.0], [382.0, 242.0], [382.0, 256.0], [272.0, 256.0]], "confidence": 0.8639336228370667, "text": "mbf[3] =_exp;↓ ", "bbox": [272.0, 242.0, 382.0, 256.0]}, {"polygon": [[397.0, 242.0], [641.0, 242.0], [641.0, 257.0], [397.0, 257.0]], "confidence": 0.940641462802887, "text": "mbf[2] = sign | (ieee[2] & 0x7f);↓", "bbox": [397.0, 242.0, 641.0, 257.0]}, {"polygon": [[141.0, 265.0], [675.0, 265.0], [675.0, 280.0], [141.0, 280.0]], "confidence": 0.9865225553512573, "text": "Figure 2: Examples of patch boundaries from datasets that we study. Spacelike bytes are underlined", "bbox": [141.0, 265.0, 675.0, 280.0]}, {"polygon": [[142.0, 280.0], [674.0, 280.0], [674.0, 294.0], [142.0, 294.0]], "confidence": 0.9892745018005371, "text": "and colored blue. Patches boundaries are drawn above the text. Each patch ends after a spacelike", "bbox": [142.0, 280.0, 674.0, 294.0]}, {"polygon": [[141.0, 295.0], [674.0, 295.0], [674.0, 308.0], [141.0, 308.0]], "confidence": 0.9893143177032471, "text": "byte that is not preceded by another spacelike byte. Consequently, each patch begins with zero or", "bbox": [141.0, 295.0, 674.0, 308.0]}, {"polygon": [[141.0, 309.0], [674.0, 309.0], [674.0, 322.0], [141.0, 322.0]], "confidence": 0.9896695613861084, "text": "more spacelike bytes, followed by one or more non-spacelike bytes, and ends with a single spacelike", "bbox": [141.0, 309.0, 674.0, 322.0]}, {"polygon": [[141.0, 323.0], [675.0, 323.0], [675.0, 338.0], [141.0, 338.0]], "confidence": 0.9826891422271729, "text": "byte. The global blocks predict the first character of each patch. The downward arrow (↓) denotes a", "bbox": [141.0, 323.0, 675.0, 338.0]}, {"polygon": [[141.0, 338.0], [675.0, 338.0], [675.0, 354.0], [141.0, 354.0]], "confidence": 0.9854704737663269, "text": "newline byte. The left and right quotation characters, (\") and (\") in the PG-19 example, are encoded", "bbox": [141.0, 338.0, 675.0, 354.0]}, {"polygon": [[142.0, 353.0], [674.0, 353.0], [674.0, 367.0], [142.0, 367.0]], "confidence": 0.9886829853057861, "text": "using three bytes in UTF-8. The first of the three bytes is spacelike, while the later two bytes are", "bbox": [142.0, 353.0, 674.0, 367.0]}, {"polygon": [[142.0, 368.0], [675.0, 368.0], [675.0, 382.0], [142.0, 382.0]], "confidence": 0.987036943435669, "text": "UTF-8 continuation bytes, which are not spacelike and are each denoted using a bullet point (•) above.", "bbox": [142.0, 368.0, 675.0, 382.0]}, {"polygon": [[142.0, 427.0], [674.0, 427.0], [674.0, 444.0], [142.0, 444.0]], "confidence": 0.988288402557373, "text": "Although this very simple \"spacelike\" rule is likely not the optimal rule, we find that it works", "bbox": [142.0, 427.0, 674.0, 444.0]}, {"polygon": [[142.0, 443.0], [675.0, 443.0], [675.0, 459.0], [142.0, 459.0]], "confidence": 0.9895360469818115, "text": "surprisingly well in practice for English text, LaTeX formatted papers, and code. Nevertheless, a", "bbox": [142.0, 443.0, 675.0, 459.0]}, {"polygon": [[141.0, 459.0], [635.0, 459.0], [635.0, 473.0], [141.0, 473.0]], "confidence": 0.9897189140319824, "text": "critical future direction is to optimize better rules using data rather than our simple heuristic.", "bbox": [141.0, 459.0, 635.0, 473.0]}, {"polygon": [[142.0, 479.0], [675.0, 479.0], [675.0, 494.0], [142.0, 494.0]], "confidence": 0.990057647228241, "text": "Important Details Since the global blocks are not applied as often as the local transformer blocks, it", "bbox": [142.0, 479.0, 675.0, 494.0]}, {"polygon": [[142.0, 494.0], [674.0, 494.0], [674.0, 509.0], [142.0, 509.0]], "confidence": 0.9897602200508118, "text": "is advantageous to use a larger model dimension for the global transformer blocks. To increase the", "bbox": [142.0, 494.0, 674.0, 509.0]}, {"polygon": [[141.0, 510.0], [674.0, 510.0], [674.0, 523.0], [141.0, 523.0]], "confidence": 0.9882562756538391, "text": "dimensions of an activation vector before the global blocks, we simply pad the activation vector with", "bbox": [141.0, 510.0, 674.0, 523.0]}, {"polygon": [[142.0, 524.0], [502.0, 524.0], [502.0, 538.0], [142.0, 538.0]], "confidence": 0.9853008985519409, "text": "zeros. To decrease the dimension, we truncate the activation vector.", "bbox": [142.0, 524.0, 502.0, 538.0]}, {"polygon": [[142.0, 545.0], [674.0, 545.0], [674.0, 560.0], [142.0, 560.0]], "confidence": 0.9849352836608887, "text": "In our experiments, we use a significantly larger context size than the model dimension D local of the", "bbox": [142.0, 545.0, 674.0, 560.0]}, {"polygon": [[142.0, 560.0], [674.0, 560.0], [674.0, 574.0], [142.0, 574.0]], "confidence": 0.9897682070732117, "text": "local transformer blocks. To prevent the attention mechanism from dominating the compute costs for", "bbox": [142.0, 560.0, 674.0, 574.0]}, {"polygon": [[141.0, 573.0], [675.0, 575.0], [675.0, 591.0], [141.0, 589.0]], "confidence": 0.9819210171699524, "text": "the local model, we use an attention window [ 19 – 21 ] of length D local for the local transformer blocks.", "bbox": [141.0, 573.0, 675.0, 591.0]}, {"polygon": [[142.0, 594.0], [675.0, 597.0], [675.0, 613.0], [142.0, 610.0]], "confidence": 0.9879484176635742, "text": "See Appendix C for pseudocode. Additional details specific to our experiments are provided in", "bbox": [142.0, 594.0, 675.0, 613.0]}, {"polygon": [[142.0, 611.0], [346.0, 611.0], [346.0, 625.0], [142.0, 625.0]], "confidence": 0.9587879180908203, "text": "Sections 4.1 and 4.2 and Appendix A .", "bbox": [142.0, 611.0, 346.0, 625.0]}, {"polygon": [[163.0, 656.0], [263.0, 656.0], [263.0, 672.0], [163.0, 672.0]], "confidence": 0.9167842268943787, "text": "Related Work", "bbox": [163.0, 656.0, 263.0, 672.0]}, {"polygon": [[141.0, 657.0], [157.0, 657.0], [157.0, 671.0], [141.0, 671.0]], "confidence": 0.19170135259628296, "text": "3", "bbox": [141.0, 657.0, 157.0, 671.0]}, {"polygon": [[141.0, 693.0], [675.0, 693.0], [675.0, 707.0], [141.0, 707.0]], "confidence": 0.9889094233512878, "text": "The most straight-forward consequence of modeling bytes instead of subword tokens is that the length", "bbox": [141.0, 693.0, 675.0, 707.0]}, {"polygon": [[141.0, 708.0], [673.0, 706.0], [673.0, 721.0], [141.0, 723.0]], "confidence": 0.9899368286132812, "text": "of a sequence typically increases by about a factor of four. This increased sequence length increases", "bbox": [141.0, 708.0, 673.0, 721.0]}, {"polygon": [[141.0, 722.0], [674.0, 722.0], [674.0, 737.0], [141.0, 737.0]], "confidence": 0.9895709753036499, "text": "the training and inference compute cost for modeling a given long sequence of text for a Transformer", "bbox": [141.0, 722.0, 674.0, 737.0]}, {"polygon": [[141.0, 736.0], [359.0, 736.0], [359.0, 751.0], [141.0, 751.0]], "confidence": 0.9761539101600647, "text": "due to the quadratic scaling of attention.", "bbox": [141.0, 736.0, 359.0, 751.0]}, {"polygon": [[142.0, 758.0], [674.0, 758.0], [674.0, 773.0], [142.0, 773.0]], "confidence": 0.9893901348114014, "text": "MegaByte The MegaByte architecture strives to use multiscale Transformer modeling to lessen these", "bbox": [142.0, 758.0, 674.0, 773.0]}, {"polygon": [[142.0, 773.0], [675.0, 773.0], [675.0, 787.0], [142.0, 787.0]], "confidence": 0.988756537437439, "text": "performance issues. In particular, MegaByte groups bytes into patches of a fixed patch size P . Each", "bbox": [142.0, 773.0, 675.0, 787.0]}, {"polygon": [[142.0, 787.0], [674.0, 787.0], [674.0, 804.0], [142.0, 804.0]], "confidence": 0.9896973371505737, "text": "patch of bytes is vectorized and then fed into a \"global\" Transformer model. The output of the global", "bbox": [142.0, 787.0, 674.0, 804.0]}, {"polygon": [[142.0, 802.0], [674.0, 802.0], [674.0, 816.0], [142.0, 816.0]], "confidence": 0.9834907054901123, "text": "model is then fed into a \"local\" Transformer model that autoregressively outputs byte-level logits. [4]", "bbox": [142.0, 802.0, 674.0, 816.0]}, {"polygon": [[141.0, 823.0], [673.0, 823.0], [673.0, 839.0], [141.0, 839.0]], "confidence": 0.9887034893035889, "text": "For a context size of T bytes, MegaByte's global Transformer model compresses the context into only", "bbox": [141.0, 823.0, 673.0, 839.0]}, {"polygon": [[142.0, 838.0], [674.0, 838.0], [674.0, 853.0], [142.0, 853.0]], "confidence": 0.9821351766586304, "text": "T / P patches, which can significantly decrease the compute cost for modeling long sequences. Similar", "bbox": [142.0, 838.0, 674.0, 853.0]}, {"polygon": [[141.0, 853.0], [675.0, 853.0], [675.0, 868.0], [141.0, 868.0]], "confidence": 0.9811856150627136, "text": "to Yu et al. [ 4 ], we also find that MegaByte outperforms a standard byte-level Transformer. However,", "bbox": [141.0, 853.0, 675.0, 868.0]}, {"polygon": [[141.0, 868.0], [674.0, 868.0], [674.0, 882.0], [141.0, 882.0]], "confidence": 0.9847716689109802, "text": "we find that MegaByte's performance is remarkably close to a stronger byte-level Transformer", "bbox": [141.0, 868.0, 674.0, 882.0]}, {"polygon": [[141.0, 883.0], [674.0, 883.0], [674.0, 897.0], [141.0, 897.0]], "confidence": 0.9881958365440369, "text": "baseline that simply uses a sliding window attention mechanism [ 19 – 21 ] to increase the context size", "bbox": [141.0, 883.0, 674.0, 897.0]}, {"polygon": [[141.0, 897.0], [343.0, 897.0], [343.0, 911.0], [141.0, 911.0]], "confidence": 0.9733694195747375, "text": "without increasing the compute costs.", "bbox": [141.0, 897.0, 343.0, 911.0]}, {"polygon": [[142.0, 918.0], [675.0, 918.0], [675.0, 933.0], [142.0, 933.0]], "confidence": 0.9822573065757751, "text": "Yu et al. [ 4 ] do not compare MegaByte to subword-level Transformer in compute controlled", "bbox": [142.0, 918.0, 675.0, 933.0]}, {"polygon": [[141.0, 933.0], [673.0, 933.0], [673.0, 947.0], [141.0, 947.0]], "confidence": 0.98324054479599, "text": "experiments.  In our compute controlled experiments, we find that MegaByte's performance", "bbox": [141.0, 933.0, 673.0, 947.0]}, {"polygon": [[141.0, 948.0], [435.0, 948.0], [435.0, 962.0], [141.0, 962.0]], "confidence": 0.9744278788566589, "text": "significantly lags behind a subword-level Transformer.", "bbox": [141.0, 948.0, 435.0, 962.0]}, {"polygon": [[403.0, 989.0], [414.0, 989.0], [414.0, 1001.0], [403.0, 1001.0]], "confidence": 0.36552104353904724, "text": "3", "bbox": [403.0, 989.0, 414.0, 1001.0]}], "languages": ["en"], "image_bbox": [0.0, 0.0, 816.0, 1056.0], "page": 3}, {"text_lines": [{"polygon": [[141.0, 96.0], [674.0, 96.0], [674.0, 111.0], [141.0, 111.0]], "confidence": 0.9895609021186829, "text": "Compared to MegaByte, SpaceByte makes the crucial change that patches are dynamically sized to be", "bbox": [141.0, 96.0, 674.0, 111.0]}, {"polygon": [[141.0, 112.0], [674.0, 112.0], [674.0, 125.0], [141.0, 125.0]], "confidence": 0.9897844195365906, "text": "commensurate with the text, e.g. with word boundaries. We also add an additional local model before", "bbox": [141.0, 112.0, 674.0, 125.0]}, {"polygon": [[141.0, 126.0], [674.0, 126.0], [674.0, 139.0], [141.0, 139.0]], "confidence": 0.9895771741867065, "text": "the global model (while MegaByte only utilizes a single local model after the global model) to help", "bbox": [141.0, 126.0, 674.0, 139.0]}, {"polygon": [[142.0, 140.0], [674.0, 140.0], [674.0, 154.0], [142.0, 154.0]], "confidence": 0.9866209030151367, "text": "the model deal with the dynamical patch sizes. We also use significantly longer attention windows", "bbox": [142.0, 140.0, 674.0, 154.0]}, {"polygon": [[142.0, 155.0], [674.0, 155.0], [674.0, 169.0], [142.0, 169.0]], "confidence": 0.9880099892616272, "text": "for our local models. We find that these changes allow SpaceByte to significantly improve upon the", "bbox": [142.0, 155.0, 674.0, 169.0]}, {"polygon": [[142.0, 169.0], [648.0, 169.0], [648.0, 184.0], [142.0, 184.0]], "confidence": 0.9875073432922363, "text": "performance of MegaByte and roughly match the performance of subword-level Transformers.", "bbox": [142.0, 169.0, 648.0, 184.0]}, {"polygon": [[142.0, 191.0], [673.0, 191.0], [673.0, 206.0], [142.0, 206.0]], "confidence": 0.9802519679069519, "text": "MambaByte The MambaByte architecture [ 5 ] takes an alternative approach to avoiding the quadratic", "bbox": [142.0, 191.0, 673.0, 206.0]}, {"polygon": [[141.0, 206.0], [674.0, 206.0], [674.0, 220.0], [141.0, 220.0]], "confidence": 0.9895509481430054, "text": "compute scaling of the attention mechanism by replacing the Transformer block with a Mamba block", "bbox": [141.0, 206.0, 674.0, 220.0]}, {"polygon": [[141.0, 221.0], [673.0, 221.0], [673.0, 234.0], [141.0, 234.0]], "confidence": 0.984556257724762, "text": "[ 17 ]. Wang et al. [ 5 ] find that their byte-level MambaByte models outperform byte-level Transformer", "bbox": [141.0, 221.0, 673.0, 234.0]}, {"polygon": [[141.0, 235.0], [675.0, 235.0], [675.0, 249.0], [141.0, 249.0]], "confidence": 0.9886062741279602, "text": "and byte-level MegaByte models. They perform one controlled experiment with a subword model,", "bbox": [141.0, 235.0, 675.0, 249.0]}, {"polygon": [[141.0, 249.0], [674.0, 249.0], [674.0, 264.0], [141.0, 264.0]], "confidence": 0.9883994460105896, "text": "where they find that MambaByte slightly outperforms Mamba (using tokens) when controlling for the", "bbox": [141.0, 249.0, 674.0, 264.0]}, {"polygon": [[142.0, 264.0], [675.0, 264.0], [675.0, 279.0], [142.0, 279.0]], "confidence": 0.9887852668762207, "text": "amount of model parameters and training data (which was 14 epochs of the PG-19 dataset). But this", "bbox": [142.0, 264.0, 675.0, 279.0]}, {"polygon": [[140.0, 278.0], [674.0, 278.0], [674.0, 293.0], [140.0, 293.0]], "confidence": 0.989290177822113, "text": "experiment was not controlled for compute as MambaByte was trained using roughly four times as", "bbox": [140.0, 278.0, 674.0, 293.0]}, {"polygon": [[141.0, 293.0], [673.0, 293.0], [673.0, 307.0], [141.0, 307.0]], "confidence": 0.9883729815483093, "text": "much compute than Mamba. We view the Mamba and MambaByte architectures as complementary", "bbox": [141.0, 293.0, 673.0, 307.0]}, {"polygon": [[141.0, 308.0], [673.0, 308.0], [673.0, 322.0], [141.0, 322.0]], "confidence": 0.9890989065170288, "text": "to our work, as the Mamba block could be integrated into SpaceByte (or MegaByte) in place of", "bbox": [141.0, 308.0, 673.0, 322.0]}, {"polygon": [[142.0, 323.0], [251.0, 323.0], [251.0, 337.0], [142.0, 337.0]], "confidence": 0.9486986398696899, "text": "Transformer blocks.", "bbox": [142.0, 323.0, 251.0, 337.0]}, {"polygon": [[141.0, 344.0], [674.0, 344.0], [674.0, 359.0], [141.0, 359.0]], "confidence": 0.9884380102157593, "text": "Layer Skipping SpaceByte could be though of as a Transformer that employs a novel kind of", "bbox": [141.0, 344.0, 674.0, 359.0]}, {"polygon": [[141.0, 359.0], [463.0, 359.0], [463.0, 374.0], [141.0, 374.0]], "confidence": 0.9833350777626038, "text": "text-dependent layer skipping [ 22 – 28 ] on the middle layers.", "bbox": [141.0, 359.0, 463.0, 374.0]}, {"polygon": [[141.0, 381.0], [673.0, 381.0], [673.0, 396.0], [141.0, 396.0]], "confidence": 0.9888657331466675, "text": "Word Boundary Using word boundaries to partition patches in autoregressive language modeling", "bbox": [141.0, 381.0, 673.0, 396.0]}, {"polygon": [[141.0, 396.0], [675.0, 396.0], [675.0, 410.0], [141.0, 410.0]], "confidence": 0.9835067391395569, "text": "was explored by Thawani et al. [ 6 ] (and also Edman et al. [ 12 ] for encoder-decoder modeling).", "bbox": [141.0, 396.0, 675.0, 410.0]}, {"polygon": [[142.0, 410.0], [673.0, 410.0], [673.0, 425.0], [142.0, 425.0]], "confidence": 0.9885022044181824, "text": "However, their experiment methodology differed very significantly from ours. While we study large", "bbox": [142.0, 410.0, 673.0, 425.0]}, {"polygon": [[142.0, 425.0], [673.0, 425.0], [673.0, 439.0], [142.0, 439.0]], "confidence": 0.979610800743103, "text": "language model perplexity (via bits-per-byte) in compute-limited settings, Thawani et al. [ 6 ] study", "bbox": [142.0, 425.0, 673.0, 439.0]}, {"polygon": [[142.0, 440.0], [674.0, 440.0], [674.0, 453.0], [142.0, 453.0]], "confidence": 0.9763135313987732, "text": "models trained on small datasets ( ~ 5 MB) over 100 epochs and evaluated using word prediction", "bbox": [142.0, 440.0, 674.0, 453.0]}, {"polygon": [[142.0, 454.0], [673.0, 454.0], [673.0, 467.0], [142.0, 467.0]], "confidence": 0.9899141192436218, "text": "accuracies. The design of their local blocks also differ from SpaceByte as their local blocks do not", "bbox": [142.0, 454.0, 673.0, 467.0]}, {"polygon": [[142.0, 468.0], [673.0, 468.0], [673.0, 482.0], [142.0, 482.0]], "confidence": 0.989653468132019, "text": "attend across word boundaries and they use non-causal attention for the local blocks that preceed the", "bbox": [142.0, 468.0, 673.0, 482.0]}, {"polygon": [[142.0, 483.0], [219.0, 483.0], [219.0, 498.0], [142.0, 498.0]], "confidence": 0.930680513381958, "text": "global blocks.", "bbox": [142.0, 483.0, 219.0, 498.0]}, {"polygon": [[163.0, 520.0], [293.0, 520.0], [293.0, 536.0], [163.0, 536.0]], "confidence": 0.9401968717575073, "text": "Experiment Setup", "bbox": [163.0, 520.0, 293.0, 536.0]}, {"polygon": [[142.0, 522.0], [156.0, 522.0], [156.0, 535.0], [142.0, 535.0]], "confidence": 0.210004523396492, "text": "4", "bbox": [142.0, 522.0, 156.0, 535.0]}, {"polygon": [[141.0, 553.0], [675.0, 553.0], [675.0, 568.0], [141.0, 568.0]], "confidence": 0.9872804880142212, "text": "Our experiments compare the performance of our byte-level SpaceByte architecture to subword-", "bbox": [141.0, 553.0, 675.0, 568.0]}, {"polygon": [[142.0, 568.0], [673.0, 568.0], [673.0, 583.0], [142.0, 583.0]], "confidence": 0.9883092641830444, "text": "level Transformer and byte-level Transformer and MegaByte architectures. To fairly compare the", "bbox": [142.0, 568.0, 673.0, 583.0]}, {"polygon": [[142.0, 583.0], [673.0, 583.0], [673.0, 597.0], [142.0, 597.0]], "confidence": 0.9890725612640381, "text": "performance between the byte and subword level models, we measure the cross-entropy of the test", "bbox": [142.0, 583.0, 673.0, 597.0]}, {"polygon": [[141.0, 598.0], [674.0, 598.0], [674.0, 613.0], [141.0, 613.0]], "confidence": 0.9886173605918884, "text": "dataset in terms of bits-per-byte. 3 Given the substantial variation in inference compute costs across", "bbox": [141.0, 598.0, 674.0, 613.0]}, {"polygon": [[141.0, 612.0], [674.0, 612.0], [674.0, 626.0], [141.0, 626.0]], "confidence": 0.9889985918998718, "text": "the models we study, we also compare their inference compute costs to provide a more comprehensive", "bbox": [141.0, 612.0, 674.0, 626.0]}, {"polygon": [[141.0, 627.0], [674.0, 627.0], [674.0, 641.0], [141.0, 641.0]], "confidence": 0.9863373041152954, "text": "evaluation. We use FLOPs-per-byte as a simple software and hardware–independent proxy for", "bbox": [141.0, 627.0, 674.0, 641.0]}, {"polygon": [[142.0, 641.0], [674.0, 641.0], [674.0, 656.0], [142.0, 656.0]], "confidence": 0.9874922633171082, "text": "inference compute costs, which is the number of FLOPs (see Appendix A.1 ) required to model a byte", "bbox": [142.0, 641.0, 674.0, 656.0]}, {"polygon": [[141.0, 656.0], [186.0, 656.0], [186.0, 670.0], [141.0, 670.0]], "confidence": 0.9056621789932251, "text": "of text. 4", "bbox": [141.0, 656.0, 186.0, 670.0]}, {"polygon": [[142.0, 677.0], [673.0, 677.0], [673.0, 692.0], [142.0, 692.0]], "confidence": 0.9887703657150269, "text": "Note that by controlling for both total training compute and FLOPs-per-byte, we are also controlling", "bbox": [142.0, 677.0, 673.0, 692.0]}, {"polygon": [[142.0, 692.0], [675.0, 692.0], [675.0, 706.0], [142.0, 706.0]], "confidence": 0.9846006035804749, "text": "for the amount of training data since (bytes trained) = (training FLOPs)/(training FLOPs-per-byte).", "bbox": [142.0, 692.0, 675.0, 706.0]}, {"polygon": [[141.0, 707.0], [674.0, 707.0], [674.0, 721.0], [141.0, 721.0]], "confidence": 0.9892412424087524, "text": "The FLOPs-per-byte during training is equal to three times the FLOPs-per-byte during inference (due", "bbox": [141.0, 707.0, 674.0, 721.0]}, {"polygon": [[142.0, 721.0], [352.0, 721.0], [352.0, 735.0], [142.0, 735.0]], "confidence": 0.9747122526168823, "text": "to the backwards pass during training).", "bbox": [142.0, 721.0, 352.0, 735.0]}, {"polygon": [[142.0, 742.0], [675.0, 742.0], [675.0, 757.0], [142.0, 757.0]], "confidence": 0.9884417057037354, "text": "We therefore study the Pareto frontier of lowest bits-per-byte and lowest FLOPs-per-byte. We train", "bbox": [142.0, 742.0, 675.0, 757.0]}, {"polygon": [[141.0, 756.0], [674.0, 756.0], [674.0, 772.0], [141.0, 772.0]], "confidence": 0.987993597984314, "text": "all models using a compute-controlled setup, using either 10 18 or 10 19 FLOPs. In order to effectively", "bbox": [141.0, 756.0, 674.0, 772.0]}, {"polygon": [[141.0, 772.0], [674.0, 769.0], [674.0, 786.0], [141.0, 788.0]], "confidence": 0.989829957485199, "text": "explore this Pareto frontier, we train models using a grid of different model dimensions and numbers", "bbox": [141.0, 772.0, 674.0, 786.0]}, {"polygon": [[141.0, 787.0], [354.0, 787.0], [354.0, 801.0], [141.0, 801.0]], "confidence": 0.9706522822380066, "text": "of layers, as specified in Appendix B.3 .", "bbox": [141.0, 787.0, 354.0, 801.0]}, {"polygon": [[142.0, 808.0], [674.0, 808.0], [674.0, 823.0], [142.0, 823.0]], "confidence": 0.9710288047790527, "text": "Datasets Following the MegaByte [ 4 ] and MambaByte [ 5 ] experiments, we benchmark our models", "bbox": [142.0, 808.0, 674.0, 823.0]}, {"polygon": [[141.0, 823.0], [675.0, 823.0], [675.0, 836.0], [141.0, 836.0]], "confidence": 0.9880750179290771, "text": "on a diverse range of long-form datasets: PG-19 (English-language books written before 1919) [ 31 ];", "bbox": [141.0, 823.0, 675.0, 836.0]}, {"polygon": [[142.0, 837.0], [675.0, 837.0], [675.0, 852.0], [142.0, 852.0]], "confidence": 0.9848245978355408, "text": "arXiv (papers from ArXiv written in LaTeX, extracted from the arXiv component of The Pile [ 32 ]);", "bbox": [142.0, 837.0, 675.0, 852.0]}, {"polygon": [[142.0, 852.0], [673.0, 852.0], [673.0, 866.0], [142.0, 866.0]], "confidence": 0.9854705929756165, "text": "and Github (open-source code repositories, extracted from the Github component of The Pile [ 32 ]).", "bbox": [142.0, 852.0, 673.0, 866.0]}, {"polygon": [[165.0, 880.0], [675.0, 880.0], [675.0, 895.0], [165.0, 895.0]], "confidence": 0.9511371850967407, "text": "a The bits-per-byte (BPB) is equal to BPB = XE N 10kens /( Nbytes In 2), i.e. the cross entropy per token (XE)", "bbox": [165.0, 880.0, 675.0, 895.0]}, {"polygon": [[141.0, 894.0], [673.0, 893.0], [673.0, 908.0], [141.0, 910.0]], "confidence": 0.9788721203804016, "text": "times the number of tokens per byte ( Niokens/Nbytes ) divided by In 2 (to convert nats to bits). Niokens and Nbytes", "bbox": [141.0, 894.0, 673.0, 908.0]}, {"polygon": [[142.0, 908.0], [673.0, 909.0], [673.0, 923.0], [142.0, 921.0]], "confidence": 0.9829649329185486, "text": "are the number of tokens and bytes in the dataset, respectively. For byte-level models, Nokens = Nbytes since", "bbox": [142.0, 908.0, 673.0, 923.0]}, {"polygon": [[142.0, 923.0], [303.0, 923.0], [303.0, 934.0], [142.0, 934.0]], "confidence": 0.9711079001426697, "text": "bytes are used in place of tokens.", "bbox": [142.0, 923.0, 303.0, 934.0]}, {"polygon": [[161.0, 935.0], [673.0, 935.0], [673.0, 949.0], [161.0, 949.0]], "confidence": 0.986307680606842, "text": "4We note that in order for memory bandwidth to not be a bottleneck during inference, the batch size must be", "bbox": [161.0, 935.0, 673.0, 949.0]}, {"polygon": [[142.0, 949.0], [494.0, 949.0], [494.0, 963.0], [142.0, 963.0]], "confidence": 0.9862276911735535, "text": "sufficiently large and e.g. grouped-query attention [ 29 , 30 ] must be used.", "bbox": [142.0, 949.0, 494.0, 963.0]}, {"polygon": [[402.0, 990.0], [414.0, 990.0], [414.0, 1001.0], [402.0, 1001.0]], "confidence": 0.2189652919769287, "text": "14", "bbox": [402.0, 990.0, 414.0, 1001.0]}], "languages": ["en"], "image_bbox": [0.0, 0.0, 816.0, 1056.0], "page": 4}, {"text_lines": [{"polygon": [[283.0, 96.0], [529.0, 96.0], [529.0, 111.0], [283.0, 111.0]], "confidence": 0.9734863638877869, "text": "Listing 1: Pytorch pseudocode for SpaceByte", "bbox": [283.0, 96.0, 529.0, 111.0]}, {"polygon": [[142.0, 117.0], [444.0, 117.0], [444.0, 129.0], [142.0, 129.0]], "confidence": 0.7140176892280579, "text": "dddyyy   f", "bbox": [142.0, 117.0, 444.0, 129.0]}, {"polygon": [[172.0, 131.0], [520.0, 131.0], [520.0, 143.0], [172.0, 143.0]], "confidence": 0.9752629995346069, "text": "B, T = tokens.shape # batch size, context size", "bbox": [172.0, 131.0, 520.0, 143.0]}, {"polygon": [[172.0, 142.0], [439.0, 142.0], [439.0, 156.0], [172.0, 156.0]], "confidence": 0.9600189924240112, "text": "T_global = self.global_context_size", "bbox": [172.0, 142.0, 439.0, 156.0]}, {"polygon": [[172.0, 156.0], [446.0, 156.0], [446.0, 169.0], [172.0, 169.0]], "confidence": 0.957567572593689, "text": "D_local = self.local_model_dimension", "bbox": [172.0, 156.0, 446.0, 169.0]}, {"polygon": [[172.0, 168.0], [408.0, 168.0], [408.0, 183.0], [172.0, 183.0]], "confidence": 0.9590803980827332, "text": "D = self.global_model_dimension", "bbox": [172.0, 168.0, 408.0, 183.0]}, {"polygon": [[172.0, 195.0], [263.0, 195.0], [263.0, 209.0], [172.0, 209.0]], "confidence": 0.8865635395050049, "text": "# embedding:", "bbox": [172.0, 195.0, 263.0, 209.0]}, {"polygon": [[172.0, 208.0], [415.0, 208.0], [415.0, 223.0], [172.0, 223.0]], "confidence": 0.9350532293319702, "text": "x = self .token_embedding(tokens)", "bbox": [172.0, 208.0, 415.0, 223.0]}, {"polygon": [[172.0, 223.0], [445.0, 223.0], [445.0, 235.0], [172.0, 235.0]], "confidence": 0.960904598236084, "text": "x = x + self.local_position_encoding", "bbox": [172.0, 223.0, 445.0, 235.0]}, {"polygon": [[172.0, 236.0], [436.0, 236.0], [436.0, 249.0], [172.0, 249.0]], "confidence": 0.9688520431518555, "text": "# initial local transformer blocks:", "bbox": [172.0, 236.0, 436.0, 249.0]}, {"polygon": [[172.0, 249.0], [422.0, 249.0], [422.0, 262.0], [172.0, 262.0]], "confidence": 0.9541330933570862, "text": "for block in self.initial_blocks:", "bbox": [172.0, 249.0, 422.0, 262.0]}, {"polygon": [[201.0, 262.0], [295.0, 262.0], [295.0, 276.0], [201.0, 276.0]], "confidence": 0.8763787150382996, "text": "x = block(x)", "bbox": [201.0, 262.0, 295.0, 276.0]}, {"polygon": [[172.0, 288.0], [400.0, 288.0], [400.0, 302.0], [172.0, 302.0]], "confidence": 0.9632362127304077, "text": "# global block insertion rule:", "bbox": [172.0, 288.0, 400.0, 302.0]}, {"polygon": [[172.0, 301.0], [664.0, 301.0], [664.0, 315.0], [172.0, 315.0]], "confidence": 0.9748207926750183, "text": "use_global = ( # not a letter, number, or UTF-8 continuation byte", "bbox": [172.0, 301.0, 664.0, 315.0]}, {"polygon": [[202.0, 315.0], [364.0, 315.0], [364.0, 329.0], [202.0, 329.0]], "confidence": 0.8859825730323792, "text": "(tokens < ord('0')) |", "bbox": [202.0, 315.0, 364.0, 329.0]}, {"polygon": [[202.0, 327.0], [543.0, 327.0], [543.0, 341.0], [202.0, 341.0]], "confidence": 0.9606501460075378, "text": "(ord('9') < tokens) & (tokens < ord('A'))) |", "bbox": [202.0, 327.0, 543.0, 341.0]}, {"polygon": [[202.0, 340.0], [544.0, 340.0], [544.0, 354.0], [202.0, 354.0]], "confidence": 0.9572538733482361, "text": "(ord('Z') < tokens) & (tokens < ord('a'))) |", "bbox": [202.0, 340.0, 544.0, 354.0]}, {"polygon": [[202.0, 354.0], [566.0, 355.0], [566.0, 369.0], [202.0, 368.0]], "confidence": 0.9566536545753479, "text": "(ord('z') < tokens) & (tokens < 0b1000_000)) |", "bbox": [202.0, 354.0, 566.0, 369.0]}, {"polygon": [[203.0, 368.0], [393.0, 368.0], [393.0, 381.0], [203.0, 381.0]], "confidence": 0.9421311616897583, "text": "(0b1100_0000 <= tokens) )", "bbox": [203.0, 368.0, 393.0, 381.0]}, {"polygon": [[172.0, 381.0], [618.0, 381.0], [618.0, 396.0], [172.0, 396.0]], "confidence": 0.9808796048164368, "text": "use_global[:, 1:] &= use_global[:, :-1].bitwise_not() # not", "bbox": [172.0, 381.0, 618.0, 396.0]}, {"polygon": [[199.0, 395.0], [458.0, 393.0], [458.0, 408.0], [199.0, 410.0]], "confidence": 0.9547462463378906, "text": "preceded by another spacelike byte", "bbox": [199.0, 395.0, 458.0, 408.0]}, {"polygon": [[172.0, 408.0], [664.0, 408.0], [664.0, 422.0], [172.0, 422.0]], "confidence": 0.973702609539032, "text": "use_global |= tokens == self.BOS_token # always use global blocks", "bbox": [172.0, 408.0, 664.0, 422.0]}, {"polygon": [[199.0, 420.0], [322.0, 420.0], [322.0, 434.0], [199.0, 434.0]], "confidence": 0.9379698634147644, "text": "after BOS tokens", "bbox": [199.0, 420.0, 322.0, 434.0]}, {"polygon": [[172.0, 448.0], [422.0, 448.0], [422.0, 462.0], [172.0, 462.0]], "confidence": 0.9678651094436646, "text": "# calculate global block indices:", "bbox": [172.0, 448.0, 422.0, 462.0]}, {"polygon": [[172.0, 461.0], [657.0, 461.0], [657.0, 476.0], [172.0, 476.0]], "confidence": 0.9807314276695251, "text": "num_global = torch.full((B,), -1) # number of global blocks used", "bbox": [172.0, 461.0, 657.0, 476.0]}, {"polygon": [[172.0, 474.0], [671.0, 474.0], [671.0, 488.0], [172.0, 488.0]], "confidence": 0.8811887502670288, "text": "ggghhhhoooowwwggghhhooowww ", "bbox": [172.0, 474.0, 671.0, 488.0]}, {"polygon": [[173.0, 488.0], [310.0, 488.0], [310.0, 501.0], [173.0, 501.0]], "confidence": 0.9186892509460449, "text": "for b in range(B):", "bbox": [173.0, 488.0, 310.0, 501.0]}, {"polygon": [[201.0, 500.0], [528.0, 500.0], [528.0, 515.0], [201.0, 515.0]], "confidence": 0.9745851755142212, "text": "idx, = use_global[b].nonzero(as_tuple=True)", "bbox": [201.0, 500.0, 528.0, 515.0]}, {"polygon": [[202.0, 515.0], [558.0, 515.0], [558.0, 528.0], [202.0, 528.0]], "confidence": 0.9647204279899597, "text": "if targets is not None and len(idx) > T_global:", "bbox": [202.0, 515.0, 558.0, 528.0]}, {"polygon": [[231.0, 527.0], [602.0, 527.0], [602.0, 541.0], [231.0, 541.0]], "confidence": 0.4997062683105469, "text": "#", "bbox": [231.0, 527.0, 602.0, 541.0]}, {"polygon": [[231.0, 540.0], [468.0, 540.0], [468.0, 555.0], [231.0, 555.0]], "confidence": 0.9642905592918396, "text": "targets[b, idx[T_global]:] = -1", "bbox": [231.0, 540.0, 468.0, 555.0]}, {"polygon": [[201.0, 554.0], [468.0, 554.0], [468.0, 568.0], [201.0, 568.0]], "confidence": 0.9689338207244873, "text": "num_global[b] = len(idx[:T_global])", "bbox": [201.0, 554.0, 468.0, 568.0]}, {"polygon": [[201.0, 567.0], [550.0, 567.0], [550.0, 581.0], [201.0, 581.0]], "confidence": 0.9657446146011353, "text": "global_idx[b, :num_global[b]] = idx[:T_global]", "bbox": [201.0, 567.0, 550.0, 581.0]}, {"polygon": [[172.0, 594.0], [467.0, 594.0], [467.0, 608.0], [172.0, 608.0]], "confidence": 0.9690865278244019, "text": "# select activations for global blocks:", "bbox": [172.0, 594.0, 467.0, 608.0]}, {"polygon": [[172.0, 606.0], [671.0, 606.0], [671.0, 622.0], [172.0, 622.0]], "confidence": 0.9800307750701904, "text": "y = x.gather(1, global_idx[:,:,None].expand(B, T_global, D_local))", "bbox": [172.0, 606.0, 671.0, 622.0]}, {"polygon": [[172.0, 621.0], [529.0, 621.0], [529.0, 635.0], [172.0, 635.0]], "confidence": 0.9764097332954407, "text": "# expand model dimension by padding with zeros:", "bbox": [172.0, 621.0, 529.0, 635.0]}, {"polygon": [[172.0, 632.0], [633.0, 634.0], [633.0, 650.0], [172.0, 648.0]], "confidence": 0.978893518447876, "text": "y = torch.cat([torch.zeros(B, T_global, D - D_local), y], -1)", "bbox": [172.0, 632.0, 633.0, 650.0]}, {"polygon": [[172.0, 660.0], [384.0, 660.0], [384.0, 674.0], [172.0, 674.0]], "confidence": 0.9620829224586487, "text": "# global transformer blocks:", "bbox": [172.0, 660.0, 384.0, 674.0]}, {"polygon": [[172.0, 673.0], [452.0, 673.0], [452.0, 688.0], [172.0, 688.0]], "confidence": 0.9615734219551086, "text": "y = y + self.global_position_encoding", "bbox": [172.0, 673.0, 452.0, 688.0]}, {"polygon": [[172.0, 687.0], [414.0, 687.0], [414.0, 701.0], [172.0, 701.0]], "confidence": 0.9626643657684326, "text": "for block in self.global_blocks:", "bbox": [172.0, 687.0, 414.0, 701.0]}, {"polygon": [[201.0, 700.0], [295.0, 700.0], [295.0, 713.0], [201.0, 713.0]], "confidence": 0.9087409377098083, "text": "y = block(y)", "bbox": [201.0, 700.0, 295.0, 713.0]}, {"polygon": [[172.0, 727.0], [528.0, 727.0], [528.0, 740.0], [172.0, 740.0]], "confidence": 0.9775040745735168, "text": "# add global block activations to local blocks:", "bbox": [172.0, 727.0, 528.0, 740.0]}, {"polygon": [[172.0, 740.0], [303.0, 740.0], [303.0, 753.0], [172.0, 753.0]], "confidence": 0.9167881011962891, "text": "x = torch.stack([", "bbox": [172.0, 740.0, 303.0, 753.0]}, {"polygon": [[201.0, 752.0], [633.0, 754.0], [633.0, 769.0], [201.0, 767.0]], "confidence": 0.9794090390205383, "text": "x[b].index_add(0, global_idx[b, :n], y[b, :n, -D_local:])", "bbox": [201.0, 752.0, 633.0, 769.0]}, {"polygon": [[201.0, 767.0], [475.0, 767.0], [475.0, 780.0], [201.0, 780.0]], "confidence": 0.965859591960907, "text": "for b, n in enumerate(num_global) ])", "bbox": [201.0, 767.0, 475.0, 780.0]}, {"polygon": [[172.0, 792.0], [422.0, 792.0], [422.0, 806.0], [172.0, 806.0]], "confidence": 0.9662094116210938, "text": "# final local transformer blocks:", "bbox": [172.0, 792.0, 422.0, 806.0]}, {"polygon": [[172.0, 806.0], [407.0, 806.0], [407.0, 820.0], [172.0, 820.0]], "confidence": 0.9522694945335388, "text": "for block in self.final_blocks:", "bbox": [172.0, 806.0, 407.0, 820.0]}, {"polygon": [[201.0, 820.0], [295.0, 820.0], [295.0, 833.0], [201.0, 833.0]], "confidence": 0.8772574663162231, "text": "x = block(x)", "bbox": [201.0, 820.0, 295.0, 833.0]}, {"polygon": [[172.0, 834.0], [285.0, 834.0], [285.0, 846.0], [172.0, 846.0]], "confidence": 0.9300330281257629, "text": "# de-embedding:", "bbox": [172.0, 834.0, 285.0, 846.0]}, {"polygon": [[172.0, 845.0], [528.0, 847.0], [528.0, 862.0], [172.0, 860.0]], "confidence": 0.9708499312400818, "text": "logits = self.logits_linear(self.layer_norm(x))", "bbox": [172.0, 845.0, 528.0, 862.0]}, {"polygon": [[172.0, 874.0], [363.0, 873.0], [363.0, 888.0], [172.0, 889.0]], "confidence": 0.8686609864234924, "text": "cc", "bbox": [172.0, 874.0, 363.0, 888.0]}, {"polygon": [[172.0, 887.0], [347.0, 887.0], [347.0, 900.0], [172.0, 900.0]], "confidence": 0.5631725788116455, "text": "iiiiippp    tttteeee", "bbox": [172.0, 887.0, 347.0, 900.0]}, {"polygon": [[201.0, 898.0], [619.0, 898.0], [619.0, 913.0], [201.0, 913.0]], "confidence": 0.9429188370704651, "text": "cc", "bbox": [201.0, 898.0, 619.0, 913.0]}, {"polygon": [[231.0, 911.0], [544.0, 911.0], [544.0, 926.0], [231.0, 926.0]], "confidence": 0.9684365391731262, "text": "logits.view(B*T, 256), targets.view(B*T),", "bbox": [231.0, 911.0, 544.0, 926.0]}, {"polygon": [[232.0, 927.0], [438.0, 925.0], [438.0, 938.0], [232.0, 940.0]], "confidence": 0.9503308534622192, "text": "ignore_index=-1).view(B, T)", "bbox": [232.0, 927.0, 438.0, 938.0]}, {"polygon": [[172.0, 939.0], [423.0, 939.0], [423.0, 953.0], [172.0, 953.0]], "confidence": 0.9683800935745239, "text": "return logits, cross_entropy_loss", "bbox": [172.0, 939.0, 423.0, 953.0]}, {"polygon": [[400.0, 990.0], [416.0, 990.0], [416.0, 1002.0], [400.0, 1002.0]], "confidence": 0.6299313306808472, "text": "18", "bbox": [400.0, 990.0, 416.0, 1002.0]}], "languages": ["en"], "image_bbox": [0.0, 0.0, 816.0, 1056.0], "page": 5}]}