{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RAG based Querying on PDF Documents\n",
    "The scope of this exercise is to build a simple step within a data pipeline to help with data collection and transformation for an AI assistant based system. The primary data source will be a PDF document for this pipeline, and multiple documents can be used. The objective is that these steps are delivering a RAG based context for the AI assistant to answer questions based on the contents within the document(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from pipeline.text_extraction import read_ocr_results\n",
    "# input the openai key below, used for embedding.\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read file from the local OCR output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_json_files(directory):\n",
    "    \"\"\"\n",
    "    Function to find all json files in a directory and its subdirectories.\n",
    "    \"\"\"\n",
    "    json_files = []\n",
    "    for json_file in glob.glob(os.path.join(directory, '**', '*.json')):\n",
    "        json_files.append(json_file)\n",
    "    return json_files\n",
    "\n",
    "directory_path = 'data/results/'\n",
    "json_files = find_json_files(directory_path)\n",
    "\n",
    "text_data_all_files = []\n",
    "metadata_all_files = []\n",
    "for file in json_files:\n",
    "    text_data, metadata = read_ocr_results(file)\n",
    "    text_data_all_files += text_data\n",
    "    metadata_all_files += metadata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning\n",
    "# Remove references in the format [<text>] and special characters and convert to lower case.\n",
    "cleaned_text = [re.sub(r'\\[[^\\]]*\\]', '', text) for text in text_data_all_files]\n",
    "cleaned_text = [text.lower() for text in cleaned_text]\n",
    "cleaned_text = [re.sub(r'[^a-zA-Z0-9 .]', '', text) for text in cleaned_text]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Splitting: Chunk the text into smaller pieces with overlap.\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=300,\n",
    "    length_function=len\n",
    ")\n",
    "texts = text_splitter.create_documents(cleaned_text, metadata_all_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ChromaDB based Vector storage\n",
    "Create vector embeddings out of the text chunks that are extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector DB\n",
    "directory_name = 'vector_db'\n",
    "\n",
    "embedding = OpenAIEmbeddings(model='text-embedding-ada-002')\n",
    "\n",
    "vectordb = Chroma.from_documents(documents=texts, \n",
    "                                 embedding=embedding,\n",
    "                                 persist_directory=directory_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can load the persisted database from disk, and use it as normal. \n",
    "vectordb_loaded = Chroma(persist_directory=directory_name, \n",
    "                  embedding_function=embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Direct use of similarity search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='offers an overview of image quality assessment iqa metrics sorted by reference image availability and nature and reviews the current evaluation framework for novel view synthesis nvs. 2.1 image quality assessment metrics fullreference metrics mean squared error mse peak signaltonoise ratio psnr and structural similarity index measure ssim  are key metrics in the friqa group for comparing a query image with the ground truth due to their simplicity and accuracy. further several variants are proposed to improve performance by evaluating at multiscale  utilising handcrafted features  and extend to specific applications such as image stitching  and high dynamic range hdr images . recently deep neural networks have advanced friqa towards aligning assessments more closely with human visual perception . overall friqa offers detailed evaluation at the cost of requiring ground truth images. reducedreference metrics rriqa methods are designed to address situa tions where only partial information', metadata={'file_name': 'sample2', 'page_no': 3}),\n",
       " Document(page_content='offers an overview of image quality assessment iqa metrics sorted by reference image availability and nature and reviews the current evaluation framework for novel view synthesis nvs. 2.1 image quality assessment metrics fullreference metrics mean squared error mse peak signaltonoise ratio psnr and structural similarity index measure ssim  are key metrics in the friqa group for comparing a query image with the ground truth due to their simplicity and accuracy. further several variants are proposed to improve performance by evaluating at multiscale  utilising handcrafted features  and extend to specific applications such as image stitching  and high dynamic range hdr images . recently deep neural networks have advanced friqa towards aligning assessments more closely with human visual perception . overall friqa offers detailed evaluation at the cost of requiring ground truth images. reducedreference metrics rriqa methods are designed to address situa tions where only partial information', metadata={'file_name': 'sample2', 'page_no': 3}),\n",
       " Document(page_content='offers an overview of image quality assessment iqa metrics sorted by reference image availability and nature and reviews the current evaluation framework for novel view synthesis nvs. 2.1 image quality assessment metrics fullreference metrics mean squared error mse peak signaltonoise ratio psnr and structural similarity index measure ssim  are key metrics in the friqa group for comparing a query image with the ground truth due to their simplicity and accuracy. further several variants are proposed to improve performance by evaluating at multiscale  utilising handcrafted features  and extend to specific applications such as image stitching  and high dynamic range hdr images . recently deep neural networks have advanced friqa towards aligning assessments more closely with human visual perception . overall friqa offers detailed evaluation at the cost of requiring ground truth images. reducedreference metrics rriqa methods are designed to address situa tions where only partial information', metadata={'file_name': 'sample2', 'page_no': 3}),\n",
       " Document(page_content='2 z. wang et al. 1 introduction accurate image quality evaluation is critical for enhancing the performance of computer vision tasks including image processing image generation and novel view synthesis nvs. image quality assessment iqa methods can be categorised based on the type of referencing. the most popular group fullreference fr metric such as psnr ssim  lpips  evaluates the differences between a query image and a reference image in terms of pixels and perceptual quality. these metrics are essential for tasks such as superresolution denoising and compression and importantly assume the availability of an oracle reference image. ground truth images however may not always be available for instance in image generation tasks. consequently multiple attempts have been made to alleviate the dependency on ground truth oracles. for instance fid  is a generalreference gr scheme that evaluates the discrepancy of data dis tribution between image sets. alternatively multimodalreference mmr ap', metadata={'file_name': 'sample2', 'page_no': 2})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is IQA?\"\n",
    "docs = vectordb_loaded.similarity_search(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert Query to Vector and then DB search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='offers an overview of image quality assessment iqa metrics sorted by reference image availability and nature and reviews the current evaluation framework for novel view synthesis nvs. 2.1 image quality assessment metrics fullreference metrics mean squared error mse peak signaltonoise ratio psnr and structural similarity index measure ssim  are key metrics in the friqa group for comparing a query image with the ground truth due to their simplicity and accuracy. further several variants are proposed to improve performance by evaluating at multiscale  utilising handcrafted features  and extend to specific applications such as image stitching  and high dynamic range hdr images . recently deep neural networks have advanced friqa towards aligning assessments more closely with human visual perception . overall friqa offers detailed evaluation at the cost of requiring ground truth images. reducedreference metrics rriqa methods are designed to address situa tions where only partial information', metadata={'file_name': 'sample2', 'page_no': 3}),\n",
       " Document(page_content='category and distortion type. clipiqa  applies clip to iqa with simple antonym prompts to access image qualities such as brightness and noisi ness. by associating semantics between text and vision these metrics are com monly used in texttoimage generation and editing  and image captioning tasks  yet they lack the capability for detailed evaluation. 2.2 image quality assessment in nvs systems common evaluation metrics for nvs tasks include fullreference fr metrics such as psnr ssim  and lpips  which produce detailed similarity assessment between rendered and ground truth images. enhancements to the', metadata={'file_name': 'sample2', 'page_no': 4}),\n",
       " Document(page_content='baselines we choose five wellestablished iqa methods as baselines. two from friqa family ssim  and psnr and three from nriqa family brisque  niqe  and piqe . network training architecture we adopt a pretrained dinov2small net work as the image encoder enc  which encodes images with a patch size 14  14 and produces features in 384 channels. the cls token is ignored. the cross reference module  cross incorporates 2 transformer decoder layers with hidden dimension 384 and the decoder  dec is equipped with a 2layer mlp. prepro cessing during training we randomly crop 518  518 images from both query and reference images whilst during inference our model supports inputs at ar bitrary resolutions. notably raw ssim maps may occasionally present values', metadata={'file_name': 'sample2', 'page_no': 8}),\n",
       " Document(page_content='the blurring from patchwise en coding of vit models. second tackling the issue with unconventional images such as those from fisheye lenses that lead to inaccurate predictions as illustrated in fig. 7. fig. 7 evaluating a fish eyestyle query image. 5 conclusion in summary we introduce a novel crossreference image quality assessment criqa scheme filling a critical gap in existing iqa schemes. by leveraging a neural network with crossattention mechanisms and a unique nvsenabled data collection pipeline we demonstrate the feasibility of accurately evaluating the quality of an image by comparing it with other views of the same scene. our experimental results indicate that our predictions closely align with ground truthdependent metrics. acknowledgement this research is supported by an aria research gift grant from meta reality lab. we gratefully thank shangzhe wu tengda han zihang lai for insightful discussions and michael hobley for proofreading.', metadata={'file_name': 'sample2', 'page_no': 13})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_embed = embedding.embed_query(query)\n",
    "docs = await vectordb_loaded.amax_marginal_relevance_search_by_vector(query_embed)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a LLM retriever based on prompt template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectordb_loaded.as_retriever(search_type='similarity_score_threshold' ,search_kwargs={\"score_threshold\": 0.7})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\\n\\n{context}\\n\\nQuestion: {question}\\nHelpful Answer:\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(llm=OpenAI(), \n",
    "                                  chain_type=\"stuff\", \n",
    "                                  retriever=retriever, \n",
    "                                  return_source_documents=True)\n",
    "\n",
    "# Default prompt template\n",
    "qa_chain.combine_documents_chain.llm_chain.prompt.template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_llm_response(llm_response):\n",
    "    \"\"\"\n",
    "    Function to process the response from the LLM model.\n",
    "    \"\"\"\n",
    "    print(llm_response['result'])\n",
    "    print('\\n\\nSources:')\n",
    "    for source in llm_response[\"source_documents\"][:1]:\n",
    "        print(\"Page No: \", source.metadata['page_no'])\n",
    "        print(\"PDF File: \", source.metadata['file_name'])\n",
    "        print(\"Page Content: \", source.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example Usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " IQA stands for image quality assessment.\n",
      "\n",
      "\n",
      "Sources:\n",
      "Page No:  3\n",
      "PDF File:  sample2\n",
      "Page Content:  offers an overview of image quality assessment iqa metrics sorted by reference image availability and nature and reviews the current evaluation framework for novel view synthesis nvs. 2.1 image quality assessment metrics fullreference metrics mean squared error mse peak signaltonoise ratio psnr and structural similarity index measure ssim  are key metrics in the friqa group for comparing a query image with the ground truth due to their simplicity and accuracy. further several variants are proposed to improve performance by evaluating at multiscale  utilising handcrafted features  and extend to specific applications such as image stitching  and high dynamic range hdr images . recently deep neural networks have advanced friqa towards aligning assessments more closely with human visual perception . overall friqa offers detailed evaluation at the cost of requiring ground truth images. reducedreference metrics rriqa methods are designed to address situa tions where only partial information\n"
     ]
    }
   ],
   "source": [
    "query = \"What is IQA?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " SpaceByte is a new bytelevel decoder architecture that utilizes multiscale modeling and a dynamic patching rule to improve efficiency and performance on a variety of text modalities. It has been shown to outperform other bytelevel architectures and roughly match the performance of subword transformers.\n",
      "\n",
      "\n",
      "Sources:\n",
      "Page No:  2\n",
      "PDF File:  sample1\n",
      "Page Content:  as a subword level transformer. to close this substantial performance gap we propose a new bytelevel decoder architecture spacebyte. spacebyte also utilizes multiscale modeling to improve efficiency by grouping bytes into patches. but ulike megabyte which uses a fixed patch size spacebyte uses a simple rule to dynamically partition the bytes into patches that are aligned with word and other language boundaries. a similar technique was also explored by thawani et al. . our experiments show that this simple modification is crucial for performance allowing spacebyte to outperform other bytelevel architectures and roughly match the performance of subword transformers across a variety of text modalities. our experiments are performed on datasets consisting of english books latex formatted arxiv pers and opensource code. for other data modalities spacebyte with our simple patching rule might not be as effective. 2 spacebyte the spacebyte architecture is summarized in figure 1 . in a\n"
     ]
    }
   ],
   "source": [
    "query = \"What is SpaceByte?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Tokenization imposes several disadvantages, including performance biases, increased adversarial vulnerability, decreased character level modeling performance, and increased modeling complexity.\n",
      "\n",
      "\n",
      "Sources:\n",
      "Page No:  1\n",
      "PDF File:  sample1\n",
      "Page Content:  spacebyte towards deleting tokenization from large language modeling kevin slagle rice university kevin.slaglerice.edu rxiv2404.14408v1  22 apr 202 abstract tokenization is widely used in large language models because it significantly improves performance. however tokenization imposes several disadvantages such as performance biases increased adversarial vulnerability decreased character level modeling performance and increased modeling complexity. to address these disadvantages without sacrificing performance we propose spacebyte a novel bytelevel decoder architecture that closes the performance gap between bytelevel and subword autoregressive language modeling. spacebyte consists of a bytelevel transformer model but with extra larger transformer blocks inserted in the middle of the layers. we find that performance is significantly improved by applying these larger blocks only after certain bytes such as space characters which typically denote word boundaries.  our experiments show\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the disaddvantage of Tokenization?\"\n",
    "llm_response = qa_chain(query)\n",
    "process_llm_response(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
